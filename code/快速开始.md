# 🚀 快速开始训练

## 📋 训练前检查清单

1. **数据集验证** ✅
   - 训练数据: `/home/wangbinrui/research_projects/llama_rlhf/datasets/ultrafeedback_binarized/train_prefs_ultrafeedback_binarized.jsonl`
   - 测试数据: `/home/wangbinrui/research_projects/llama_rlhf/datasets/ultrafeedback_binarized/test_prefs_ultrafeedback_binarized.jsonl`

2. **模型路径** ✅
   - 策略模型: `Qwen/Qwen2.5-1.5B-Instruct`
   - 参考模型: `Qwen/Qwen2.5-1.5B-Instruct`

3. **输出目录** ✅
   - 模型输出: `./model_output/Preference_Guided_DPO`
   - 日志目录: `./logs/Preference_Guided_DPO`

## 🎯 两种训练模式

### 模式1: 文本指令训练 (推荐新手)
```bash
cd /home/wangbinrui/research_projects/llama_rlhf/code
./fast_train.sh
```

### 模式2: 嵌入向量训练 (需要预训练嵌入)
1. 编辑 `fast_train.sh`
2. 设置 `TRAINING_MODE="embedding"`
3. 设置 `PROMPT_EMBEDDING_PATH="/path/to/your/embeddings.pt"`
4. 运行 `./fast_train.sh`

## ⚙️ 关键参数说明 (在fast_train.sh中修改)

```bash
# 🎛️ 核心参数
BETA=0.05              # DPO温度 (0.01-0.1, 越小越保守)
ALPHA=0.1              # 偏好权重 (0.05-0.2, 控制偏好强度)
LEARNING_RATE=5e-4     # 学习率
NUM_EPOCHS=1           # 训练轮数

# 🔗 LoRA参数  
LORA_R=16              # LoRA秩 (8/16/32)
LORA_ALPHA=32          # LoRA缩放
LORA_DROPOUT=0.1       # Dropout率

# 📏 长度限制
MAX_LENGTH=300         # 最大序列长度
MAX_PROMPT_LENGTH=128  # 最大提示长度
GRADIENT_ACCUM_STEPS=512  # 梯度累积步数

# 💬 偏好指令 (文本模式)
PREFERENCE_TEXT="请提供一个有用、诚实、无害且简洁的回答，尊重用户的自主权。"
```

## 🚀 立即开始训练

```bash
# 进入项目目录
cd /home/wangbinrui/research_projects/llama_rlhf/code

# 给脚本执行权限
chmod +x fast_train.sh
chmod +x test_deepspeed_cmd.sh

# 🧪 测试DeepSpeed命令构建 (可选)
./test_deepspeed_cmd.sh

# 🚀 开始训练 (使用DeepSpeed分布式训练)
./fast_train.sh
```

## 🔥 DeepSpeed训练说明

脚本会自动使用您原来的DeepSpeed命令格式:
```bash
CUDA_ALLOC_CONF=expandable_segments deepspeed --include=localhost:1,2 train_with_preference_prompt.py [参数...]
```

这确保了:
- ✅ 扩展内存段配置 (`expandable_segments`)
- ✅ 指定GPU节点 (`localhost:1,2`) 
- ✅ 分布式训练优化
- ✅ 与您现有训练流程兼容

## 📊 训练监控

训练过程中会显示:
- 🔧 环境配置状态
- 📁 数据集加载进度  
- 🏗️ 模型初始化状态
- 🚀 训练进度和损失
- ✅ 完成状态和输出路径

## 🔍 手动验证命令

```bash
# 检查训练脚本
python train_with_preference_prompt.py --help

# 检查数据集
wc -l /home/wangbinrui/research_projects/llama_rlhf/datasets/ultrafeedback_binarized/*.jsonl

# 检查GPU
nvidia-smi

# 测试导入
python -c "import torch, transformers, peft, deepspeed; print('✅ 所有依赖正常')"
```

## 🎯 预期输出

训练成功后会在以下位置找到:
- 📁 模型文件: `./model_output/Preference_Guided_DPO/`
- 📊 训练日志: `./logs/Preference_Guided_DPO/`
- 🏆 最终模型可直接用于推理

## ❗ 常见问题

1. **内存不足**: 减少 `GRADIENT_ACCUM_STEPS` 或 `MAX_LENGTH`
2. **速度太慢**: 增加 GPU 数量或减少 `LORA_R`
3. **效果不好**: 调整 `BETA` 和 `ALPHA` 参数
4. **导入错误**: 确保所有依赖包已安装

立即开始您的偏好引导DPO训练之旅! 🎉
