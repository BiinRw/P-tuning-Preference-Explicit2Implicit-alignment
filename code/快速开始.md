# ğŸš€ å¿«é€Ÿå¼€å§‹è®­ç»ƒ

## ğŸ“‹ è®­ç»ƒå‰æ£€æŸ¥æ¸…å•

1. **æ•°æ®é›†éªŒè¯** âœ…
   - è®­ç»ƒæ•°æ®: `/home/wangbinrui/research_projects/llama_rlhf/datasets/ultrafeedback_binarized/train_prefs_ultrafeedback_binarized.jsonl`
   - æµ‹è¯•æ•°æ®: `/home/wangbinrui/research_projects/llama_rlhf/datasets/ultrafeedback_binarized/test_prefs_ultrafeedback_binarized.jsonl`

2. **æ¨¡å‹è·¯å¾„** âœ…
   - ç­–ç•¥æ¨¡å‹: `Qwen/Qwen2.5-1.5B-Instruct`
   - å‚è€ƒæ¨¡å‹: `Qwen/Qwen2.5-1.5B-Instruct`

3. **è¾“å‡ºç›®å½•** âœ…
   - æ¨¡å‹è¾“å‡º: `./model_output/Preference_Guided_DPO`
   - æ—¥å¿—ç›®å½•: `./logs/Preference_Guided_DPO`

## ğŸ¯ ä¸¤ç§è®­ç»ƒæ¨¡å¼

### æ¨¡å¼1: æ–‡æœ¬æŒ‡ä»¤è®­ç»ƒ (æ¨èæ–°æ‰‹)
```bash
cd /home/wangbinrui/research_projects/llama_rlhf/code
./fast_train.sh
```

### æ¨¡å¼2: åµŒå…¥å‘é‡è®­ç»ƒ (éœ€è¦é¢„è®­ç»ƒåµŒå…¥)
1. ç¼–è¾‘ `fast_train.sh`
2. è®¾ç½® `TRAINING_MODE="embedding"`
3. è®¾ç½® `PROMPT_EMBEDDING_PATH="/path/to/your/embeddings.pt"`
4. è¿è¡Œ `./fast_train.sh`

## âš™ï¸ å…³é”®å‚æ•°è¯´æ˜ (åœ¨fast_train.shä¸­ä¿®æ”¹)

```bash
# ğŸ›ï¸ æ ¸å¿ƒå‚æ•°
BETA=0.05              # DPOæ¸©åº¦ (0.01-0.1, è¶Šå°è¶Šä¿å®ˆ)
ALPHA=0.1              # åå¥½æƒé‡ (0.05-0.2, æ§åˆ¶åå¥½å¼ºåº¦)
LEARNING_RATE=5e-4     # å­¦ä¹ ç‡
NUM_EPOCHS=1           # è®­ç»ƒè½®æ•°

# ğŸ”— LoRAå‚æ•°  
LORA_R=16              # LoRAç§© (8/16/32)
LORA_ALPHA=32          # LoRAç¼©æ”¾
LORA_DROPOUT=0.1       # Dropoutç‡

# ğŸ“ é•¿åº¦é™åˆ¶
MAX_LENGTH=300         # æœ€å¤§åºåˆ—é•¿åº¦
MAX_PROMPT_LENGTH=128  # æœ€å¤§æç¤ºé•¿åº¦
GRADIENT_ACCUM_STEPS=512  # æ¢¯åº¦ç´¯ç§¯æ­¥æ•°

# ğŸ’¬ åå¥½æŒ‡ä»¤ (æ–‡æœ¬æ¨¡å¼)
PREFERENCE_TEXT="è¯·æä¾›ä¸€ä¸ªæœ‰ç”¨ã€è¯šå®ã€æ— å®³ä¸”ç®€æ´çš„å›ç­”ï¼Œå°Šé‡ç”¨æˆ·çš„è‡ªä¸»æƒã€‚"
```

## ğŸš€ ç«‹å³å¼€å§‹è®­ç»ƒ

```bash
# è¿›å…¥é¡¹ç›®ç›®å½•
cd /home/wangbinrui/research_projects/llama_rlhf/code

# ç»™è„šæœ¬æ‰§è¡Œæƒé™
chmod +x fast_train.sh
chmod +x test_deepspeed_cmd.sh

# ğŸ§ª æµ‹è¯•DeepSpeedå‘½ä»¤æ„å»º (å¯é€‰)
./test_deepspeed_cmd.sh

# ğŸš€ å¼€å§‹è®­ç»ƒ (ä½¿ç”¨DeepSpeedåˆ†å¸ƒå¼è®­ç»ƒ)
./fast_train.sh
```

## ğŸ”¥ DeepSpeedè®­ç»ƒè¯´æ˜

è„šæœ¬ä¼šè‡ªåŠ¨ä½¿ç”¨æ‚¨åŸæ¥çš„DeepSpeedå‘½ä»¤æ ¼å¼:
```bash
CUDA_ALLOC_CONF=expandable_segments deepspeed --include=localhost:1,2 train_with_preference_prompt.py [å‚æ•°...]
```

è¿™ç¡®ä¿äº†:
- âœ… æ‰©å±•å†…å­˜æ®µé…ç½® (`expandable_segments`)
- âœ… æŒ‡å®šGPUèŠ‚ç‚¹ (`localhost:1,2`) 
- âœ… åˆ†å¸ƒå¼è®­ç»ƒä¼˜åŒ–
- âœ… ä¸æ‚¨ç°æœ‰è®­ç»ƒæµç¨‹å…¼å®¹

## ğŸ“Š è®­ç»ƒç›‘æ§

è®­ç»ƒè¿‡ç¨‹ä¸­ä¼šæ˜¾ç¤º:
- ğŸ”§ ç¯å¢ƒé…ç½®çŠ¶æ€
- ğŸ“ æ•°æ®é›†åŠ è½½è¿›åº¦  
- ğŸ—ï¸ æ¨¡å‹åˆå§‹åŒ–çŠ¶æ€
- ğŸš€ è®­ç»ƒè¿›åº¦å’ŒæŸå¤±
- âœ… å®ŒæˆçŠ¶æ€å’Œè¾“å‡ºè·¯å¾„

## ğŸ” æ‰‹åŠ¨éªŒè¯å‘½ä»¤

```bash
# æ£€æŸ¥è®­ç»ƒè„šæœ¬
python train_with_preference_prompt.py --help

# æ£€æŸ¥æ•°æ®é›†
wc -l /home/wangbinrui/research_projects/llama_rlhf/datasets/ultrafeedback_binarized/*.jsonl

# æ£€æŸ¥GPU
nvidia-smi

# æµ‹è¯•å¯¼å…¥
python -c "import torch, transformers, peft, deepspeed; print('âœ… æ‰€æœ‰ä¾èµ–æ­£å¸¸')"
```

## ğŸ¯ é¢„æœŸè¾“å‡º

è®­ç»ƒæˆåŠŸåä¼šåœ¨ä»¥ä¸‹ä½ç½®æ‰¾åˆ°:
- ğŸ“ æ¨¡å‹æ–‡ä»¶: `./model_output/Preference_Guided_DPO/`
- ğŸ“Š è®­ç»ƒæ—¥å¿—: `./logs/Preference_Guided_DPO/`
- ğŸ† æœ€ç»ˆæ¨¡å‹å¯ç›´æ¥ç”¨äºæ¨ç†

## â— å¸¸è§é—®é¢˜

1. **å†…å­˜ä¸è¶³**: å‡å°‘ `GRADIENT_ACCUM_STEPS` æˆ– `MAX_LENGTH`
2. **é€Ÿåº¦å¤ªæ…¢**: å¢åŠ  GPU æ•°é‡æˆ–å‡å°‘ `LORA_R`
3. **æ•ˆæœä¸å¥½**: è°ƒæ•´ `BETA` å’Œ `ALPHA` å‚æ•°
4. **å¯¼å…¥é”™è¯¯**: ç¡®ä¿æ‰€æœ‰ä¾èµ–åŒ…å·²å®‰è£…

ç«‹å³å¼€å§‹æ‚¨çš„åå¥½å¼•å¯¼DPOè®­ç»ƒä¹‹æ—…! ğŸ‰
