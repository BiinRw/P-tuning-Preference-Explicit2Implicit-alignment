import torch
import torch.nn.functional as F
from typing import Dict, List, Optional, Any
from transformers import Trainer, TrainerCallback
from ptuning_model import PTuningModel
import logging
import wandb
import numpy as np
from tqdm import tqdm
import os

logger = logging.getLogger(__name__)


class PTuningEarlyStoppingCallback(TrainerCallback):
    """
    P-tuningæ—©åœå›è°ƒå‡½æ•°
    """
    
    def __init__(self, patience: int = 5, threshold: float = 0.01, target_accuracy: float = 0.9):
        self.patience = patience
        self.threshold = threshold
        self.target_accuracy = target_accuracy
        self.best_accuracy = 0.0
        self.best_margin = 0.0
        self.patience_counter = 0
        self.should_stop = False
    
    def on_evaluate(self, args, state, control, logs=None, **kwargs):
        """
        éªŒè¯æ—¶è°ƒç”¨
        """
        current_accuracy = logs.get("eval_ranking_accuracy", 0.0)
        current_margin = logs.get("eval_mean_margin", 0.0)
        
        # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°ç›®æ ‡å‡†ç¡®ç‡
        if current_accuracy >= self.target_accuracy:
            print(f"ğŸ¯ Target accuracy {self.target_accuracy:.1%} reached! Current: {current_accuracy:.1%}")
            control.should_training_stop = True
            return control
        
        # æ—©åœé€»è¾‘ï¼šåŸºäºå‡†ç¡®ç‡å’Œè¾¹è·çš„ç»¼åˆæ”¹å–„
        improvement = (current_accuracy - self.best_accuracy) + 0.1 * (current_margin - self.best_margin)
        
        if improvement > self.threshold:
            self.best_accuracy = max(self.best_accuracy, current_accuracy)
            self.best_margin = max(self.best_margin, current_margin)
            self.patience_counter = 0
            print(f"âœ… Validation improved! Accuracy: {current_accuracy:.1%}, Margin: {current_margin:.4f}")
        else:
            self.patience_counter += 1
            print(f"âš ï¸ No improvement for {self.patience_counter}/{self.patience} evaluations")
            
            if self.patience_counter >= self.patience:
                print(f"ğŸ›‘ Early stopping triggered! Best accuracy: {self.best_accuracy:.1%}")
                control.should_training_stop = True
        
        return control


class PTuningTrainer(Trainer):
    """
    P-tuningä¸“ç”¨è®­ç»ƒå™¨ï¼Œå¸¦éªŒè¯è¯„ä¼°å’Œwandbç›‘æ§
    """
    
    def __init__(
        self,
        model: PTuningModel,
        preference_loss_weight: float = 1.0,
        margin: float = 0.1,
        kl_loss_weight: float = 0.1,  # ğŸ†• æ–°å¢KLæ•£åº¦æŸå¤±æƒé‡
        early_stopping_patience: int = 5,
        early_stopping_threshold: float = 0.01,
        target_accuracy: float = 0.9,
        use_wandb: bool = True,
        **kwargs
    ):
        """
        åˆå§‹åŒ–P-tuningè®­ç»ƒå™¨
        
        Args:
            kl_loss_weight: KLæ•£åº¦æŸå¤±æƒé‡ï¼Œç”¨äºçº¦æŸpromptå¯¹åŸæ¨¡å‹è¾“å‡ºåˆ†å¸ƒçš„å½±å“
        """
        super().__init__(model=model, **kwargs)
        self.preference_loss_weight = preference_loss_weight
        self.margin = margin
        self.kl_loss_weight = kl_loss_weight  # ğŸ†• KLæ•£åº¦çº¦æŸæƒé‡
        self.use_wandb = use_wandb
        
        # æ·»åŠ æ—©åœå›è°ƒ
        early_stopping_callback = PTuningEarlyStoppingCallback(
            patience=early_stopping_patience,
            threshold=early_stopping_threshold,
            target_accuracy=target_accuracy
        )
        self.add_callback(early_stopping_callback)
        
        # è®­ç»ƒçŠ¶æ€è®°å½•
        self.step_count = 0
    
    def compute_loss(self, model, inputs, return_outputs=False):
        """
        è®¡ç®—æ€»æŸå¤± = åå¥½æŸå¤± + KLçº¦æŸæŸå¤±
        """
        # ç›´æ¥å¤„ç†ä½ çš„æ•°æ®æ ¼å¼
        device = next(model.parameters()).device
        
        # ä»æ•°æ®é›†ä¸­è·å–chosenå’Œrejectedæ–‡æœ¬
        if 'chosen' in inputs and 'rejected' in inputs:
            chosen_texts = inputs['chosen']
            rejected_texts = inputs['rejected']
            prompts = inputs.get('prompt', [''] * len(chosen_texts))
            
            # è·å–tokenizerè¿›è¡Œç¼–ç 
            if hasattr(self, 'processing_class'):
                tokenizer_processor = self.processing_class
            elif hasattr(self, 'tokenizer'):
                tokenizer_processor = self.tokenizer
            
            # ğŸš¨ å…³é”®ä¿®å¤ï¼šåªéœ€è¦ç¼–ç åŸå§‹chosen/rejectedæ–‡æœ¬
            chosen_encoding = tokenizer_processor(
                chosen_texts,
                padding=True,
                truncation=True,
                max_length=512,
                return_tensors="pt"
            )
            
            rejected_encoding = tokenizer_processor(
                rejected_texts,
                padding=True,
                truncation=True,
                max_length=512,
                return_tensors="pt"
            )
            
            # ç§»åŠ¨åˆ°è®¾å¤‡
            chosen_input_ids = chosen_encoding["input_ids"].to(device)
            chosen_attention_mask = chosen_encoding["attention_mask"].to(device)
            rejected_input_ids = rejected_encoding["input_ids"].to(device)
            rejected_attention_mask = rejected_encoding["attention_mask"].to(device)
            
        else:
            raise ValueError(f"Expected 'chosen' and 'rejected' keys in inputs, got: {list(inputs.keys())}")
        
        # ğŸš¨ ä¿®å¤ï¼šè®¡ç®—åå¥½æŸå¤±å’ŒKLæŸå¤±ï¼Œä¼ å…¥æ­£ç¡®çš„å‚æ•°
        preference_loss, kl_loss = self._compute_preference_loss(
            model=model,
            preferred_input_ids=chosen_input_ids,
            rejected_input_ids=rejected_input_ids,
            preferred_attention_mask=chosen_attention_mask,
            rejected_attention_mask=rejected_attention_mask
        )
        
        # ç»„åˆæ€»æŸå¤±
        total_loss = (
            self.preference_loss_weight * preference_loss + 
            self.kl_loss_weight * kl_loss
        )
        
        # è®°å½•è®­ç»ƒæŒ‡æ ‡åˆ°wandb
        if self.use_wandb and wandb.run is not None:
            self.step_count += 1
            wandb.log({
                "train/total_loss": total_loss.item(),
                "train/preference_loss": preference_loss.item(),
                "train/kl_loss": kl_loss.item(),
                "train/step": self.step_count
            })
        
        return (total_loss, None) if return_outputs else total_loss
    
    def evaluate(self, eval_dataset=None, ignore_keys=None, metric_key_prefix="eval"):
        """
        æ‰§è¡ŒéªŒè¯è¯„ä¼°ï¼Œè®¡ç®—ranking accuracyå’Œmean margin
        """
        eval_dataset = eval_dataset if eval_dataset is not None else self.eval_dataset
        
        if (eval_dataset is None):
            print("âš ï¸ No evaluation dataset provided")
            return {}
        
        print(f"ğŸ” Starting evaluation on {len(eval_dataset)} samples...")
        
        # è®¾ç½®æ¨¡å‹ä¸ºè¯„ä¼°æ¨¡å¼
        self.model.eval()
        
        correct_predictions = 0
        total_predictions = 0
        log_prob_differences = []
        
        device = next(self.model.parameters()).device
        
        # è·å–tokenizer
        if hasattr(self, 'processing_class'):
            tokenizer_processor = self.processing_class
        elif hasattr(self, 'tokenizer'):
            tokenizer_processor = self.tokenizer
        
        with torch.no_grad():
            for i in tqdm(range(len(eval_dataset)), desc="Evaluating"):
                sample = eval_dataset[i]
                
                # å‡†å¤‡æ•°æ®
                chosen_text = sample['chosen']
                rejected_text = sample['rejected']
                
                # ç¼–ç 
                chosen_encoding = tokenizer_processor(
                    [chosen_text],
                    padding=True,
                    truncation=True,
                    max_length=512,
                    return_tensors="pt"
                )
                
                rejected_encoding = tokenizer_processor(
                    [rejected_text],
                    padding=True,
                    truncation=True,
                    max_length=512,
                    return_tensors="pt"
                )
                
                # ç§»åŠ¨åˆ°è®¾å¤‡
                chosen_input_ids = chosen_encoding["input_ids"].to(device)
                chosen_attention_mask = chosen_encoding["attention_mask"].to(device)
                rejected_input_ids = rejected_encoding["input_ids"].to(device)
                rejected_attention_mask = rejected_encoding["attention_mask"].to(device)
                
                # å‰å‘ä¼ æ’­
                chosen_outputs = self.model(
                    input_ids=chosen_input_ids,
                    attention_mask=chosen_attention_mask
                )
                rejected_outputs = self.model(
                    input_ids=rejected_input_ids,
                    attention_mask=rejected_attention_mask
                )
                
                # è®¡ç®—å¯¹æ•°æ¦‚ç‡
                chosen_log_prob = self._compute_sequence_log_probs(
                    chosen_outputs.logits, chosen_input_ids, chosen_attention_mask
                )
                rejected_log_prob = self._compute_sequence_log_probs(
                    rejected_outputs.logits, rejected_input_ids, rejected_attention_mask
                )
                
                # è®¡ç®—æ¦‚ç‡å·® Î´pi = log P(y_wi|xi,p) - log P(y_li|xi,p)
                log_prob_diff = chosen_log_prob.item() - rejected_log_prob.item()
                log_prob_differences.append(log_prob_diff)
                
                # åˆ¤æ–­æ˜¯å¦æ­£ç¡®é¢„æµ‹ï¼ˆÎ´pi > 0 è¡¨ç¤ºchosenæ¦‚ç‡æ›´é«˜ï¼‰
                if log_prob_diff > 0:
                    correct_predictions += 1
                
                total_predictions += 1
        
        # è®¡ç®—æŒ‡æ ‡
        ranking_accuracy = correct_predictions / total_predictions if total_predictions > 0 else 0.0
        mean_margin = np.mean(log_prob_differences) if log_prob_differences else 0.0
        std_margin = np.std(log_prob_differences) if log_prob_differences else 0.0
        
        # åˆ›å»ºè¯„ä¼°ç»“æœ
        eval_results = {
            f"{metric_key_prefix}_ranking_accuracy": ranking_accuracy,
            f"{metric_key_prefix}_mean_margin": mean_margin,
            f"{metric_key_prefix}_std_margin": std_margin,
            f"{metric_key_prefix}_correct_predictions": correct_predictions,
            f"{metric_key_prefix}_total_predictions": total_predictions,
        }
        
        # è®°å½•åˆ°wandb
        if self.use_wandb and wandb.run is not None:
            wandb_logs = {
                "eval/ranking_accuracy": ranking_accuracy,
                "eval/mean_margin": mean_margin,
                "eval/std_margin": std_margin,
                "eval/correct_predictions": correct_predictions,
                "eval/total_predictions": total_predictions,
            }
            
            # æ·»åŠ æ¦‚ç‡å·®åˆ†å¸ƒç›´æ–¹å›¾
            if log_prob_differences:
                wandb_logs["eval/margin_distribution"] = wandb.Histogram(log_prob_differences)
            
            wandb.log(wandb_logs)
        
        # æ‰“å°ç»“æœ
        print(f"ğŸ“Š Evaluation Results:")
        print(f"   Ranking Accuracy: {ranking_accuracy:.1%}")
        print(f"   Mean Margin: {mean_margin:.4f}")
        print(f"   Std Margin: {std_margin:.4f}")
        print(f"   Correct Predictions: {correct_predictions}/{total_predictions}")
        
        # æ¢å¤è®­ç»ƒæ¨¡å¼
        self.model.train()
        
        return eval_results

    def _compute_preference_loss(
        self,
        model: PTuningModel,
        preferred_input_ids: torch.Tensor,
        rejected_input_ids: torch.Tensor,
        preferred_attention_mask: Optional[torch.Tensor] = None,
        rejected_attention_mask: Optional[torch.Tensor] = None
    ) -> tuple:
        """
        è®¡ç®—åå¥½æ’åºæŸå¤± + KLæ•£åº¦çº¦æŸæŸå¤±
        
        ğŸ¯ æ­£ç¡®çš„KLæ•£åº¦è®¡ç®—é€»è¾‘ï¼š
        
        1. P-tuningæ¨¡å‹ï¼šP(y|x, prompt_embeddings)
           - å¯¹åŸå§‹æ–‡æœ¬xä½¿ç”¨model()ï¼Œå†…éƒ¨ä¼šè‡ªåŠ¨æ·»åŠ è®­ç»ƒçš„prompt embeddings
           
        2. åŸå§‹æ¨¡å‹ï¼šP(y|x)  
           - å¯¹ç›¸åŒåŸå§‹æ–‡æœ¬xä½¿ç”¨model.base_model()ï¼Œä¸æ·»åŠ ä»»ä½•prompt
           
        3. KLçº¦æŸï¼šç¡®ä¿æ·»åŠ promptåçš„åˆ†å¸ƒä¸è¦åç¦»åŸå§‹åˆ†å¸ƒå¤ªè¿œ
           - KL(P(y|x, prompt_embeddings) || P(y|x)) < threshold
        
        Args:
            model: P-tuningæ¨¡å‹
            preferred_input_ids: åå¥½å›å¤çš„token ID
            rejected_input_ids: éåå¥½å›å¤çš„token ID
            preferred_attention_mask: åå¥½å›å¤çš„æ³¨æ„åŠ›æ©ç 
            rejected_attention_mask: éåå¥½å›å¤çš„æ³¨æ„åŠ›æ©ç 
            
        Returns:
            (preference_loss, kl_loss): åå¥½æŸå¤±å’ŒKLæ•£åº¦æŸå¤±çš„å…ƒç»„
        """
        # 1. è®¡ç®—P-tuningæ¨¡å‹çš„è¾“å‡ºï¼ˆåŒ…å«prompt embeddingsï¼‰
        # model() å†…éƒ¨ä¼šè‡ªåŠ¨æ·»åŠ prompt embeddingsåˆ°è¾“å…¥å‰é¢
        preferred_outputs_ptuning = model(
            input_ids=preferred_input_ids,
            attention_mask=preferred_attention_mask
        )
        rejected_outputs_ptuning = model(
            input_ids=rejected_input_ids,
            attention_mask=rejected_attention_mask
        )
        
        # 2. è®¡ç®—P-tuningæ¨¡å‹çš„åºåˆ—å¯¹æ•°æ¦‚ç‡
        preferred_log_probs_ptuning = self._compute_sequence_log_probs(
            preferred_outputs_ptuning.logits, preferred_input_ids, preferred_attention_mask
        )
        rejected_log_probs_ptuning = self._compute_sequence_log_probs(
            rejected_outputs_ptuning.logits, rejected_input_ids, rejected_attention_mask
        )
        
        # 3. è®¡ç®—åå¥½æŸå¤±
        preference_loss = F.relu(
            rejected_log_probs_ptuning - preferred_log_probs_ptuning + self.margin
        )
        
        # 4. ğŸš¨ æ­£ç¡®çš„KLæ•£åº¦çº¦æŸè®¡ç®—
        kl_loss = torch.tensor(0.0, device=preferred_input_ids.device)
        
        if self.kl_loss_weight > 0:  # åªæœ‰å½“KLæƒé‡>0æ—¶æ‰è®¡ç®—
            # è®¡ç®—åŸå§‹æ¨¡å‹åœ¨ç›¸åŒæ–‡æœ¬ä¸Šçš„è¾“å‡ºï¼ˆä¸ä½¿ç”¨promptï¼‰
            with torch.no_grad():  # ä¸éœ€è¦å¯¹åŸå§‹æ¨¡å‹è®¡ç®—æ¢¯åº¦
                preferred_outputs_original = model.base_model(
                    input_ids=preferred_input_ids,
                    attention_mask=preferred_attention_mask
                )
                rejected_outputs_original = model.base_model(
                    input_ids=rejected_input_ids,
                    attention_mask=rejected_attention_mask
                )
            
            # è®¡ç®—åŸå§‹æ¨¡å‹çš„åºåˆ—å¯¹æ•°æ¦‚ç‡
            preferred_log_probs_original = self._compute_sequence_log_probs_original(
                preferred_outputs_original.logits, 
                preferred_input_ids, 
                preferred_attention_mask
            )
            rejected_log_probs_original = self._compute_sequence_log_probs_original(
                rejected_outputs_original.logits, 
                rejected_input_ids, 
                rejected_attention_mask
            )
            
            # ğŸš¨ å…³é”®ä¿®å¤ï¼šæ­£ç¡®è®¡ç®—KLæ•£åº¦
            # ç›®æ ‡ï¼šP-tuningæ¨¡å‹çš„åˆ†å¸ƒä¸è¦åç¦»åŸå§‹æ¨¡å‹å¤ªè¿œ
            # ä½¿ç”¨MSEæŸå¤±è¿‘ä¼¼KLæ•£åº¦ï¼ˆåœ¨å¯¹æ•°ç©ºé—´ä¸­ï¼‰
            kl_loss_preferred = F.mse_loss(
                preferred_log_probs_ptuning, 
                preferred_log_probs_original.detach()  # ç¡®ä¿ä¸å¯¹åŸå§‹æ¨¡å‹è®¡ç®—æ¢¯åº¦
            )
            kl_loss_rejected = F.mse_loss(
                rejected_log_probs_ptuning, 
                rejected_log_probs_original.detach()
            )
            kl_loss = (kl_loss_preferred + kl_loss_rejected) / 2
        
        return preference_loss.mean(), kl_loss

    def _compute_sequence_log_probs_original(
        self,
        logits: torch.Tensor,
        input_ids: torch.Tensor,
        attention_mask: Optional[torch.Tensor] = None
    ) -> torch.Tensor:
        """
        ä¸“é—¨ç”¨äºåŸå§‹æ¨¡å‹çš„åºåˆ—å¯¹æ•°æ¦‚ç‡è®¡ç®—
        
        ä¸_compute_sequence_log_probsçš„åŒºåˆ«ï¼š
        - è¿™ä¸ªæ–¹æ³•ç”¨äºåŸå§‹æ¨¡å‹ï¼ˆä¸å«prompt embeddingsï¼‰
        - ç›´æ¥å¤„ç†å®Œæ•´çš„input_idsï¼Œä¸éœ€è¦è·³è¿‡promptéƒ¨åˆ†
        - logitsé•¿åº¦ = input_idsé•¿åº¦
        
        Args:
            logits: åŸå§‹æ¨¡å‹è¾“å‡ºçš„logits [batch_size, seq_len, vocab_size]
            input_ids: è¾“å…¥token ID [batch_size, seq_len]
            attention_mask: æ³¨æ„åŠ›æ©ç  [batch_size, seq_len]
            
        Returns:
            åºåˆ—çš„å¹³å‡å¯¹æ•°æ¦‚ç‡ [batch_size]
        """
        # å°†logitsè½¬æ¢ä¸ºå¯¹æ•°æ¦‚ç‡
        log_probs = F.log_softmax(logits, dim=-1)
        
        # ğŸš¨ å…³é”®ï¼šåŸå§‹æ¨¡å‹çš„logitsé•¿åº¦åº”è¯¥ç­‰äºinput_idsé•¿åº¦
        # å¦‚æœä¸ç­‰ï¼Œéœ€è¦æˆªæ–­åˆ°è¾ƒçŸ­çš„é•¿åº¦
        seq_len = min(log_probs.size(1), input_ids.size(1))
        log_probs = log_probs[:, :seq_len, :]
        input_ids = input_ids[:, :seq_len]
        
        if attention_mask is not None:
            attention_mask = attention_mask[:, :seq_len]
        
        # æ”¶é›†ç›®æ ‡tokençš„å¯¹æ•°æ¦‚ç‡
        gathered_log_probs = log_probs.gather(
            dim=-1, 
            index=input_ids.unsqueeze(-1)
        ).squeeze(-1)  # [batch_size, seq_len]
        
        # è®¡ç®—å¹³å‡å¯¹æ•°æ¦‚ç‡
        if attention_mask is not None:
            masked_log_probs = gathered_log_probs * attention_mask.float()
            valid_lengths = attention_mask.sum(dim=1).clamp(min=1)
            return masked_log_probs.sum(dim=1) / valid_lengths
        else:
            return gathered_log_probs.mean(dim=1)
    
    def _compute_sequence_log_probs(
        self,
        logits: torch.Tensor,
        input_ids: torch.Tensor,
        attention_mask: Optional[torch.Tensor] = None
    ) -> torch.Tensor:
        """
        è®¡ç®—æ•´ä¸ªåºåˆ—çš„å¯¹æ•°æ¦‚ç‡
        
        å…³é”®ä¿®æ”¹ï¼š
        1. è®¡ç®—å®Œæ•´åºåˆ—çš„æ¦‚ç‡ï¼ˆåŒ…æ‹¬promptå½±å“ï¼‰
        2. ä¸è·³è¿‡promptéƒ¨åˆ†ï¼Œå› ä¸ºpromptçš„ä½œç”¨å°±æ˜¯å½±å“æ•´ä¸ªåºåˆ—çš„æ¦‚ç‡åˆ†å¸ƒ
        
        Args:
            logits: æ¨¡å‹è¾“å‡ºçš„logits [batch_size, seq_len_with_prompt, vocab_size]
            input_ids: è¾“å…¥token ID [batch_size, seq_len_without_prompt]
            attention_mask: æ³¨æ„åŠ›æ©ç  [batch_size, seq_len_without_prompt]
            
        Returns:
            åºåˆ—çš„å¹³å‡å¯¹æ•°æ¦‚ç‡
        """
        # å°†logitsè½¬æ¢ä¸ºå¯¹æ•°æ¦‚ç‡
        log_probs = F.log_softmax(logits, dim=-1)
        
        # ç”±äºpromptè¢«æ·»åŠ åˆ°åºåˆ—å‰é¢ï¼Œæˆ‘ä»¬éœ€è¦ä»logitsä¸­æå–å¯¹åº”åŸå§‹è¾“å…¥çš„éƒ¨åˆ†
        num_prompt_tokens = self.model.num_virtual_tokens
        
        # æå–å¯¹åº”åŸå§‹è¾“å…¥åºåˆ—çš„logitsï¼ˆè·³è¿‡promptéƒ¨åˆ†ï¼‰
        # logitså½¢çŠ¶: [batch_size, num_prompt_tokens + seq_len, vocab_size]
        # æˆ‘ä»¬éœ€è¦: [batch_size, seq_len, vocab_size]
        relevant_log_probs = log_probs[:, num_prompt_tokens:num_prompt_tokens + input_ids.size(1), :]
        
        # æ”¶é›†ç›®æ ‡tokençš„å¯¹æ•°æ¦‚ç‡
        gathered_log_probs = relevant_log_probs.gather(
            dim=-1, 
            index=input_ids.unsqueeze(-1)
        ).squeeze(-1)  # [batch_size, seq_len]
        
        # å¦‚æœæœ‰attention_maskï¼Œåªè®¡ç®—épaddingéƒ¨åˆ†çš„å¹³å‡æ¦‚ç‡
        if attention_mask is not None:
            # å°†paddingä½ç½®çš„æ¦‚ç‡è®¾ä¸º0
            masked_log_probs = gathered_log_probs * attention_mask.float()
            # è®¡ç®—æœ‰æ•ˆtokençš„å¹³å‡å¯¹æ•°æ¦‚ç‡
            valid_lengths = attention_mask.sum(dim=1).clamp(min=1)
            return masked_log_probs.sum(dim=1) / valid_lengths
        else:
            # å¦‚æœæ²¡æœ‰attention_maskï¼Œè®¡ç®—æ‰€æœ‰ä½ç½®çš„å¹³å‡æ¦‚ç‡
            return gathered_log_probs.mean(dim=1)
    
    def save_prompt_embeddings(self, output_dir: str):
        """
        ä¿å­˜è®­ç»ƒå¥½çš„æç¤ºåµŒå…¥
        
        Args:
            output_dir: è¾“å‡ºç›®å½•
        """
        import os
        prompt_path = os.path.join(output_dir, "prompt_embeddings.pt")
        self.model.save_prompt_embeddings(prompt_path)
        print(f"Prompt embeddings saved to {prompt_path}")
    
    def training_step(self, model, inputs, num_items_in_batch=None):
        """
        é‡å†™è®­ç»ƒæ­¥éª¤ä»¥é¿å…optimizer.train()è°ƒç”¨
        """
        model.train()
        inputs = self._prepare_inputs(inputs)

        with self.compute_loss_context_manager():
            loss = self.compute_loss(model, inputs)

        del inputs
        
        if self.args.n_gpu > 1:
            loss = loss.mean()  # mean() to average on multi-gpu parallel training

        if self.use_apex:
            with amp.scale_loss(loss, self.optimizer) as scaled_loss:
                scaled_loss.backward()
        else:
            self.accelerator.backward(loss)

        return loss.detach() / self.args.gradient_accumulation_steps

    def _prepare_inputs(self, inputs):
        """
        é‡å†™_prepare_inputsä»¥å¤„ç†è‡ªå®šä¹‰æ•°æ®æ ¼å¼
        """
        # ç›´æ¥è¿”å›inputsï¼Œå› ä¸ºæˆ‘ä»¬åœ¨compute_lossä¸­å¤„ç†æ•°æ®è½¬æ¢
        return inputs

    def _save(self, output_dir: Optional[str] = None, state_dict=None):
        """
        é‡å†™ä¿å­˜æ–¹æ³•ï¼Œåªä¿å­˜prompt embeddingsè€Œä¸æ˜¯æ•´ä¸ªæ¨¡å‹
        
        ç”±äºæˆ‘ä»¬åªè®­ç»ƒprompt embeddingsï¼ŒåŸºç¡€æ¨¡å‹å‚æ•°è¢«å†»ç»“ï¼Œ
        å› æ­¤åªéœ€è¦ä¿å­˜prompt embeddingså³å¯ï¼Œé¿å…ä¿å­˜å…±äº«tensorçš„é—®é¢˜ã€‚
        """
        # å¦‚æœæ²¡æœ‰æŒ‡å®šè¾“å‡ºç›®å½•ï¼Œä½¿ç”¨é»˜è®¤çš„
        if (output_dir is None):
            output_dir = self.args.output_dir

        os.makedirs(output_dir, exist_ok=True)
        
        # åªä¿å­˜prompt embeddings
        prompt_embeddings_path = os.path.join(output_dir, "prompt_embeddings.pt")
        self.model.save_prompt_embeddings(prompt_embeddings_path)
        
        # ä¿å­˜è®­ç»ƒé…ç½®
        config_path = os.path.join(output_dir, "ptuning_config.json")
        config = {
            "model_name_or_path": self.model.base_model.config.name_or_path if hasattr(self.model.base_model.config, 'name_or_path') else 'unknown',
            "num_virtual_tokens": self.model.num_virtual_tokens,
            "prompt_embedding_dim": self.model.prompt_embedding_dim,
            "margin": self.model.margin,
        }
        
        with open(config_path, 'w') as f:
            import json
            json.dump(config, f, indent=2)
        
        print(f"ğŸ’¾ P-tuning model saved to {output_dir}")
        print(f"   - Prompt embeddings: {prompt_embeddings_path}")
        print(f"   - Config: {config_path}")
    
    def save_model(self, output_dir: Optional[str] = None, _internal_call: bool = False):
        """
        é‡å†™ä¿å­˜æ¨¡å‹æ–¹æ³•ï¼Œåªä¿å­˜prompt embeddings
        """
        self._save(output_dir)
    
    def _save_checkpoint(self, model, trial, metrics=None):
        """
        é‡å†™ä¿å­˜æ£€æŸ¥ç‚¹æ–¹æ³•
        """
        # ç”Ÿæˆæ£€æŸ¥ç‚¹ç›®å½•å
        checkpoint_folder = f"checkpoint-{self.state.global_step}"
        run_dir = self._get_output_dir(trial=trial)
        output_dir = os.path.join(run_dir, checkpoint_folder)
        
        # ä¿å­˜prompt embeddings
        self._save(output_dir)
        
        # ä¿å­˜è®­ç»ƒçŠ¶æ€
        self.state.save_to_json(os.path.join(output_dir, "trainer_state.json"))
        
        # ä¿å­˜è®­ç»ƒå‚æ•°
        torch.save(self.args, os.path.join(output_dir, "training_args.bin"))
        
        # å¦‚æœæ˜¯æœ€ä½³æ¨¡å‹ï¼Œåˆ›å»ºæœ€ä½³æ¨¡å‹çš„æ ‡è®°
        if getattr(self, '_load_best_model_at_end', False) and metrics is not None:
            metric_key = self.args.metric_for_best_model
            if metric_key in metrics:
                best_metric_path = os.path.join(run_dir, "best_metric.txt")
                with open(best_metric_path, "w") as f:
                    f.write(f"{metrics[metric_key]}")
        
        # æ¸…ç†æ—§çš„æ£€æŸ¥ç‚¹
        self._rotate_checkpoints(use_mtime=False, output_dir=run_dir)
