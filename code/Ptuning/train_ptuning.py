import os
import sys
import torch
import json
import wandb
import numpy as np
from dataclasses import dataclass, field
from typing import Optional, List, Dict, Any, Tuple
from transformers import (
    HfArgumentParser,
    TrainingArguments,
    AutoTokenizer,
    AutoModelForCausalLM,
)
from tqdm import tqdm
from sklearn.model_selection import train_test_split

# ç¡®ä¿èƒ½å¤Ÿå¯¼å…¥æœ¬åœ°æ¨¡å—
sys.path.append(os.path.dirname(os.path.abspath(__file__)))
from ptuning_model import PTuningModel
from ptuning_trainer import PTuningTrainer

parent_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.append(parent_dir)

@dataclass
class PTuningArguments:
    """
    P-tuningè®­ç»ƒå‚æ•°é…ç½®ç±»
    
    å®šä¹‰äº†P-tuningè®­ç»ƒè¿‡ç¨‹ä¸­éœ€è¦çš„æ‰€æœ‰å‚æ•°ï¼ŒåŒ…æ‹¬æ¨¡å‹é…ç½®ã€æ•°æ®é›†é…ç½®ã€
    è®­ç»ƒè¶…å‚æ•°ç­‰ã€‚ä½¿ç”¨dataclassè£…é¥°å™¨ç®€åŒ–å‚æ•°ç®¡ç†ã€‚
    """
    model_name_or_path: str = field(
        default="Qwen/Qwen2.5-1.5B",
        metadata={"help": "é¢„è®­ç»ƒæ¨¡å‹è·¯å¾„æˆ–æ¨¡å‹æ ‡è¯†ç¬¦"}
    )
    dataset_name: str = field(
        default="Anthropic/hh-rlhf",
        metadata={"help": "åå¥½å­¦ä¹ æ•°æ®é›†åç§°"}
    )
    num_virtual_tokens: int = field(
        default=50,
        metadata={"help": "P-tuningè™šæ‹Ÿtokenæ•°é‡ï¼Œæ§åˆ¶è½¯æç¤ºçš„é•¿åº¦"}
    )
    prompt_init_method: str = field(
        default="random",
        metadata={"help": "æç¤ºåµŒå…¥åˆå§‹åŒ–æ–¹æ³•ï¼šrandomæˆ–vocab"}
    )
    preference_loss_weight: float = field(
        default=1.0,
        metadata={"help": "åå¥½æŸå¤±æƒé‡ï¼Œç”¨äºå¹³è¡¡ä¸åŒæŸå¤±é¡¹"}
    )
    margin: float = field(
        default=0.1,
        metadata={"help": "åå¥½æ’åºæŸå¤±çš„è¾¹è·å‚æ•°ï¼Œæ§åˆ¶åå¥½å¼ºåº¦"}
    )
    # ğŸ†• æ–°å¢KLæ•£åº¦çº¦æŸå‚æ•°
    kl_loss_weight: float = field(
        default=0.1,
        metadata={"help": "KLæ•£åº¦æŸå¤±æƒé‡ï¼Œç”¨äºçº¦æŸpromptå¯¹åŸæ¨¡å‹åˆ†å¸ƒçš„å½±å“"}
    )
    max_length: int = field(
        default=512,
        metadata={"help": "æœ€å¤§åºåˆ—é•¿åº¦ï¼Œè¶…è¿‡æ­¤é•¿åº¦çš„åºåˆ—å°†è¢«æˆªæ–­"}
    )
    # æ·»åŠ æ–°çš„éªŒè¯å’Œç›‘æ§å‚æ•°
    eval_split_ratio: float = field(
        default=0.1,
        metadata={"help": "éªŒè¯é›†åˆ’åˆ†æ¯”ä¾‹"}
    )
    early_stopping_patience: int = field(
        default=5,
        metadata={"help": "æ—©åœè€å¿ƒå€¼ï¼Œè¿ç»­å¤šå°‘æ¬¡éªŒè¯æ²¡æœ‰æ”¹å–„å°±åœæ­¢"}
    )
    early_stopping_threshold: float = field(
        default=0.01,
        metadata={"help": "æ—©åœé˜ˆå€¼ï¼ŒéªŒè¯æŒ‡æ ‡æ”¹å–„å°äºæ­¤å€¼è®¤ä¸ºæ²¡æœ‰æ”¹å–„"}
    )
    target_accuracy: float = field(
        default=0.9,
        metadata={"help": "ç›®æ ‡å‡†ç¡®ç‡ï¼Œè¾¾åˆ°æ­¤å€¼åå¯ä»¥è€ƒè™‘åœæ­¢è®­ç»ƒ"}
    )
    wandb_project: str = field(
        default="ptuning-preference",
        metadata={"help": "Wandbé¡¹ç›®åç§°"}
    )
    use_wandb: bool = field(
        default=True,
        metadata={"help": "æ˜¯å¦ä½¿ç”¨wandbç›‘æ§"}
    )
    # æ³¨æ„ï¼šsave_steps å’Œ eval_steps å‚æ•°å·²åœ¨ TrainingArguments ä¸­å®šä¹‰ï¼Œä¸éœ€è¦é‡å¤å®šä¹‰
    # ç§»é™¤ output_dir å‚æ•°ï¼Œå› ä¸º TrainingArguments å·²ç»åŒ…å«äº†è¿™ä¸ªå‚æ•°


def prepare_preference_data(examples, tokenizer, max_length):
    """
    å‡†å¤‡åå¥½å­¦ä¹ æ•°æ®
    
    å°†åŸå§‹çš„åå¥½æ•°æ®ï¼ˆchosen/rejectedå¯¹ï¼‰è½¬æ¢ä¸ºæ¨¡å‹è®­ç»ƒæ‰€éœ€çš„æ ¼å¼ï¼Œ
    åŒ…æ‹¬åˆ†è¯ã€å¡«å……ã€æˆªæ–­ç­‰é¢„å¤„ç†æ­¥éª¤ã€‚
    
    Args:
        examples: åŒ…å«chosenå’Œrejectedå­—æ®µçš„æ•°æ®æ ·æœ¬
        tokenizer: åˆ†è¯å™¨
        max_length: æœ€å¤§åºåˆ—é•¿åº¦
        
    Returns:
        å¤„ç†åçš„æ•°æ®å­—å…¸ï¼ŒåŒ…å«åå¥½å’Œéåå¥½å›å¤çš„token IDå’Œæ³¨æ„åŠ›æ©ç 
    """
    preferred_texts = examples["chosen"]    # åå¥½å›å¤æ–‡æœ¬
    rejected_texts = examples["rejected"]   # éåå¥½å›å¤æ–‡æœ¬
    
    # å¯¹åå¥½å›å¤è¿›è¡Œåˆ†è¯å¤„ç†ï¼ˆæ·»åŠ è¿›åº¦æ¡ï¼‰
    print("Processing preferred texts...")
    preferred_encodings = tokenizer(
        preferred_texts,
        truncation=True,           # å¯ç”¨æˆªæ–­
        padding="max_length",      # å¡«å……åˆ°æœ€å¤§é•¿åº¦
        max_length=max_length,     # æœ€å¤§åºåˆ—é•¿åº¦
        return_tensors="pt"        # è¿”å›PyTorchå¼ é‡
    )
    
    # å¯¹éåå¥½å›å¤è¿›è¡Œåˆ†è¯å¤„ç†
    print("Processing rejected texts...")
    rejected_encodings = tokenizer(
        rejected_texts,
        truncation=True,
        padding="max_length",
        max_length=max_length,
        return_tensors="pt"
    )
    
    # è¿”å›æ ¼å¼åŒ–çš„æ•°æ®
    return {
        "preferred_input_ids": preferred_encodings["input_ids"],
        "preferred_attention_mask": preferred_encodings["attention_mask"],
        "rejected_input_ids": rejected_encodings["input_ids"],
        "rejected_attention_mask": rejected_encodings["attention_mask"],
    }


def load_preference_dataset(file_path: str) -> List[Dict[str, str]]:
    """
    åŠ è½½åå¥½æ•°æ®é›†ï¼Œæ”¯æŒJSONLæ ¼å¼
    
    Args:
        file_path: æ•°æ®æ–‡ä»¶è·¯å¾„ï¼Œæ”¯æŒ.jsonlæ ¼å¼
        
    Returns:
        åŒ…å«å­—å…¸çš„åˆ—è¡¨ï¼Œæ¯ä¸ªå­—å…¸åŒ…å«promptã€chosenã€rejectedå­—æ®µ
    """
    print(f"ğŸ“š Loading dataset from: {file_path}")
    
    dataset = []
    
    if not os.path.exists(file_path):
        raise FileNotFoundError(f"Dataset file not found: {file_path}")
    
    file_extension = os.path.splitext(file_path)[1].lower()
    
    if file_extension == '.jsonl':
        # å¤„ç†JSONLæ ¼å¼
        with open(file_path, 'r', encoding='utf-8') as f:
            for line_num, line in enumerate(f, 1):
                line = line.strip()
                if not line:
                    continue
                
                try:
                    data = json.loads(line)
                    
                    # éªŒè¯å¿…éœ€å­—æ®µ
                    if not isinstance(data, dict):
                        print(f"Warning: Line {line_num} is not a dict, skipping")
                        continue
                    
                    # æ£€æŸ¥å¿…éœ€å­—æ®µ
                    required_fields = ['chosen', 'rejected']
                    missing_fields = [field for field in required_fields if field not in data]
                    
                    if missing_fields:
                        print(f"Warning: Line {line_num} missing fields {missing_fields}, skipping")
                        continue
                    
                    # æ ‡å‡†åŒ–æ•°æ®æ ¼å¼
                    sample = {
                        'prompt': str(data.get('prompt', '')),
                        'chosen': str(data['chosen']),
                        'rejected': str(data['rejected'])
                    }
                    
                    # éªŒè¯æ•°æ®ä¸ä¸ºç©º
                    if not sample['chosen'].strip() or not sample['rejected'].strip():
                        print(f"Warning: Line {line_num} has empty chosen/rejected, skipping")
                        continue
                    
                    dataset.append(sample)
                    
                except json.JSONDecodeError as e:
                    print(f"Warning: Line {line_num} is not valid JSON: {e}")
                    continue
                except Exception as e:
                    print(f"Warning: Error processing line {line_num}: {e}")
                    continue
    
    elif file_extension == '.json':
        # å¤„ç†JSONæ ¼å¼
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        if isinstance(data, list):
            # å¦‚æœæ˜¯åˆ—è¡¨æ ¼å¼
            for i, item in enumerate(data):
                if isinstance(item, dict) and 'chosen' in item and 'rejected' in item:
                    sample = {
                        'prompt': str(item.get('prompt', '')),
                        'chosen': str(item['chosen']),
                        'rejected': str(item['rejected'])
                    }
                    dataset.append(sample)
                else:
                    print(f"Warning: Item {i} missing required fields, skipping")
        
        elif isinstance(data, dict):
            # å¦‚æœæ˜¯å­—å…¸æ ¼å¼ï¼ˆç±»ä¼¼defaultdictï¼‰
            for key, item in data.items():
                if isinstance(item, dict) and 'chosen' in item and 'rejected' in item:
                    sample = {
                        'prompt': str(item.get('prompt', '')),
                        'chosen': str(item['chosen']),
                        'rejected': str(item['rejected'])
                    }
                    dataset.append(sample)
                else:
                    print(f"Warning: Item {key} missing required fields, skipping")
    
    else:
        raise ValueError(f"Unsupported file format: {file_extension}. Supported: .json, .jsonl")
    
    print(f"âœ… Successfully loaded {len(dataset)} samples")
    
    # æ˜¾ç¤ºæ ·æœ¬é¢„è§ˆ
    if dataset:
        sample = dataset[0]
        print(f"ğŸ“ Sample preview:")
        print(f"  Prompt: {sample['prompt'][:100]}..." if sample['prompt'] else "  Prompt: (empty)")
        print(f"  Chosen: {sample['chosen'][:100]}...")
        print(f"  Rejected: {sample['rejected'][:100]}...")
    
    return dataset


class PreferenceDataset:
    """
    åå¥½æ•°æ®é›†ç±»ï¼Œç”¨äºP-tuningè®­ç»ƒ
    """
    
    def __init__(self, data: List[Dict[str, str]]):
        """
        åˆå§‹åŒ–æ•°æ®é›†
        
        Args:
            data: åŒ…å«promptã€chosenã€rejectedçš„å­—å…¸åˆ—è¡¨
        """
        self.data = data
    
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx):
        if isinstance(idx, int):
            return self.data[idx]
        elif isinstance(idx, slice):
            return [self.data[i] for i in range(*idx.indices(len(self.data)))]
        else:
            raise TypeError(f"Invalid index type: {type(idx)}")
    
    def __iter__(self):
        return iter(self.data)


def create_data_collator():
    """
    åˆ›å»ºæ•°æ®æ”¶é›†å™¨ï¼Œå¤„ç†æ‰¹æ¬¡æ•°æ®
    
    Returns:
        æ•°æ®æ”¶é›†å™¨å‡½æ•°
    """
    def collate_fn(batch: List[Dict[str, str]]) -> Dict[str, List[str]]:
        """
        å°†ä¸€ä¸ªæ‰¹æ¬¡çš„æ ·æœ¬æ”¶é›†æˆæ‰¹æ¬¡å­—å…¸
        
        Args:
            batch: åŒ…å«promptã€chosenã€rejectedçš„å­—å…¸åˆ—è¡¨
            
        Returns:
            æ‰¹æ¬¡æ•°æ®ï¼ŒåŒ…å«chosenå’Œrejectedçš„æ–‡æœ¬åˆ—è¡¨
        """
        if not batch:
            return {}
        
        # æå–æ‰¹æ¬¡æ•°æ®
        chosen_texts = []
        rejected_texts = []
        prompts = []
        
        for sample in batch:
            # éªŒè¯æ ·æœ¬æ ¼å¼
            if not isinstance(sample, dict):
                raise ValueError(f"Expected dict sample, got {type(sample)}")
            
            if 'chosen' not in sample or 'rejected' not in sample:
                raise ValueError(f"Sample missing required fields. Available: {list(sample.keys())}")
            
            # æå–æ–‡æœ¬
            chosen_texts.append(str(sample['chosen']))
            rejected_texts.append(str(sample['rejected']))
            prompts.append(str(sample.get('prompt', '')))
        
        return {
            'chosen': chosen_texts,
            'rejected': rejected_texts,
            'prompt': prompts
        }
    
    return collate_fn


def split_dataset(dataset: List[Dict[str, str]], eval_ratio: float = 0.1, seed: int = 42) -> Tuple[List[Dict[str, str]], List[Dict[str, str]]]:
    """
    åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†
    
    Args:
        dataset: å®Œæ•´æ•°æ®é›†
        eval_ratio: éªŒè¯é›†æ¯”ä¾‹
        seed: éšæœºç§å­
        
    Returns:
        (train_dataset, eval_dataset)
    """
    if eval_ratio <= 0 or eval_ratio >= 1:
        raise ValueError(f"eval_ratio must be between 0 and 1, got {eval_ratio}")
    
    train_data, eval_data = train_test_split(
        dataset, 
        test_size=eval_ratio, 
        random_state=seed,
        shuffle=True
    )
    
    print(f"ğŸ“Š Dataset split: {len(train_data)} train, {len(eval_data)} eval")
    return train_data, eval_data


def main():
    """
    ä¸»è®­ç»ƒå‡½æ•°
    """
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    parser = HfArgumentParser((PTuningArguments, TrainingArguments))
    ptuning_args, training_args = parser.parse_args_into_dataclasses()
    
    # åœ¨è§£æåç«‹å³è®¾ç½®load_best_model_at_endå’Œç›¸å…³å‚æ•°
    # è¿™æ ·å¯ä»¥é¿å…åˆå§‹åŒ–æ—¶çš„ç­–ç•¥éªŒè¯é”™è¯¯
    training_args.load_best_model_at_end = True
    training_args.evaluation_strategy = "steps"
    training_args.save_strategy = "steps"
    
    # å¦‚æœç”¨æˆ·æ²¡æœ‰æŒ‡å®š save_steps å’Œ eval_stepsï¼Œåˆ™ä½¿ç”¨é»˜è®¤å€¼
    if training_args.save_steps is None or training_args.save_steps == 500:  # 500æ˜¯HFé»˜è®¤å€¼
        training_args.save_steps = 200  # æˆ‘ä»¬çš„é»˜è®¤å€¼
    if training_args.eval_steps is None or training_args.eval_steps == 500:  # 500æ˜¯HFé»˜è®¤å€¼
        training_args.eval_steps = 200  # æˆ‘ä»¬çš„é»˜è®¤å€¼
    
    training_args.metric_for_best_model = "eval_ranking_accuracy"
    training_args.greater_is_better = True
    
    print(f"âœ… Set training strategies: eval={training_args.evaluation_strategy}, save={training_args.save_strategy}")
    print(f"âœ… Set steps: eval_steps={training_args.eval_steps}, save_steps={training_args.save_steps}")
    print(f"âœ… Set load_best_model_at_end: {training_args.load_best_model_at_end}")

    # åˆå§‹åŒ–wandb
    if (ptuning_args.use_wandb):
        wandb.init(
            project=ptuning_args.wandb_project,
            name=f"ptuning-{ptuning_args.model_name_or_path.split('/')[-1]}-{ptuning_args.num_virtual_tokens}tokens",
            config={
                **vars(ptuning_args),
                **vars(training_args)
            }
        )
    
    # åŠ è½½åˆ†è¯å™¨å’ŒåŸºç¡€æ¨¡å‹
    print(f"ğŸ”§ Loading model and tokenizer: {ptuning_args.model_name_or_path}")
    with tqdm(total=2, desc="Loading model components") as pbar:
        tokenizer = AutoTokenizer.from_pretrained(ptuning_args.model_name_or_path)
        pbar.update(1)
        
        base_model = AutoModelForCausalLM.from_pretrained(
            ptuning_args.model_name_or_path,
            torch_dtype=torch.float16,  # ä½¿ç”¨åŠç²¾åº¦ä»¥èŠ‚çœæ˜¾å­˜
            device_map="auto"           # è‡ªåŠ¨è®¾å¤‡æ˜ å°„
        )
        pbar.update(1)
    
    # æ·»åŠ å¡«å……tokenï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
    if (tokenizer.pad_token is None):
        tokenizer.pad_token = tokenizer.eos_token
    
    # åˆ›å»ºP-tuningæ¨¡å‹åŒ…è£…å™¨
    print(f"ğŸ¯ Creating P-tuning model with {ptuning_args.num_virtual_tokens} virtual tokens")
    ptuning_model = PTuningModel(
        base_model=base_model,
        num_virtual_tokens=ptuning_args.num_virtual_tokens,
        init_method=ptuning_args.prompt_init_method,
        margin=ptuning_args.margin  # æ·»åŠ marginå‚æ•°
    )
    
    # è¾“å‡ºå¯è®­ç»ƒå‚æ•°ä¿¡æ¯
    total_params = sum(p.numel() for p in ptuning_model.parameters())
    trainable_params = sum(p.numel() for p in ptuning_model.parameters() if p.requires_grad)
    print(f"ğŸ“Š Total parameters: {total_params:,}")
    print(f"ğŸ”¥ Trainable parameters: {trainable_params:,} ({trainable_params/total_params*100:.2f}%)")
    
    # åŠ è½½æ•°æ®é›† - ä½¿ç”¨æ–°çš„æ•°æ®åŠ è½½æ–¹æ³•
    print(f"ğŸ“š Loading dataset: {ptuning_args.dataset_name}")
    
    try:
        # ç›´æ¥åŠ è½½æ•°æ®
        raw_data = load_preference_dataset(ptuning_args.dataset_name)
        
        # åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†
        train_data, eval_data = split_dataset(raw_data, ptuning_args.eval_split_ratio)
        
        # åˆ›å»ºæ•°æ®é›†å¯¹è±¡
        train_dataset = PreferenceDataset(train_data)
        eval_dataset = PreferenceDataset(eval_data)
        
        print(f"ğŸ“Š Train dataset size: {len(train_dataset)} samples")
        print(f"ğŸ“Š Eval dataset size: {len(eval_dataset)} samples")
        
        # éªŒè¯æ•°æ®é›†
        if len(train_dataset) == 0:
            raise ValueError("Train dataset is empty!")
        if len(eval_dataset) == 0:
            raise ValueError("Eval dataset is empty!")
        
        # æ£€æŸ¥ç¬¬ä¸€ä¸ªæ ·æœ¬
        sample = train_dataset[0]
        print(f"âœ… Dataset loaded successfully")
        print(f"Sample keys: {list(sample.keys())}")
        
    except Exception as e:
        print(f"âŒ Error loading dataset: {e}")
        return
    
    # åˆ›å»ºæ•°æ®æ”¶é›†å™¨
    data_collator = create_data_collator()
    
    # åˆ›å»ºP-tuningè®­ç»ƒå™¨ï¼ˆæ·»åŠ KLçº¦æŸå‚æ•°ï¼‰
    trainer = PTuningTrainer(
        model=ptuning_model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=eval_dataset,  # æ·»åŠ éªŒè¯é›†
        preference_loss_weight=ptuning_args.preference_loss_weight,
        margin=ptuning_args.margin,
        kl_loss_weight=ptuning_args.kl_loss_weight,  # ğŸ†• ä¼ å…¥KLæŸå¤±æƒé‡
        data_collator=data_collator,
        processing_class=tokenizer,
        # æ—©åœå‚æ•°
        early_stopping_patience=ptuning_args.early_stopping_patience,
        early_stopping_threshold=ptuning_args.early_stopping_threshold,
        target_accuracy=ptuning_args.target_accuracy,
        use_wandb=ptuning_args.use_wandb,
    )
    
    # å¼€å§‹è®­ç»ƒ
    print("ğŸš€ Starting P-tuning training...")
    trainer.train()
    
    # åªä¿å­˜prompt embeddingsï¼ˆå› ä¸ºåŸºç¡€æ¨¡å‹å‚æ•°è¢«å†»ç»“ï¼‰
    # P-tuningçš„æ ¸å¿ƒä¼˜åŠ¿ï¼šåªéœ€è¦ä¿å­˜å°‘é‡çš„prompt embeddingå‚æ•°
    print("ğŸ’¾ Saving trained prompt embeddings...")
    
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    os.makedirs(training_args.output_dir, exist_ok=True)
    
    # ä¿å­˜prompt embeddings
    prompt_embeddings_path = os.path.join(training_args.output_dir, "prompt_embeddings.pt")
    ptuning_model.save_prompt_embeddings(prompt_embeddings_path)
    
    # ä¿å­˜è®­ç»ƒé…ç½®ä»¥ä¾¿åç»­åŠ è½½
    config_path = os.path.join(training_args.output_dir, "ptuning_config.json")
    config = {
        "model_name_or_path": ptuning_args.model_name_or_path,
        "num_virtual_tokens": ptuning_args.num_virtual_tokens,
        "prompt_init_method": ptuning_args.prompt_init_method,
        "prompt_embedding_dim": ptuning_model.prompt_embedding_dim,
        "trainable_params": trainable_params,
        "total_params": total_params
    }
    
    with open(config_path, 'w') as f:
        json.dump(config, f, indent=2)
    
    # è®¡ç®—ä¿å­˜çš„æ–‡ä»¶å¤§å°
    embedding_size_mb = os.path.getsize(prompt_embeddings_path) / (1024 * 1024)
    
    print(f"âœ… Training completed!")
    print(f"ğŸ“ Prompt embeddings saved to: {prompt_embeddings_path}")
    print(f"ğŸ“ Config saved to: {config_path}")
    print(f"ğŸ’¾ Embedding file size: {embedding_size_mb:.2f} MB")
    print(f"ğŸ¯ Efficiency: Only {trainable_params:,} parameters need to be saved!")
    print(f"   (vs {total_params:,} parameters for full model fine-tuning)")


if __name__ == "__main__":
    main()
