import os
import sys
import torch
import json
import re
from typing import List, Dict, Optional
from transformers import AutoTokenizer, AutoModelForCausalLM
from tqdm import tqdm
import argparse
import numpy as np
from datetime import datetime

# ç¡®ä¿èƒ½å¤Ÿå¯¼å…¥æœ¬åœ°æ¨¡å—
sys.path.append(os.path.dirname(os.path.abspath(__file__)))
from ptuning_model import PTuningModel

def load_trained_ptuning_model(
    base_model_path: str,
    prompt_embeddings_path: str,
    config_path: Optional[str] = None,
    device: str = "auto"
) -> tuple:
    """
    åŠ è½½è®­ç»ƒå¥½çš„P-tuningæ¨¡å‹
    """
    print(f"ğŸ”§ Loading base model and tokenizer from: {base_model_path}")
    
    # åŠ è½½åŸºç¡€æ¨¡å‹å’Œåˆ†è¯å™¨
    tokenizer = AutoTokenizer.from_pretrained(base_model_path)
    base_model = AutoModelForCausalLM.from_pretrained(
        base_model_path,
        torch_dtype=torch.float16,
        device_map=device
    )
    
    # ğŸš¨ å…³é”®ä¿®å¤ï¼šè®¾ç½®ä¸åŒçš„pad_token
    if tokenizer.pad_token is None:
        # ä¸è¦ä½¿ç”¨eos_tokenä½œä¸ºpad_tokenï¼Œè¿™ä¼šå¯¼è‡´attention maské—®é¢˜
        if tokenizer.unk_token is not None:
            tokenizer.pad_token = tokenizer.unk_token
        else:
            # æ·»åŠ ä¸€ä¸ªæ–°çš„pad token
            tokenizer.add_special_tokens({'pad_token': '<|pad|>'})
            base_model.resize_token_embeddings(len(tokenizer))
    
    print(f"ğŸ”§ Tokenizer config:")
    print(f"   pad_token: {tokenizer.pad_token} (id: {tokenizer.pad_token_id})")
    print(f"   eos_token: {tokenizer.eos_token} (id: {tokenizer.eos_token_id})")
    
    print(f"ğŸ“ Loading P-tuning configuration and embeddings...")
    
    # åŠ è½½P-tuningé…ç½® - ä¿®å¤è¯­æ³•é”™è¯¯
    if config_path and os.path.exists(config_path):
        with open(config_path, 'r') as f:
            config = json.load(f)
        print(f"âœ… Loaded config: {config}")
    else:
        # ä½¿ç”¨é»˜è®¤é…ç½®
        config = {
            "num_virtual_tokens": 50,
            "prompt_embedding_dim": base_model.config.hidden_size,
            "margin": 0.1
        }
        print(f"âš ï¸ Using default config: {config}")
    
    # åˆ›å»ºP-tuningæ¨¡å‹
    ptuning_model = PTuningModel(
        base_model=base_model,
        num_virtual_tokens=config["num_virtual_tokens"],
        prompt_embedding_dim=config.get("prompt_embedding_dim"),
        margin=config.get("margin", 0.1)
    )
    
    # è·å–æ¨¡å‹æ‰€åœ¨çš„è®¾å¤‡å’Œæ•°æ®ç±»å‹
    model_device = next(ptuning_model.parameters()).device
    model_dtype = next(ptuning_model.parameters()).dtype
    print(f"ğŸ¯ Model device: {model_device}")
    print(f"ğŸ¯ Model dtype: {model_dtype}")
    
    # åŠ è½½è®­ç»ƒå¥½çš„prompt embeddingså¹¶ç¡®ä¿åœ¨æ­£ç¡®è®¾å¤‡å’Œæ•°æ®ç±»å‹ä¸Š
    print(f"ğŸ“¥ Loading prompt embeddings from: {prompt_embeddings_path}")
    state_dict = torch.load(prompt_embeddings_path, map_location=model_device)
    ptuning_model.prompt_embeddings.load_state_dict(state_dict)
    
    # ç¡®ä¿prompt embeddingsåœ¨æ­£ç¡®è®¾å¤‡å’Œæ•°æ®ç±»å‹ä¸Š
    ptuning_model.prompt_embeddings = ptuning_model.prompt_embeddings.to(device=model_device, dtype=model_dtype)
    
    # éªŒè¯è®¾å¤‡å’Œæ•°æ®ç±»å‹ä¸€è‡´æ€§
    prompt_device = ptuning_model.prompt_embeddings.weight.device
    prompt_dtype = ptuning_model.prompt_embeddings.weight.dtype
    print(f"âœ… Prompt embeddings loaded to device: {prompt_device}, dtype: {prompt_dtype}")
    
    if model_device != prompt_device or model_dtype != prompt_dtype:
        print(f"âš ï¸ Device/dtype mismatch detected! Moving prompt embeddings to {model_device}, {model_dtype}")
        ptuning_model.prompt_embeddings = ptuning_model.prompt_embeddings.to(device=model_device, dtype=model_dtype)
        print(f"âœ… Fixed device/dtype mismatch")
    
    # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
    ptuning_model.eval()
    
    print(f"âœ… P-tuning model loaded successfully!")
    print(f"ğŸ“Š Virtual tokens: {config['num_virtual_tokens']}")
    print(f"ğŸ“Š Embedding dimension: {config.get('prompt_embedding_dim', 'auto')}")
    
    return ptuning_model, tokenizer

def generate_with_ptuning(
    model: PTuningModel,
    tokenizer,
    input_text: str,
    max_length: int = 2048,  # ğŸš¨ å¢å¤§æœ€å¤§é•¿åº¦é™åˆ¶ï¼Œä½†ä¸å¼ºåˆ¶ä½¿ç”¨
    temperature: float = 0.7,
    do_sample: bool = True,
    top_p: float = 0.9,
    top_k: int = 50,
    repetition_penalty: float = 1.1,
    pad_token_id: Optional[int] = None,
    # ğŸ†• ç§»é™¤max_new_tokensç­‰é™åˆ¶å‚æ•°
    **kwargs  # æ¥æ”¶å…¶ä»–å‚æ•°ä½†ä¸ä½¿ç”¨
) -> str:
    """
    ä½¿ç”¨P-tuningæ¨¡å‹ç”Ÿæˆæ–‡æœ¬ - è®©æ¨¡å‹è‡ªç„¶ç”Ÿæˆåˆ°EOS token
    
    Args:
        max_length: åºåˆ—æ€»é•¿åº¦ä¸Šé™ï¼ˆåŒ…æ‹¬è¾“å…¥ï¼‰ï¼Œé˜²æ­¢æ— é™ç”Ÿæˆ
        temperature: é‡‡æ ·æ¸©åº¦
        do_sample: æ˜¯å¦ä½¿ç”¨é‡‡æ ·
        top_p: nucleusé‡‡æ ·å‚æ•°
        top_k: top-ké‡‡æ ·å‚æ•°
        repetition_penalty: é‡å¤æƒ©ç½š
        pad_token_id: padding token ID
    """
    device = next(model.parameters()).device
    dtype = next(model.parameters()).dtype
    
    # ç¡®ä¿prompt embeddingsåœ¨æ­£ç¡®è®¾å¤‡å’Œæ•°æ®ç±»å‹ä¸Š
    if (model.prompt_embeddings.weight.device != device or 
        model.prompt_embeddings.weight.dtype != dtype):
        print(f"âš ï¸ Moving prompt embeddings from {model.prompt_embeddings.weight.device}:{model.prompt_embeddings.weight.dtype} to {device}:{dtype}")
        model.prompt_embeddings = model.prompt_embeddings.to(device=device, dtype=dtype)
    
    # ç¼–ç è¾“å…¥æ–‡æœ¬
    encoding = tokenizer(
        input_text,
        return_tensors="pt",
        add_special_tokens=True,
        padding=False,
        truncation=False  # ğŸš¨ ä¸æˆªæ–­è¾“å…¥
    )
    
    input_ids = encoding["input_ids"].to(device)
    input_attention_mask = encoding["attention_mask"].to(device)
    
    print(f"ğŸ“ P-tuning input: {input_text[:100]}{'...' if len(input_text) > 100 else ''}")
    print(f"ğŸ”¢ P-tuning input length: {input_ids.size(1)} tokens")
    print(f"ğŸ”¢ Prompt tokens: {model.num_virtual_tokens} tokens")
    
    batch_size = input_ids.size(0)
    # è·å–input embeddings
    input_embeddings = model.base_model.get_input_embeddings()(input_ids)
    
    # è·å–prompt embeddings
    prompt_embeddings = model.get_prompt_embeddings(batch_size)
    
    # æ‹¼æ¥ embeddings: [prompt_embeddings | input_embeddings]
    inputs_embeds = torch.cat([prompt_embeddings, input_embeddings], dim=1)
    
    # åˆ›å»ºå®Œæ•´çš„attention mask [prompt_mask | input_mask]
    prompt_attention_mask = torch.ones(
        batch_size, model.num_virtual_tokens,
        dtype=input_attention_mask.dtype,
        device=device
    )
    
    # æ‹¼æ¥attention mask
    full_attention_mask = torch.cat([prompt_attention_mask, input_attention_mask], dim=1)
    
    print(f"ğŸ“Š Combined embeddings shape: {inputs_embeds.shape}")
    print(f"ğŸ“Š Full attention mask shape: {full_attention_mask.shape}")
    
    # è®¾ç½®ç”Ÿæˆå‚æ•°
    if pad_token_id is None:
        pad_token_id = tokenizer.pad_token_id if tokenizer.pad_token_id is not None else tokenizer.eos_token_id
    
    # ğŸš¨ å…³é”®ä¿®æ”¹ï¼šç§»é™¤æ‰€æœ‰é™åˆ¶æ€§å‚æ•°ï¼Œè®©æ¨¡å‹è‡ªç„¶ç”Ÿæˆ
    generate_kwargs = {
        "inputs_embeds": inputs_embeds,
        "attention_mask": full_attention_mask,
        "do_sample": do_sample,
        "temperature": temperature,
        "top_p": top_p,
        "top_k": top_k,
        "repetition_penalty": repetition_penalty,
        "pad_token_id": pad_token_id,
        "eos_token_id": tokenizer.eos_token_id,
        "use_cache": True,
        # ğŸš¨ å…³é”®ï¼šåªè®¾ç½®æ€»é•¿åº¦ä¸Šé™ï¼Œä¸è®¾ç½®æ–°ç”Ÿæˆçš„tokenæ•°é‡é™åˆ¶
        "max_length": max_length,  # æ€»åºåˆ—é•¿åº¦ä¸Šé™
        # ğŸš¨ ç§»é™¤è¿™äº›é™åˆ¶æ€§å‚æ•°ï¼š
        # "max_new_tokens": ä¸è®¾ç½®ï¼Œè®©æ¨¡å‹è‡ªç”±ç”Ÿæˆ
        # "min_new_tokens": ä¸è®¾ç½®æœ€å°ç”Ÿæˆæ•°
        # "early_stopping": ä½¿ç”¨é»˜è®¤è¡Œä¸º
    }
    
    print(f"ğŸ”§ P-tuning generation mode: Natural ending (no token limits)")
    print(f"ğŸ”§ P-tuning generation parameters:")
    for key, value in generate_kwargs.items():
        if key not in ["inputs_embeds", "attention_mask"]:
            print(f"   {key}: {value}")
    
    # ç”Ÿæˆ
    with torch.no_grad():
        output_ids = model.base_model.generate(**generate_kwargs)
    
    print(f"ğŸ”¢ P-tuning generated output shape: {output_ids.shape}")
    
    # ğŸš¨ å…³é”®ä¿®å¤ï¼šmodel.generate()è¿”å›çš„æ˜¯å®Œæ•´çš„ç”Ÿæˆåºåˆ—
    # å½“ä½¿ç”¨inputs_embedsæ—¶ï¼Œè¾“å‡ºç»“æ„æ˜¯ï¼š[æ–°ç”Ÿæˆçš„tokens]
    generated_ids = output_ids[0]  # ç›´æ¥ä½¿ç”¨æ‰€æœ‰ç”Ÿæˆçš„token
    
    print(f"ğŸ”¢ P-tuning analysis:")
    print(f"   Generated total length: {len(generated_ids)} tokens")
    
    # æ£€æŸ¥ç”Ÿæˆç»“æŸåŸå› 
    if len(generated_ids) > 0:
        last_token = generated_ids[-1].item()
        if last_token == tokenizer.eos_token_id:
            print(f"âœ… P-tuning generation ended naturally with EOS token")
        elif len(generated_ids) >= max_length:
            print(f"âš ï¸ P-tuning generation reached max_length limit ({max_length})")
        else:
            print(f"â„¹ï¸ P-tuning generation ended (reason: unknown)")
    
    # è§£ç ç”Ÿæˆçš„æ–‡æœ¬
    generated_text = tokenizer.decode(generated_ids, skip_special_tokens=True)
    
    print(f"ğŸ¯ P-tuning final generated text length: {len(generated_text)} chars")
    
    return generated_text

def debug_model_state(model: PTuningModel, tokenizer, test_input: str = "Hello, how are you?"):
    """
    è°ƒè¯•æ¨¡å‹çŠ¶æ€ - å¢å¼ºç‰ˆæœ¬ï¼ŒåŒ…æ‹¬prompt embeddingsè§£ç 
    """
    print("ğŸ” Debugging model state...")
    
    device = next(model.parameters()).device
    
    # æ£€æŸ¥prompt embeddings
    prompt_emb = model.prompt_embeddings.weight
    print(f"ğŸ“Š Prompt embeddings shape: {prompt_emb.shape}")
    print(f"ğŸ“Š Prompt embeddings device: {prompt_emb.device}")
    print(f"ğŸ“Š Prompt embeddings dtype: {prompt_emb.dtype}")
    print(f"ğŸ“Š Prompt embeddings mean: {prompt_emb.mean().item():.6f}")
    print(f"ğŸ“Š Prompt embeddings std: {prompt_emb.std().item():.6f}")
    print(f"ğŸ“Š Prompt embeddings min: {prompt_emb.min().item():.6f}")
    print(f"ğŸ“Š Prompt embeddings max: {prompt_emb.max().item():.6f}")
    
    # ğŸ†• è§£ç prompt embeddings - æ‰¾åˆ°æœ€ç›¸ä¼¼çš„è¯æ±‡
    print(f"\nğŸ”¤ Decoding prompt embeddings to nearest tokens:")
    decode_prompt_embeddings(model, tokenizer)
    
    # ğŸš¨ å…³é”®æ£€æŸ¥ï¼šprompt embeddingsçš„æ•°å€¼èŒƒå›´
    mean_val = abs(prompt_emb.mean().item())
    std_val = prompt_emb.std().item()
    
    if mean_val > 1.0:
        print(f"âš ï¸ Warning: Prompt embeddings mean ({mean_val:.4f}) is large, might be overtrained!")
    
    if std_val > 2.0:
        print(f"âš ï¸ Warning: Prompt embeddings std ({std_val:.4f}) is large, might be overtrained!")
    
    if std_val < 0.01:
        print(f"âš ï¸ Warning: Prompt embeddings std ({std_val:.4f}) is too small, might be undertrained!")
    
    # æ£€æŸ¥æ˜¯å¦æœ‰å¼‚å¸¸å€¼
    if torch.isnan(prompt_emb).any():
        print("âŒ Found NaN in prompt embeddings!")
    if torch.isinf(prompt_emb).any():
        print("âŒ Found Inf in prompt embeddings!")
    
    # æ£€æŸ¥æ•°å€¼åˆ†å¸ƒ
    emb_flat = prompt_emb.flatten().float()
    percentiles = torch.quantile(emb_flat, torch.tensor([0.1, 0.25, 0.5, 0.75, 0.9]).to(device))
    print(f"ğŸ“Š Prompt embeddings percentiles:")
    print(f"   10%: {percentiles[0].item():.4f}")
    print(f"   25%: {percentiles[1].item():.4f}")
    print(f"   50%: {percentiles[2].item():.4f}")
    print(f"   75%: {percentiles[3].item():.4f}")
    print(f"   90%: {percentiles[4].item():.4f}")
    
    # ğŸš¨ æ–°å¢ï¼šæ£€æŸ¥tokenizeré…ç½®
    print(f"\nğŸ”§ Tokenizer configuration:")
    print(f"   Vocab size: {len(tokenizer)}")
    print(f"   BOS token: {tokenizer.bos_token} (id: {tokenizer.bos_token_id})")
    print(f"   EOS token: {tokenizer.eos_token} (id: {tokenizer.eos_token_id})")
    print(f"   PAD token: {tokenizer.pad_token} (id: {tokenizer.pad_token_id})")
    print(f"   UNK token: {tokenizer.unk_token} (id: {tokenizer.unk_token_id})")
    
    # æµ‹è¯•ç¼–ç å’Œæ³¨æ„åŠ›æ©ç åˆ›å»º
    encoding = tokenizer(test_input, return_tensors="pt", add_special_tokens=True)
    input_ids = encoding["input_ids"].to(device)
    attention_mask = encoding["attention_mask"].to(device)
    
    print(f"\nğŸ”¢ Test encoding:")
    print(f"   Input: '{test_input}'")
    print(f"   Input IDs: {input_ids.tolist()[0]}")
    print(f"   Attention mask: {attention_mask.tolist()[0]}")
    print(f"   Decoded: '{tokenizer.decode(input_ids[0])}'")
    
    with torch.no_grad():
        # è·å–è¾“å…¥embedding
        input_emb = model.base_model.get_input_embeddings()(input_ids)
        print(f"ğŸ“Š Input embeddings shape: {input_emb.shape}")
        print(f"ğŸ“Š Input embeddings mean: {input_emb.mean().item():.6f}")
        print(f"ğŸ“Š Input embeddings std: {input_emb.std().item():.6f}")
        
        # è·å–prompt embedding
        prompt_emb_batch = model.get_prompt_embeddings(1)
        print(f"ğŸ“Š Prompt embeddings batch shape: {prompt_emb_batch.shape}")
        print(f"ğŸ“Š Prompt embeddings batch mean: {prompt_emb_batch.mean().item():.6f}")
        print(f"ğŸ“Š Prompt embeddings batch std: {prompt_emb_batch.std().item():.6f}")
        
        # æ£€æŸ¥æ‹¼æ¥åçš„embedding
        combined_emb = torch.cat([prompt_emb_batch, input_emb], dim=1)
        print(f"ğŸ“Š Combined embeddings shape: {combined_emb.shape}")
        print(f"ğŸ“Š Combined embeddings mean: {combined_emb.mean().item():.6f}")
        print(f"ğŸ“Š Combined embeddings std: {combined_emb.std().item():.6f}")
        
        # ğŸš¨ å…³é”®æ£€æŸ¥ï¼šæ¯”è¾ƒpromptå’Œinput embeddingçš„æ•°å€¼èŒƒå›´
        prompt_range = prompt_emb_batch.max() - prompt_emb_batch.min()
        input_range = input_emb.max() - input_emb.min()
        print(f"ğŸ“Š Prompt embeddings range: {prompt_range.item():.4f}")
        print(f"ğŸ“Š Input embeddings range: {input_range.item():.4f}")
        
        if input_range.item() > 0:
            ratio = (prompt_range/input_range).item()
            print(f"ğŸ“Š Range ratio (prompt/input): {ratio:.4f}")
            
            if ratio > 10:
                print("âš ï¸ Warning: Prompt embeddings have much larger range than input embeddings!")
                print("   This might cause the model to ignore input and only focus on prompts.")
            elif ratio < 0.1:
                print("âš ï¸ Warning: Prompt embeddings have much smaller range than input embeddings!")
                print("   Prompt might not have enough influence.")
        
        # ğŸš¨ æ–°å¢ï¼šæµ‹è¯•attention maskåˆ›å»º
        print(f"\nğŸ” Testing attention mask creation:")
        prompt_mask = torch.ones(1, model.num_virtual_tokens, dtype=attention_mask.dtype, device=device)
        full_mask = torch.cat([prompt_mask, attention_mask], dim=1)
        
        print(f"   Prompt mask shape: {prompt_mask.shape}")
        print(f"   Input mask shape: {attention_mask.shape}")
        print(f"   Full mask shape: {full_mask.shape}")
        print(f"   Full mask: {full_mask.tolist()[0]}")
        
        expected_length = model.num_virtual_tokens + input_ids.size(1)
        if full_mask.size(1) != expected_length:
            print(f"âŒ Attention mask length mismatch! Expected: {expected_length}, Got: {full_mask.size(1)}")
        else:
            print(f"âœ… Attention mask length correct: {full_mask.size(1)}")

def decode_prompt_embeddings(model: PTuningModel, tokenizer, top_k: int = 5):
    """
    å°†prompt embeddingsè§£ç ä¸ºæœ€ç›¸ä¼¼çš„è¯æ±‡tokens
    
    Args:
        model: P-tuningæ¨¡å‹
        tokenizer: åˆ†è¯å™¨
        top_k: æ˜¾ç¤ºæ¯ä¸ªprompt tokençš„top-kæœ€ç›¸ä¼¼è¯æ±‡
    """
    print(f"ğŸ” Finding nearest vocabulary tokens for each prompt embedding...")
    
    device = next(model.parameters()).device
    
    # è·å–è¯æ±‡è¡¨embeddings
    vocab_embeddings = model.base_model.get_input_embeddings().weight  # [vocab_size, hidden_dim]
    prompt_embeddings = model.prompt_embeddings.weight  # [num_virtual_tokens, hidden_dim]
    
    print(f"ğŸ“Š Vocabulary embeddings shape: {vocab_embeddings.shape}")
    print(f"ğŸ“Š Prompt embeddings shape: {prompt_embeddings.shape}")
    
    # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
    # å½’ä¸€åŒ–embeddings
    vocab_embeddings_norm = torch.nn.functional.normalize(vocab_embeddings, dim=1)
    prompt_embeddings_norm = torch.nn.functional.normalize(prompt_embeddings, dim=1)
    
    # è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ [num_virtual_tokens, vocab_size]
    similarity_matrix = torch.mm(prompt_embeddings_norm, vocab_embeddings_norm.t())
    
    print(f"ğŸ“Š Similarity matrix shape: {similarity_matrix.shape}")
    
    # ä¸ºæ¯ä¸ªprompt tokenæ‰¾åˆ°æœ€ç›¸ä¼¼çš„è¯æ±‡
    print(f"\nğŸ”¤ Top-{top_k} nearest tokens for each prompt position:")
    print("=" * 80)
    
    for i in range(prompt_embeddings.size(0)):
        # è·å–ç¬¬iä¸ªprompt tokençš„ç›¸ä¼¼åº¦
        similarities = similarity_matrix[i]  # [vocab_size]
        
        # æ‰¾åˆ°top-kæœ€ç›¸ä¼¼çš„token
        top_similarities, top_indices = torch.topk(similarities, top_k)
        
        print(f"Prompt Token {i:2d}:")
        
        # è§£ç å¹¶æ˜¾ç¤º
        for j, (sim_score, token_id) in enumerate(zip(top_similarities, top_indices)):
            # è§£ç token
            try:
                token_text = tokenizer.decode([token_id.item()])
                # å¤„ç†ç‰¹æ®Šå­—ç¬¦æ˜¾ç¤º
                if token_text.strip() == '':
                    token_text = '<SPACE>'
                elif token_text == '\n':
                    token_text = '<NEWLINE>'
                elif token_text == '\t':
                    token_text = '<TAB>'
                elif len(token_text.strip()) == 0:
                    token_text = '<WHITESPACE>'
                
                print(f"  {j+1}. Token {token_id.item():5d}: '{token_text:15s}' (sim: {sim_score.item():.4f})")
            except Exception as e:
                print(f"  {j+1}. Token {token_id.item():5d}: <DECODE_ERROR> (sim: {sim_score.item():.4f})")
        
        print()
        
        # æ¯10ä¸ªprompt tokenåˆ†ç»„æ˜¾ç¤ºï¼Œé¿å…è¾“å‡ºè¿‡é•¿
        if (i + 1) % 10 == 0 and i < prompt_embeddings.size(0) - 1:
            print("-" * 40)
    
    print("=" * 80)
    
    # ğŸ†• é¢å¤–åˆ†æï¼šprompt embeddingsçš„èšç±»ç‰¹å¾
    analyze_prompt_clustering(model, tokenizer, similarity_matrix)

def analyze_prompt_clustering(model: PTuningModel, tokenizer, similarity_matrix: torch.Tensor):
    """
    åˆ†æprompt embeddingsçš„èšç±»ç‰¹å¾
    
    Args:
        model: P-tuningæ¨¡å‹
        tokenizer: åˆ†è¯å™¨
        similarity_matrix: ç›¸ä¼¼åº¦çŸ©é˜µ [num_virtual_tokens, vocab_size]
    """
    print(f"\nğŸ“Š Analyzing prompt embedding clustering patterns...")
    
    # 1. åˆ†ææ¯ä¸ªprompt tokenæœ€ç›¸ä¼¼çš„è¯æ±‡ç±»å‹
    print(f"\nğŸ·ï¸ Most similar token types for each prompt position:")
    
    # è·å–æ¯ä¸ªprompt tokenæœ€ç›¸ä¼¼çš„token
    _, top_indices = torch.topk(similarity_matrix, 1, dim=1)  # [num_virtual_tokens, 1]
    
    # æŒ‰ç±»å‹åˆ†ç»„
    token_types = {
        'punctuation': [],
        'letters': [],
        'numbers': [],
        'special': [],
        'chinese': [],
        'other': []
    }
    
    for i, token_id in enumerate(top_indices.flatten()):
        try:
            token_text = tokenizer.decode([token_id.item()]).strip()
            
            if not token_text:
                token_types['special'].append(i)
            elif token_text.isalpha():
                token_types['letters'].append(i)
            elif token_text.isdigit():
                token_types['numbers'].append(i)
            elif token_text in '.,!?;:()[]{}"\'-':
                token_types['punctuation'].append(i)
            elif any('\u4e00' <= char <= '\u9fff' for char in token_text):
                token_types['chinese'].append(i)
            else:
                token_types['other'].append(i)
        except:
            token_types['special'].append(i)
    
    for token_type, positions in token_types.items():
        if positions:
            print(f"  {token_type:12s}: {len(positions):2d} tokens at positions {positions[:10]}{'...' if len(positions) > 10 else ''}")
    
    # 2. åˆ†æpromptå†…éƒ¨ç›¸ä¼¼åº¦
    print(f"\nğŸ”— Internal prompt similarity analysis:")
    prompt_embeddings = model.prompt_embeddings.weight
    prompt_embeddings_norm = torch.nn.functional.normalize(prompt_embeddings, dim=1)
    
    # è®¡ç®—prompt tokensä¹‹é—´çš„ç›¸ä¼¼åº¦
    internal_similarity = torch.mm(prompt_embeddings_norm, prompt_embeddings_norm.t())
    
    # æ’é™¤å¯¹è§’çº¿
    mask = ~torch.eye(internal_similarity.size(0), dtype=torch.bool, device=internal_similarity.device)
    off_diagonal = internal_similarity[mask]
    
    print(f"  Mean internal similarity: {off_diagonal.mean().item():.4f}")
    print(f"  Std internal similarity:  {off_diagonal.std().item():.4f}")
    print(f"  Min internal similarity:  {off_diagonal.min().item():.4f}")
    print(f"  Max internal similarity:  {off_diagonal.max().item():.4f}")
    
    # 3. æ‰¾åˆ°æœ€ç›¸ä¼¼å’Œæœ€ä¸ç›¸ä¼¼çš„prompt tokenå¯¹
    internal_similarity_flat = internal_similarity[mask]
    max_sim_idx = torch.argmax(internal_similarity_flat)
    min_sim_idx = torch.argmin(internal_similarity_flat)
    
    # å°†å¹³å¦ç´¢å¼•è½¬æ¢ä¸º2Dç´¢å¼•
    num_tokens = internal_similarity.size(0)
    max_i, max_j = divmod(max_sim_idx.item(), num_tokens - 1)
    if max_j >= max_i:
        max_j += 1
    
    min_i, min_j = divmod(min_sim_idx.item(), num_tokens - 1)
    if min_j >= min_i:
        min_j += 1
    
    print(f"  Most similar pair:     Prompt[{max_i:2d}] â†” Prompt[{max_j:2d}] (sim: {internal_similarity[max_i, max_j].item():.4f})")
    print(f"  Least similar pair:    Prompt[{min_i:2d}] â†” Prompt[{min_j:2d}] (sim: {internal_similarity[min_i, min_j].item():.4f})")

def generate_prompt_summary(model: PTuningModel, tokenizer):
    """
    ç”Ÿæˆprompt embeddingsçš„ç®€æ´æ‘˜è¦
    
    Args:
        model: P-tuningæ¨¡å‹
        tokenizer: åˆ†è¯å™¨
    """
    print(f"\nğŸ“‹ Prompt Embeddings Summary:")
    print("=" * 50)
    
    device = next(model.parameters()).device
    
    # è·å–è¯æ±‡è¡¨embeddings
    vocab_embeddings = model.base_model.get_input_embeddings().weight
    prompt_embeddings = model.prompt_embeddings.weight
    
    # è®¡ç®—ç›¸ä¼¼åº¦
    vocab_embeddings_norm = torch.nn.functional.normalize(vocab_embeddings, dim=1)
    prompt_embeddings_norm = torch.nn.functional.normalize(prompt_embeddings, dim=1)
    similarity_matrix = torch.mm(prompt_embeddings_norm, vocab_embeddings_norm.t())
    
    # è·å–æ¯ä¸ªprompt tokenæœ€ç›¸ä¼¼çš„è¯
    _, top_indices = torch.topk(similarity_matrix, 1, dim=1)
    
    # åˆ›å»ºç®€æ´çš„è¡¨ç¤º
    prompt_tokens = []
    for i, token_id in enumerate(top_indices.flatten()):
        try:
            token_text = tokenizer.decode([token_id.item()]).strip()
            if not token_text or len(token_text) > 10:
                token_text = f"[{token_id.item()}]"
            prompt_tokens.append(token_text)
        except:
            prompt_tokens.append(f"[{token_id.item()}]")
    
    # åˆ†è¡Œæ˜¾ç¤ºï¼Œæ¯è¡Œ10ä¸ªtoken
    print("Learned prompt sequence (nearest vocabulary tokens):")
    for i in range(0, len(prompt_tokens), 10):
        line_tokens = prompt_tokens[i:i+10]
        positions = f"[{i:2d}-{min(i+9, len(prompt_tokens)-1):2d}]"
        tokens_str = " ".join(f"{token:>8s}" for token in line_tokens)
        print(f"  {positions}: {tokens_str}")
    
    print("=" * 50)

def generate_with_detailed_logging(
    model: PTuningModel,
    tokenizer,
    input_text: str,
    max_new_tokens: int = 50,
    **kwargs
) -> str:
    """
    å¸¦è¯¦ç»†æ—¥å¿—çš„ç”Ÿæˆå‡½æ•° - ä½¿ç”¨ä¸æ­£å¸¸ç”Ÿæˆå®Œå…¨ç›¸åŒçš„é€»è¾‘
    """
    print(f"ğŸ” Debug mode: Using same logic as generate_with_ptuning")
    
    # ğŸš¨ å…³é”®ä¿®å¤ï¼šä½¿ç”¨å®Œå…¨ç›¸åŒçš„ç”Ÿæˆé€»è¾‘
    return generate_with_ptuning(
        model=model,
        tokenizer=tokenizer,
        input_text=input_text,
        max_new_tokens=max_new_tokens,
        temperature=0.7,
        do_sample=True,
        top_p=0.9,
        top_k=50,
        **kwargs
    )

def check_prompt_embedding_quality(model: PTuningModel, tokenizer):
    """
    æ£€æŸ¥prompt embeddingçš„è´¨é‡
    """
    print("\nğŸ” Checking prompt embedding quality...")
    
    # æµ‹è¯•ä¸åŒè¾“å…¥çš„ç”Ÿæˆä¸€è‡´æ€§
    test_inputs = [
        "Hello",
        "Hi there",
        "Good morning",
        "How are you?"
    ]
    
    device = next(model.parameters()).device
    
    # æ£€æŸ¥å¯¹äºä¸åŒè¾“å…¥ï¼Œpromptæ˜¯å¦äº§ç”Ÿç±»ä¼¼çš„å½±å“
    prompt_influences = []
    
    with torch.no_grad():
        for test_input in test_inputs:
            encoding = tokenizer(test_input, return_tensors="pt", add_special_tokens=True)
            input_ids = encoding["input_ids"].to(device)
            
            # è·å–æœ‰promptå’Œæ— promptçš„è¾“å‡ºlogits
            input_emb = model.base_model.get_input_embeddings()(input_ids)
            prompt_emb = model.get_prompt_embeddings(1)
            
            # æœ‰promptçš„æƒ…å†µ
            combined_emb = torch.cat([prompt_emb, input_emb], dim=1)
            with_prompt_output = model.base_model(inputs_embeds=combined_emb)
            
            # è®°å½•promptçš„å½±å“
            logits_mean = with_prompt_output.logits.mean().item()
            prompt_influences.append(logits_mean)
            
    print(f"ğŸ“Š Prompt influence consistency:")
    print(f"   Mean logits: {prompt_influences}")
    print(f"   Std deviation: {np.std(prompt_influences):.6f}")
    
    if np.std(prompt_influences) > 1.0:
        print("âš ï¸ Warning: Prompt influence varies significantly across inputs!")
        print("   This suggests prompt embeddings might be overtrained or unstable.")
    
    # ğŸš¨ æ·»åŠ æ›´è¯¦ç»†çš„prompt embeddingåˆ†æ
    prompt_emb = model.prompt_embeddings.weight
    
    # æ£€æŸ¥prompt embeddingçš„ç›¸ä¼¼æ€§
    print(f"\nğŸ“Š Prompt embedding similarity analysis:")
    
    # è®¡ç®—prompt tokensä¹‹é—´çš„ç›¸ä¼¼æ€§
    prompt_emb_norm = torch.nn.functional.normalize(prompt_emb, dim=1)
    similarity_matrix = torch.mm(prompt_emb_norm, prompt_emb_norm.t())
    
    # æ’é™¤å¯¹è§’çº¿ï¼ˆè‡ªå·±ä¸è‡ªå·±çš„ç›¸ä¼¼æ€§ï¼‰
    mask = ~torch.eye(similarity_matrix.size(0), dtype=torch.bool, device=device)
    off_diagonal_similarities = similarity_matrix[mask]
    
    mean_similarity = off_diagonal_similarities.mean().item()
    std_similarity = off_diagonal_similarities.std().item()
    
    print(f"   Mean similarity between prompt tokens: {mean_similarity:.4f}")
    print(f"   Std similarity: {std_similarity:.4f}")
    
    if mean_similarity > 0.9:
        print("âš ï¸ Warning: Prompt tokens are too similar! This reduces diversity.")
    elif mean_similarity < 0.1:
        print("âš ï¸ Warning: Prompt tokens are too different! This might cause instability.")
    else:
        print("âœ… Prompt token similarity looks good.")

def batch_generate_with_ptuning(
    model: PTuningModel,
    tokenizer,
    input_texts: List[str],
    **generate_kwargs
) -> List[str]:
    """
    æ‰¹é‡ç”Ÿæˆæ–‡æœ¬
    
    Args:
        model: P-tuningæ¨¡å‹
        tokenizer: åˆ†è¯å™¨
        input_texts: è¾“å…¥æ–‡æœ¬åˆ—è¡¨
        **generate_kwargs: ç”Ÿæˆå‚æ•°
        
    Returns:
        ç”Ÿæˆæ–‡æœ¬åˆ—è¡¨
    """
    generated_texts = []
    
    print(f"ğŸ”„ Batch generating for {len(input_texts)} inputs...")
    
    for i, input_text in enumerate(tqdm(input_texts, desc="Generating")):
        try:
            generated_text = generate_with_ptuning(
                model, tokenizer, input_text, **generate_kwargs
            )
            generated_texts.append(generated_text)
            
            print(f"\nğŸ“‹ Sample {i+1}/{len(input_texts)}:")
            print(f"Input: {input_text[:100]}{'...' if len(input_text) > 100 else ''}")
            print(f"Output: {generated_text[:200]}{'...' if len(generated_text) > 200 else ''}")
            print("-" * 80)
            
        except Exception as e:
            print(f"âŒ Error generating for input {i+1}: {e}")
            generated_texts.append("")
    
    return generated_texts

def load_test_dataset(file_path: str, num_samples: Optional[int] = None) -> List[str]:
    """
    åŠ è½½æµ‹è¯•æ•°æ®é›†
    
    Args:
        file_path: æ•°æ®æ–‡ä»¶è·¯å¾„
        num_samples: é‡‡æ ·æ•°é‡ï¼ŒNoneè¡¨ç¤ºåŠ è½½å…¨éƒ¨
        
    Returns:
        è¾“å…¥æ–‡æœ¬åˆ—è¡¨
    """
    print(f"ğŸ“š Loading test dataset from: {file_path}")
    
    inputs = []
    
    if (file_path.endswith('.jsonl')):
        with open(file_path, 'r', encoding='utf-8') as f:
            for line_num, line in enumerate(f, 1):
                if num_samples and len(inputs) >= num_samples:
                    break
                    
                line = line.strip()
                if not line:
                    continue
                
                try:
                    data = json.loads(line)
                    
                    # æå–è¾“å…¥æ–‡æœ¬
                    if 'prompt' in data and data['prompt'].strip():
                        inputs.append(data['prompt'])
                    elif 'input' in data:
                        inputs.append(data['input'])
                    elif 'chosen' in data:
                        # å¦‚æœæ²¡æœ‰promptï¼Œä½¿ç”¨chosenä½œä¸ºç¤ºä¾‹
                        inputs.append(data['chosen'][:100] + "...")  # æˆªå–å‰100å­—ç¬¦ä½œä¸ºè¾“å…¥
                    
                except json.JSONDecodeError as e:
                    print(f"Warning: Line {line_num} is not valid JSON: {e}")
                    continue
    
    elif file_path.endswith('.json'):
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
            
        if isinstance(data, list):
            for item in data:
                if num_samples and len(inputs) >= num_samples:
                    break
                if isinstance(item, dict):
                    if 'prompt' in item and item['prompt'].strip():
                        inputs.append(item['prompt'])
                    elif 'input' in item:
                        inputs.append(item['input'])
    
    print(f"âœ… Loaded {len(inputs)} test inputs")
    return inputs

def compare_outputs(
    base_model_path: str,
    ptuning_model_path: str,
    config_path: str,
    test_inputs: List[str],
    output_file: str = None,
    **generate_kwargs
):
    """
    æ¯”è¾ƒåŸºç¡€æ¨¡å‹å’ŒP-tuningæ¨¡å‹çš„è¾“å‡ºï¼Œç§»é™¤ç”Ÿæˆé™åˆ¶è®©æ¨¡å‹è‡ªç„¶ç»“æŸ
    """
    print("ğŸ”„ Loading models for comparison...")
    
    # åŠ è½½åŸºç¡€æ¨¡å‹
    print("Loading base model...")
    tokenizer = AutoTokenizer.from_pretrained(base_model_path)
    base_model = AutoModelForCausalLM.from_pretrained(
        base_model_path,
        torch_dtype=torch.float16,
        device_map="auto"
    )
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    # åŠ è½½P-tuningæ¨¡å‹
    print("Loading P-tuning model...")
    ptuning_model, _ = load_trained_ptuning_model(
        base_model_path, ptuning_model_path, config_path
    )
    
    print("ğŸ” Comparing outputs...")
    
    comparison_results = []
    
    # ğŸš¨ å…³é”®ä¿®æ”¹ï¼šç§»é™¤æ‰€æœ‰ç”Ÿæˆé™åˆ¶å‚æ•°
    temperature = generate_kwargs.get('temperature', 0.7)
    max_length = generate_kwargs.get('max_length', 2048)  # åªä¿ç•™æ€»é•¿åº¦ä¸Šé™
    
    print(f"ğŸ”§ ç»Ÿä¸€ç”Ÿæˆå‚æ•° (è‡ªç„¶ç»“æŸæ¨¡å¼):")
    print(f"   max_length: {max_length} (æ€»åºåˆ—é•¿åº¦ä¸Šé™)")
    print(f"   temperature: {temperature}")
    print(f"   do_sample: True")
    print(f"   top_p: 0.9")
    print(f"   top_k: 50")
    print(f"   repetition_penalty: 1.1")
    print(f"   æ— max_new_tokensé™åˆ¶ - è®©æ¨¡å‹è‡ªç„¶ç”Ÿæˆåˆ°EOS")
    
    for i, input_text in enumerate(test_inputs):
        print(f"\n{'='*80}")
        print(f"Sample {i+1}/{len(test_inputs)}: {input_text[:100]}{'...' if len(input_text) > 100 else ''}")
        print('='*80)
        
        # ğŸš¨ åŸºç¡€æ¨¡å‹ç”Ÿæˆ - ç§»é™¤æ‰€æœ‰é™åˆ¶è®©å…¶è‡ªç„¶ç”Ÿæˆ
        print("ğŸ¤– Base Model Output:")
        input_ids = tokenizer.encode(input_text, return_tensors="pt").to(base_model.device)
        
        print(f"ğŸ”§ Base model generation parameters:")
        print(f"   input_ids shape: {input_ids.shape}")
        print(f"   max_length: {max_length}")
        print(f"   temperature: {temperature}")
        print(f"   do_sample: True")
        print(f"   æ— max_new_tokensé™åˆ¶")
        
        with torch.no_grad():
            base_output = base_model.generate(
                input_ids,
                max_length=max_length,  # ğŸš¨ åªè®¾ç½®æ€»é•¿åº¦ä¸Šé™
                temperature=temperature,
                do_sample=True,
                top_p=0.9,
                top_k=50,
                repetition_penalty=1.1,
                pad_token_id=tokenizer.pad_token_id,
                eos_token_id=tokenizer.eos_token_id,
                use_cache=True
                # ğŸš¨ ç§»é™¤è¿™äº›é™åˆ¶ï¼š
                # max_new_tokens=xxx
                # min_new_tokens=xxx
                # early_stopping=xxx
            )
        
        print(f"ğŸ”¢ Base model output shape: {base_output.shape}")
        
        # ğŸš¨ ä¿®å¤ï¼šæ­£ç¡®è§£ç åŸºç¡€æ¨¡å‹è¾“å‡º
        # base_model.generate()è¿”å›å®Œæ•´åºåˆ— [input_tokens + generated_tokens]
        base_generated_ids = base_output[0][input_ids.size(1):]  # åªå–æ–°ç”Ÿæˆçš„éƒ¨åˆ†
        base_text = tokenizer.decode(base_generated_ids, skip_special_tokens=True)
        
        print(f"ğŸ”¢ Base model generated {len(base_generated_ids)} tokens")
        print(f"Base model output: {base_text}")
        
        print("\nğŸ¯ P-tuning Model Output:")
        
        # ğŸš¨ P-tuningæ¨¡å‹ç”Ÿæˆ - åŒæ ·ç§»é™¤é™åˆ¶
        ptuning_text = generate_with_ptuning(
            ptuning_model, tokenizer, input_text,
            max_length=max_length,  # ğŸš¨ åªè®¾ç½®æ€»é•¿åº¦ä¸Šé™
            temperature=temperature,
            do_sample=True,
            top_p=0.9,
            top_k=50,
            repetition_penalty=1.1
            # ğŸš¨ ç§»é™¤max_new_tokensç­‰é™åˆ¶å‚æ•°
        )
        print(ptuning_text)
        print()
        
        # æ¯”è¾ƒç”Ÿæˆé•¿åº¦
        print(f"ğŸ“Š Generation length comparison:")
        print(f"   Input length: {len(input_text)} chars")
        print(f"   Base model output: {len(base_text)} chars, {len(base_generated_ids)} tokens")
        print(f"   P-tuning output: {len(ptuning_text)} chars")
        
        # æ£€æŸ¥ç»“æŸåŸå› 
        base_ended_naturally = (len(base_generated_ids) > 0 and 
                               base_generated_ids[-1].item() == tokenizer.eos_token_id)
        
        print(f"ğŸ“Š Generation ending:")
        print(f"   Base model ended naturally: {base_ended_naturally}")
        
        # ä¿å­˜æ¯”è¾ƒç»“æœ
        comparison_results.append({
            "sample_id": i + 1,
            "input": input_text,
            "base_model_output": base_text,
            "ptuning_model_output": ptuning_text,
            "input_length": len(input_text),
            "base_output_length": len(base_text),
            "ptuning_output_length": len(ptuning_text),
            "base_tokens_generated": len(base_generated_ids),
            "base_ended_naturally": base_ended_naturally
        })
    
    # ä¿å­˜æ¯”è¾ƒç»“æœåˆ°æ–‡ä»¶
    if output_file:
        comparison_data = {
            "metadata": {
                "base_model": base_model_path,
                "ptuning_model": ptuning_model_path,
                "config": config_path,
                "comparison_timestamp": datetime.now().isoformat(),
                "total_samples": len(test_inputs),
                "generation_config": {
                    "mode": "natural_ending",
                    "max_length": max_length,
                    "temperature": temperature,
                    "note": "No max_new_tokens limit - models generate until EOS"
                }
            },
            "results": comparison_results
        }
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(comparison_data, f, indent=2, ensure_ascii=False)
        
        print(f"\nğŸ’¾ Comparison results saved to: {output_file}")
    
    return comparison_results

def extract_experiment_info(prompt_embeddings_path: str) -> Dict[str, str]:
    """
    ä»prompt embeddingsè·¯å¾„ä¸­æå–å®éªŒä¿¡æ¯
    
    Args:
        prompt_embeddings_path: prompt embeddingsæ–‡ä»¶è·¯å¾„
        
    Returns:
        åŒ…å«å®éªŒä¿¡æ¯çš„å­—å…¸
    """
    # è·å–ç»å¯¹è·¯å¾„å¹¶æ ‡å‡†åŒ–
    abs_path = os.path.abspath(prompt_embeddings_path)
    path_parts = abs_path.split(os.sep)
    
    experiment_info = {
        'model': 'unknown',
        'vtokens': 'unknown', 
        'init_method': 'unknown',
        'kl_weight': 'unknown',
        'margin': 'unknown',
        'learning_rate': 'unknown',
        'epochs': 'unknown',
        'batch_size': 'unknown',
        'timestamp': 'unknown',
        'checkpoint': 'unknown'
    }
    
    # å¯»æ‰¾åŒ…å«å®éªŒå‚æ•°çš„ç›®å½•å
    experiment_dir = None
    checkpoint_dir = None
    
    for i, part in enumerate(path_parts):
        # åŒ¹é…å®éªŒå‚æ•°ç›®å½•æ ¼å¼ï¼šmodel_vtokensX_initY_klZ_marginW_lrV_epU_bsT_timestamp
        if re.match(r'.*_vtokens\d+_init.*_kl[\d.]+_margin[\d.]+_lr[\de-]+_ep\d+_bs\d+_\d{8}_\d{6}$', part):
            experiment_dir = part
        
        # åŒ¹é…checkpointç›®å½•æ ¼å¼ï¼šcheckpoint-æ•°å­—
        if re.match(r'checkpoint-\d+$', part):
            checkpoint_dir = part
    
    # è§£æå®éªŒç›®å½•åç§°
    if experiment_dir:
        # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–å„ä¸ªå‚æ•°
        patterns = {
            'model': r'^([^_]+)',
            'vtokens': r'vtokens(\d+)',
            'init_method': r'init([^_]+)',
            'kl_weight': r'kl([\d.]+)',
            'margin': r'margin([\d.]+)',
            'learning_rate': r'lr([\de.-]+)',
            'epochs': r'ep(\d+)',
            'batch_size': r'bs(\d+)',
            'timestamp': r'(\d{8}_\d{6})$'
        }
        
        for key, pattern in patterns.items():
            match = re.search(pattern, experiment_dir)
            if match:
                experiment_info[key] = match.group(1)
    
    # æå–checkpointä¿¡æ¯
    if checkpoint_dir:
        checkpoint_match = re.search(r'checkpoint-(\d+)', checkpoint_dir)
        if checkpoint_match:
            experiment_info['checkpoint'] = checkpoint_match.group(1)
    
    # å¦‚æœæ²¡æœ‰æ‰¾åˆ°å®éªŒç›®å½•ï¼Œå°è¯•ä»æ–‡ä»¶åæœ¬èº«æå–ä¿¡æ¯
    if experiment_dir is None:
        filename = os.path.basename(prompt_embeddings_path)
        # å°è¯•ä»çˆ¶ç›®å½•è·å–ä¿¡æ¯
        parent_dirs = path_parts[-3:]  # å–æœ€å3çº§ç›®å½•
        for parent_dir in parent_dirs:
            if '_vtokens' in parent_dir or '_kl' in parent_dir:
                experiment_dir = parent_dir
                break
    
    return experiment_info

def generate_output_filename(
    prompt_embeddings_path: str,
    test_mode: str = "inference",
    timestamp: bool = True
) -> str:
    """
    ç”Ÿæˆè¾“å‡ºæ–‡ä»¶å
    
    Args:
        prompt_embeddings_path: prompt embeddingsæ–‡ä»¶è·¯å¾„
        test_mode: æµ‹è¯•æ¨¡å¼ (inference, compare, datasetç­‰)
        timestamp: æ˜¯å¦æ·»åŠ æ—¶é—´æˆ³
        
    Returns:
        ç”Ÿæˆçš„è¾“å‡ºæ–‡ä»¶å
    """
    # æå–å®éªŒä¿¡æ¯
    exp_info = extract_experiment_info(prompt_embeddings_path)
    
    # æ„å»ºæ–‡ä»¶åç»„ä»¶
    filename_parts = []
    
    # 1. æµ‹è¯•æ¨¡å¼
    filename_parts.append(f"ptuning_{test_mode}")
    
    # 2. æ¨¡å‹åç§°ï¼ˆç®€åŒ–ï¼‰
    model_name = exp_info['model'].replace('.', '-').lower()
    filename_parts.append(model_name)
    
    # 3. å…³é”®è¶…å‚æ•°
    key_params = []
    if exp_info['vtokens'] != 'unknown':
        key_params.append(f"v{exp_info['vtokens']}")
    
    if exp_info['init_method'] != 'unknown':
        init_short = exp_info['init_method']
        # ç¼©çŸ­åˆå§‹åŒ–æ–¹æ³•åç§°
        init_mapping = {
            'natural_language': 'nat',
            'natural': 'nat',
            'random': 'rand',
            'vocab': 'vocab',
            'cluster_center': 'cluster'
        }
        init_short = init_mapping.get(init_short, init_short[:4])
        key_params.append(f"init{init_short}")
    
    if exp_info['kl_weight'] != 'unknown':
        key_params.append(f"kl{exp_info['kl_weight']}")
    
    if exp_info['margin'] != 'unknown':
        key_params.append(f"m{exp_info['margin']}")
    
    if key_params:
        filename_parts.append("_".join(key_params))
    
    # 4. è®­ç»ƒå‚æ•°ï¼ˆå¯é€‰ï¼‰
    train_params = []
    if exp_info['learning_rate'] != 'unknown':
        # ç®€åŒ–å­¦ä¹ ç‡è¡¨ç¤ºï¼Œå¦‚1e-5 -> 1e5
        lr = exp_info['learning_rate'].replace('-', '').replace('e', 'e')
        train_params.append(f"lr{lr}")
    
    if exp_info['epochs'] != 'unknown':
        train_params.append(f"ep{exp_info['epochs']}")
    
    if train_params:
        filename_parts.append("_".join(train_params))
    
    # 5. Checkpointä¿¡æ¯
    if exp_info['checkpoint'] != 'unknown':
        filename_parts.append(f"ckpt{exp_info['checkpoint']}")
    
    # 6. åŸå§‹æ—¶é—´æˆ³ï¼ˆå®éªŒæ—¶é—´ï¼‰
    if exp_info['timestamp'] != 'unknown':
        # è½¬æ¢æ—¶é—´æˆ³æ ¼å¼ï¼š20250528_133602 -> 0528-1336
        ts = exp_info['timestamp']
        if len(ts) == 15:  # YYYYMMDD_HHMMSS
            short_ts = ts[4:8] + "-" + ts[9:13]  # MMDD-HHMM
            filename_parts.append(short_ts)
    
    # 7. å½“å‰æ¨ç†æ—¶é—´æˆ³ï¼ˆå¯é€‰ï¼‰
    if timestamp:
        current_ts = datetime.now().strftime("%m%d_%H%M")
        filename_parts.append(f"inf{current_ts}")
    
    # ç»„åˆæ–‡ä»¶å
    base_filename = "_".join(filename_parts)
    
    # ç¡®ä¿æ–‡ä»¶åä¸ä¼šè¿‡é•¿ï¼ˆé™åˆ¶åœ¨200å­—ç¬¦å†…ï¼‰
    if len(base_filename) > 200:
        # å¦‚æœå¤ªé•¿ï¼Œä¿ç•™æœ€é‡è¦çš„éƒ¨åˆ†
        essential_parts = [
            filename_parts[0],  # ptuning_mode
            filename_parts[1],  # model
            filename_parts[2] if len(filename_parts) > 2 else "",  # key_params
        ]
        if exp_info['checkpoint'] != 'unknown':
            essential_parts.append(f"ckpt{exp_info['checkpoint']}")
        if timestamp:
            essential_parts.append(f"inf{datetime.now().strftime('%m%d_%H%M')}")
        
        base_filename = "_".join(filter(None, essential_parts))
    
    return f"{base_filename}.json"

def create_output_directory(prompt_embeddings_path: str) -> str:
    """
    åˆ›å»ºè¾“å‡ºç›®å½•
    
    Args:
        prompt_embeddings_path: prompt embeddingsæ–‡ä»¶è·¯å¾„
        
    Returns:
        è¾“å‡ºç›®å½•è·¯å¾„
    """
    # æå–å®éªŒä¿¡æ¯ç”¨äºç›®å½•å‘½å
    exp_info = extract_experiment_info(prompt_embeddings_path)
    
    # åˆ›å»ºåŸºç¡€è¾“å‡ºç›®å½•
    base_output_dir = "./inference_outputs"
    
    # åˆ›å»ºå®éªŒç‰¹å®šçš„å­ç›®å½•
    exp_subdir_parts = []
    
    if exp_info['model'] != 'unknown':
        exp_subdir_parts.append(exp_info['model'])
    
    if exp_info['vtokens'] != 'unknown':
        exp_subdir_parts.append(f"vtokens{exp_info['vtokens']}")
    
    if exp_info['init_method'] != 'unknown':
        exp_subdir_parts.append(f"init{exp_info['init_method']}")
    
    if exp_info['timestamp'] != 'unknown':
        # ä½¿ç”¨åŸå§‹è®­ç»ƒæ—¶é—´æˆ³
        exp_subdir_parts.append(exp_info['timestamp'])
    
    if exp_subdir_parts:
        exp_subdir = "_".join(exp_subdir_parts)
        output_dir = os.path.join(base_output_dir, exp_subdir)
    else:
        output_dir = os.path.join(base_output_dir, "default")
    
    # åˆ›å»ºç›®å½•
    os.makedirs(output_dir, exist_ok=True)
    
    return output_dir

def debug_generation_process(
    model: PTuningModel,
    tokenizer,
    input_text: str = "Explain the importance of data privacy in the digital age."
):
    """
    è°ƒè¯•ç”Ÿæˆè¿‡ç¨‹ï¼Œç‰¹åˆ«å…³æ³¨tokenæˆªæ–­é—®é¢˜
    """
    print("ğŸ” Debug: Step-by-step generation process")
    print("=" * 80)
    
    device = next(model.parameters()).device
    
    # 1. è¾“å…¥ç¼–ç 
    encoding = tokenizer(input_text, return_tensors="pt", add_special_tokens=True)
    input_ids = encoding["input_ids"].to(device)
    input_attention_mask = encoding["attention_mask"].to(device)
    
    print(f"1ï¸âƒ£ Input encoding:")
    print(f"   Text: '{input_text}'")
    print(f"   Input IDs: {input_ids.tolist()[0]}")
    print(f"   Length: {input_ids.size(1)}")
    print(f"   Decoded back: '{tokenizer.decode(input_ids[0])}'")
    print()
    
    # 2. åˆ›å»ºå®Œæ•´çš„inputs_embeds
    batch_size = input_ids.size(0)
    input_embeddings = model.base_model.get_input_embeddings()(input_ids)
    prompt_embeddings = model.get_prompt_embeddings(batch_size)
    inputs_embeds = torch.cat([prompt_embeddings, input_embeddings], dim=1)
    
    prompt_attention_mask = torch.ones(
        batch_size, model.num_virtual_tokens,
        dtype=input_attention_mask.dtype,
        device=device
    )
    full_attention_mask = torch.cat([prompt_attention_mask, input_attention_mask], dim=1)
    
    print(f"2ï¸âƒ£ Combined embeddings:")
    print(f"   Prompt embeddings: {prompt_embeddings.shape}")
    print(f"   Input embeddings: {input_embeddings.shape}")
    print(f"   Combined: {inputs_embeds.shape}")
    print(f"   Attention mask: {full_attention_mask.shape}")
    print()
    
    # 3. ç”Ÿæˆ
    with torch.no_grad():
        output_ids = model.base_model.generate(
            inputs_embeds=inputs_embeds,
            attention_mask=full_attention_mask,
            max_new_tokens=512,
            do_sample=True,
            temperature=0.7,
            pad_token_id=tokenizer.pad_token_id or tokenizer.eos_token_id,
            eos_token_id=tokenizer.eos_token_id,
        )
    
    print(f"3ï¸âƒ£ Generation output:")
    print(f"   Output shape: {output_ids.shape}")
    print(f"   Output IDs: {output_ids.tolist()[0]}")
    print()
    
    # 4. åˆ†æè¾“å‡ºç»“æ„
    print(f"4ï¸âƒ£ Output analysis:")
    
    # ğŸš¨ å…³é”®ï¼šç†è§£è¾“å‡ºç»“æ„
    # å½“ä½¿ç”¨inputs_embedsæ—¶ï¼Œç”Ÿæˆçš„output_idsä¸ä¼šåŒ…å«prompt embeddingsçš„å¯¹åº”token
    # å› ä¸ºprompt embeddingsæ˜¯è¿ç»­å‘é‡ï¼Œæ²¡æœ‰å¯¹åº”çš„token ID
    # æ‰€ä»¥output_ids = [original_input_tokens] + [newly_generated_tokens]
    
    original_length = input_ids.size(1)
    total_length = output_ids.size(1)
    generated_length = total_length - original_length
    
    print(f"   Original input length: {original_length}")
    print(f"   Total output length: {total_length}")
    print(f"   Generated length: {generated_length}")
    print()
    
    # 5. éªŒè¯è¾“å…¥éƒ¨åˆ†
    if total_length >= original_length:
        input_part = output_ids[0][:original_length]
        print(f"5ï¸âƒ£ Input part verification:")
        print(f"   Original: {input_ids.tolist()[0]}")
        print(f"   In output: {input_part.tolist()}")
        print(f"   Match: {torch.equal(input_ids[0], input_part)}")
        print()
    
    # 6. æå–å’Œè§£ç ç”Ÿæˆéƒ¨åˆ†
    if generated_length > 0:
        generated_part = output_ids[0]
        print(f"6ï¸âƒ£ Generated part:")
        print(f"   Generated IDs: {generated_part.tolist()}")
        print()
        
        # é€tokenè§£ç 
        print(f"   Token-by-token decode:")
        for i, token_id in enumerate(generated_part[:10]):  # å‰10ä¸ª
            token_text = tokenizer.decode([token_id])
            print(f"     {i}: {token_id} -> '{token_text}'")
        print()
        
        # å®Œæ•´è§£ç 
        generated_text = tokenizer.decode(generated_part, skip_special_tokens=True)
        print(f"   Complete generated text:")
        print(f"     '{generated_text}'")
        print()
        
        # æ£€æŸ¥å¼€å¤´æ˜¯å¦å®Œæ•´
        if generated_text and not generated_text[0].isspace():
            first_char = generated_text[0]
            if first_char.islower():
                print(f"âš ï¸ Warning: Generated text starts with lowercase: '{first_char}'")
                print("   This might indicate incomplete word segmentation")
            else:
                print(f"âœ… Generated text starts properly: '{first_char}'")
        
    print("=" * 80)

def main():
    """
    ä¸»å‡½æ•°ï¼šæ¼”ç¤ºP-tuningæ¨¡å‹çš„æ¨ç†è¿‡ç¨‹
    """
    parser = argparse.ArgumentParser(description="P-tuning Model Inference")
    parser.add_argument("--base_model", type=str, default="Qwen/Qwen2.5-1.5B-Instruct",
                        help="Base model path")
    parser.add_argument("--prompt_embeddings", type=str, required=True,
                        help="Path to trained prompt embeddings")
    parser.add_argument("--config", type=str, 
                        help="Path to P-tuning config file")
    parser.add_argument("--test_data", type=str,
                        help="Path to test dataset")
    parser.add_argument("--num_samples", type=int, default=5,
                        help="Number of samples to test")
    # ğŸš¨ ä¿®æ”¹å‚æ•°ï¼šç§»é™¤max_new_tokensï¼Œæ”¹ä¸ºmax_length
    parser.add_argument("--max_length", type=int, default=2048,
                        help="Maximum total sequence length (including input)")
    parser.add_argument("--temperature", type=float, default=0.7,
                        help="Sampling temperature")
    parser.add_argument("--compare", action="store_true",
                        help="Compare with base model")
    parser.add_argument("--debug", action="store_true",
                        help="Enable debug mode with detailed logging")
    parser.add_argument("--debug_generation", action="store_true",
                        help="Debug generation process step by step")
    
    args = parser.parse_args()
    
    print("ğŸš€ P-tuning Model Inference Demo")
    print("=" * 50)
    
    print(f"ğŸ”§ Generation strategy: Natural ending (no token limits)")
    print(f"ğŸ”§ Max sequence length: {args.max_length}")
    
    # æ˜¾ç¤ºæå–çš„å®éªŒä¿¡æ¯
    exp_info = extract_experiment_info(args.prompt_embeddings)
    print(f"ğŸ“Š å®éªŒä¿¡æ¯æå–ç»“æœ:")
    for key, value in exp_info.items():
        if value != 'unknown':
            print(f"   {key}: {value}")
    print()
    
    # åŠ è½½è®­ç»ƒå¥½çš„P-tuningæ¨¡å‹
    ptuning_model, tokenizer = load_trained_ptuning_model(
        args.base_model,
        args.prompt_embeddings,
        args.config
    )
    
    # æ·»åŠ ç”Ÿæˆè¿‡ç¨‹è°ƒè¯•
    if args.debug_generation:
        debug_generation_process(ptuning_model, tokenizer)
        return
    
    # æ·»åŠ è°ƒè¯•ä¿¡æ¯
    if args.debug:
        debug_model_state(ptuning_model, tokenizer)
        check_prompt_embedding_quality(ptuning_model, tokenizer)
        generate_prompt_summary(ptuning_model, tokenizer)
    
    # å‡†å¤‡æµ‹è¯•è¾“å…¥
    if args.test_data:
        test_inputs = load_test_dataset(args.test_data, args.num_samples)
    else:
        test_inputs = [
            "Please help me understand the concept of machine learning.",
            "What are the benefits of renewable energy?",
            "Explain the importance of data privacy in the digital age.",
            "How can I improve my communication skills?",
            "What are some healthy eating habits I should adopt?"
        ][:args.num_samples]
    
    if args.compare:
        # æ¯”è¾ƒæ¨¡å¼
        if not test_inputs:
            print("âŒ Compare mode requires test inputs!")
            return
        
        test_mode = "compare"
        output_dir = create_output_directory(args.prompt_embeddings)
        output_filename = generate_output_filename(
            args.prompt_embeddings, 
            test_mode=test_mode,
            timestamp=True
        )
        output_file = os.path.join(output_dir, output_filename)
        
        # è¿è¡Œæ¯”è¾ƒæµ‹è¯•
        compare_outputs(
            args.base_model,
            args.prompt_embeddings,
            args.config,
            test_inputs,
            output_file=output_file,
            max_length=args.max_length,  # ğŸš¨ ä¼ å…¥max_lengthè€Œä¸æ˜¯max_new_tokens
            temperature=args.temperature
        )
    else:
        # å•ç‹¬æµ‹è¯•P-tuningæ¨¡å‹
        print(f"\nğŸ¯ Testing P-tuning model with {len(test_inputs)} samples:")
        
        # å¦‚æœå¯ç”¨è°ƒè¯•æ¨¡å¼ï¼Œå…ˆç”¨è¯¦ç»†æ—¥å¿—æµ‹è¯•ç¬¬ä¸€ä¸ªæ ·æœ¬
        if args.debug and test_inputs:
            print("\n" + "="*80)
            print("ğŸ” DETAILED DEBUG FOR FIRST SAMPLE")
            print("="*80)
            debug_result = generate_with_detailed_logging(
                ptuning_model, tokenizer, test_inputs[0], max_length=args.max_length
            )
            print(f"ğŸ¯ Debug result: '{debug_result}'")
            print("="*80)
        
        results = []
        for i, input_text in enumerate(test_inputs):
            print(f"\nğŸ“‹ Sample {i+1}/{len(test_inputs)}:")
            print(f"Input: {input_text}")
            
            # ğŸš¨ ä½¿ç”¨æ–°çš„ç”Ÿæˆå‚æ•°ï¼ˆç§»é™¤é™åˆ¶ï¼‰
            generated_text = generate_with_ptuning(
                ptuning_model,
                tokenizer,
                input_text,
                max_length=args.max_length,  # ğŸš¨ åªè®¾ç½®æ€»é•¿åº¦ä¸Šé™
                temperature=args.temperature,
                do_sample=True,
                top_p=0.9
                # ğŸš¨ ç§»é™¤max_new_tokensç­‰é™åˆ¶å‚æ•°
            )
            
            print(f"Output: {generated_text}")
            print("-" * 80)
            
            results.append({
                "input": input_text,
                "output": generated_text,
                "generation_config": {
                    "mode": "natural_ending",
                    "max_length": args.max_length,
                    "no_token_limits": True
                }
            })
        
        # ç¡®å®šæµ‹è¯•æ¨¡å¼
        if args.test_data:
            test_mode = "dataset"
        else:
            test_mode = "basic"
        
        # åˆ›å»ºè¾“å‡ºç›®å½•å’Œæ–‡ä»¶
        output_dir = create_output_directory(args.prompt_embeddings)
        output_filename = generate_output_filename(
            args.prompt_embeddings, 
            test_mode=test_mode,
            timestamp=True
        )
        output_file = os.path.join(output_dir, output_filename)
        
        # ä¿å­˜ç»“æœ
        results_with_metadata = {
            "metadata": {
                "experiment_info": exp_info,
                "inference_config": {
                    "base_model": args.base_model,
                    "prompt_embeddings_path": args.prompt_embeddings,
                    "config_path": args.config,
                    "test_mode": test_mode,
                    "num_samples": args.num_samples,
                    "max_length": args.max_length,
                    "temperature": args.temperature,
                    "generation_mode": "natural_ending",
                    "inference_timestamp": datetime.now().isoformat()
                }
            },
            "results": results
        }
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(results_with_metadata, f, indent=2, ensure_ascii=False)
        
        print(f"\nğŸ’¾ Results saved to: {output_file}")
        print(f"ğŸ“ Output directory: {output_dir}")
        
        # åˆ›å»ºæœ€æ–°ç»“æœçš„ç¬¦å·é“¾æ¥
        latest_link = os.path.join(output_dir, f"latest_{test_mode}.json")
        if os.path.lexists(latest_link):
            os.unlink(latest_link)
        
        try:
            os.symlink(output_filename, latest_link)
            print(f"ğŸ”— Latest result link: {latest_link}")
        except OSError:
            import shutil
            shutil.copy2(output_file, latest_link)
            print(f"ğŸ“‹ Latest result copy: {latest_link}")
    
    print("\nâœ… Inference completed!")

if __name__ == "__main__":
    main()