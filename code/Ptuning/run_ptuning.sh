#!/bin/bash

# åˆ‡æ¢åˆ°è„šæœ¬æ‰€åœ¨ç›®å½•
cd "$(dirname "$0")"

# P-tuningè®­ç»ƒè„šæœ¬ï¼šé€‚ç”¨äºQwen2.5-1.5Bæ¨¡å‹çš„åå¥½å­¦ä¹ 
# è¯¥è„šæœ¬é…ç½®äº†P-tuningè®­ç»ƒçš„æ‰€æœ‰é‡è¦å‚æ•°

# ğŸ†• å…³é”®è®­ç»ƒå‚æ•°é…ç½®
NUM_VIRTUAL_TOKENS=10
PROMPT_INIT_METHOD="natural_language"
PREFERENCE_LOSS_WEIGHT=1.0
MARGIN=0.05
KL_LOSS_WEIGHT=0.1
LEARNING_RATE=1e-6
NUM_EPOCHS=10
BATCH_SIZE=2

# ğŸ†• æ¨¡å‹ä¿å­˜å’Œè¯„ä¼°é¢‘ç‡é…ç½®
SAVE_STEPS=2000           # æ¯å¤šå°‘æ­¥ä¿å­˜ä¸€æ¬¡æ¨¡å‹æ£€æŸ¥ç‚¹ï¼ˆé»˜è®¤500æ­¥ï¼ŒåŸä¸º200æ­¥ï¼‰
EVAL_STEPS=2000           # æ¯å¤šå°‘æ­¥è¿›è¡Œä¸€æ¬¡è¯„ä¼°ï¼ˆé»˜è®¤500æ­¥ï¼ŒåŸä¸º200æ­¥ï¼‰

# ğŸ†• è‡ªåŠ¨ç”Ÿæˆå¸¦å‚æ•°ä¿¡æ¯çš„è¾“å‡ºç›®å½•
# æ ¼å¼ï¼šptuning_[æ¨¡å‹åç§°]_vtokens[æ•°é‡]_init[æ–¹æ³•]_kl[æƒé‡]_margin[å€¼]_lr[å­¦ä¹ ç‡]_ep[è½®æ¬¡]_[æ—¶é—´æˆ³]
MODEL_NAME="DeepSeek-R1-Distill-Qwen-1.5B"  # åŸºç¡€æ¨¡å‹åç§°
TIMESTAMP=$(date +%Y%m%d_%H%M%S)

# æ„å»ºè¾“å‡ºç›®å½•åç§°
OUTPUT_DIR="./ptuning_outputs/${MODEL_NAME}_vtokens${NUM_VIRTUAL_TOKENS}_init${PROMPT_INIT_METHOD}_kl${KL_LOSS_WEIGHT}_margin${MARGIN}_lr${LEARNING_RATE}_ep${NUM_EPOCHS}_bs${BATCH_SIZE}_${TIMESTAMP}"

# ğŸ†• æ˜¾ç¤ºé…ç½®ä¿¡æ¯
echo "ğŸš€ P-tuningè®­ç»ƒé…ç½®"
echo "================================"
echo "æ¨¡å‹: $MODEL_NAME"
echo "è™šæ‹Ÿtokenæ•°é‡: $NUM_VIRTUAL_TOKENS"
echo "åˆå§‹åŒ–æ–¹æ³•: $PROMPT_INIT_METHOD"
echo "åå¥½æŸå¤±æƒé‡: $PREFERENCE_LOSS_WEIGHT"
echo "è¾¹è·å‚æ•°: $MARGIN"
echo "KLæ•£åº¦æƒé‡: $KL_LOSS_WEIGHT"
echo "å­¦ä¹ ç‡: $LEARNING_RATE"
echo "è®­ç»ƒè½®æ¬¡: $NUM_EPOCHS"
echo "æ‰¹æ¬¡å¤§å°: $BATCH_SIZE"
echo "ä¿å­˜é¢‘ç‡: æ¯${SAVE_STEPS}æ­¥"
echo "è¯„ä¼°é¢‘ç‡: æ¯${EVAL_STEPS}æ­¥"
echo "è¾“å‡ºç›®å½•: $OUTPUT_DIR"
echo "================================"
echo ""

# ğŸ†• ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
mkdir -p "$(dirname "$OUTPUT_DIR")"

# ğŸ†• ä¿å­˜è®­ç»ƒé…ç½®åˆ°æ–‡ä»¶
CONFIG_FILE="${OUTPUT_DIR}_config.txt"
cat > "$CONFIG_FILE" << EOF
P-tuningè®­ç»ƒé…ç½®
==========================================
è®­ç»ƒæ—¶é—´: $(date '+%Y-%m-%d %H:%M:%S')
åŸºç¡€æ¨¡å‹: $MODEL_NAME
æ•°æ®é›†: /home/wangbinrui/research_projects/llama_rlhf/datasets/ultrafeedback_binarized/train_prefs_ultrafeedback_binarized.jsonl

æ ¸å¿ƒå‚æ•°:
- è™šæ‹Ÿtokenæ•°é‡: $NUM_VIRTUAL_TOKENS
- åˆå§‹åŒ–æ–¹æ³•: $PROMPT_INIT_METHOD
- åå¥½æŸå¤±æƒé‡: $PREFERENCE_LOSS_WEIGHT
- è¾¹è·å‚æ•°: $MARGIN
- KLæ•£åº¦æƒé‡: $KL_LOSS_WEIGHT

è®­ç»ƒå‚æ•°:
- å­¦ä¹ ç‡: $LEARNING_RATE
- è®­ç»ƒè½®æ¬¡: $NUM_EPOCHS
- æ‰¹æ¬¡å¤§å°: $BATCH_SIZE
- æ¢¯åº¦ç´¯ç§¯æ­¥æ•°: 8
- æœ€å¤§åºåˆ—é•¿åº¦: 512
- ä¿å­˜é¢‘ç‡: æ¯${SAVE_STEPS}æ­¥
- è¯„ä¼°é¢‘ç‡: æ¯${EVAL_STEPS}æ­¥

æ—©åœå‚æ•°:
- è€å¿ƒå€¼: 5
- é˜ˆå€¼: 0.01
- ç›®æ ‡å‡†ç¡®ç‡: 0.9

è¾“å‡ºç›®å½•: $OUTPUT_DIR
==========================================
EOF

echo "ğŸ“ é…ç½®ä¿¡æ¯å·²ä¿å­˜åˆ°: $CONFIG_FILE"
echo ""

# ğŸ†• è¯¢é—®ç”¨æˆ·ç¡®è®¤
read -p "æ˜¯å¦å¼€å§‹è®­ç»ƒï¼Ÿ(y/N): " -n 1 -r
echo
if [[ ! $REPLY =~ ^[Yy]$ ]]; then
    echo "âŒ ç”¨æˆ·å–æ¶ˆè®­ç»ƒ"
    exit 0
fi

echo "ğŸš€ å¼€å§‹P-tuningè®­ç»ƒ..."
echo ""

# æ‰§è¡Œè®­ç»ƒ
CUDA_VISIBLE_DEVICES=2, python train_ptuning.py \
    --model_name_or_path "deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B" \
    --dataset_name "/home/wangbinrui/research_projects/llama_rlhf/datasets/ultrafeedback_binarized/train_prefs_ultrafeedback_binarized.jsonl" \
    --num_virtual_tokens $NUM_VIRTUAL_TOKENS \
    --prompt_init_method $PROMPT_INIT_METHOD \
    --preference_loss_weight $PREFERENCE_LOSS_WEIGHT \
    --margin $MARGIN \
    --kl_loss_weight $KL_LOSS_WEIGHT \
    --save_steps $SAVE_STEPS \
    --eval_steps $EVAL_STEPS \
    --max_length 512 \
    --eval_split_ratio 0.1 \
    --early_stopping_patience 5 \
    --early_stopping_threshold 0.01 \
    --target_accuracy 0.9 \
    --wandb_project "ptuning-preference-learning" \
    --use_wandb True \
    --output_dir "$OUTPUT_DIR" \
    --per_device_train_batch_size $BATCH_SIZE \
    --per_device_eval_batch_size 4 \
    --gradient_accumulation_steps 8 \
    --num_train_epochs $NUM_EPOCHS \
    --learning_rate $LEARNING_RATE \
    --warmup_steps 100 \
    --logging_steps 50 \
    --save_total_limit 10 \
    --remove_unused_columns False \
    --dataloader_pin_memory False \
    --fp16 \
    --dataloader_num_workers 0

# ğŸ†• æ£€æŸ¥è®­ç»ƒç»“æœ
TRAINING_EXIT_CODE=$?

echo ""
echo "================================"
if [ $TRAINING_EXIT_CODE -eq 0 ]; then
    echo "âœ… è®­ç»ƒå®Œæˆï¼"
    echo "ğŸ“ è¾“å‡ºç›®å½•: $OUTPUT_DIR"
    echo "ğŸ“ é…ç½®æ–‡ä»¶: $CONFIG_FILE"
    
    # ğŸ†• æ˜¾ç¤ºè¾“å‡ºæ–‡ä»¶
    if [ -d "$OUTPUT_DIR" ]; then
        echo ""
        echo "ğŸ“‹ è¾“å‡ºæ–‡ä»¶åˆ—è¡¨:"
        ls -la "$OUTPUT_DIR"
        
        # ğŸ†• æ˜¾ç¤ºprompt embeddingsæ–‡ä»¶ä¿¡æ¯
        PROMPT_EMBEDDINGS_FILE="$OUTPUT_DIR/prompt_embeddings.pt"
        if [ -f "$PROMPT_EMBEDDINGS_FILE" ]; then
            FILE_SIZE=$(du -h "$PROMPT_EMBEDDINGS_FILE" | cut -f1)
            echo ""
            echo "ğŸ¯ Prompt Embeddingsä¿¡æ¯:"
            echo "   æ–‡ä»¶: $PROMPT_EMBEDDINGS_FILE"
            echo "   å¤§å°: $FILE_SIZE"
            echo "   å‚æ•°: ${NUM_VIRTUAL_TOKENS}ä¸ªè™šæ‹Ÿtokenï¼Œåˆå§‹åŒ–æ–¹æ³•=${PROMPT_INIT_METHOD}"
        fi
        
        # ğŸ†• åˆ›å»ºç¬¦å·é“¾æ¥åˆ°æœ€æ–°è®­ç»ƒç»“æœ
        LATEST_LINK="./ptuning_outputs/latest_${MODEL_NAME}"
        if [ -L "$LATEST_LINK" ]; then
            rm "$LATEST_LINK"
        fi
        ln -s "$(basename "$OUTPUT_DIR")" "$LATEST_LINK"
        echo "ğŸ”— æœ€æ–°ç»“æœé“¾æ¥: $LATEST_LINK -> $(basename "$OUTPUT_DIR")"
    fi
    
    # ğŸ†• æä¾›æ¨ç†æµ‹è¯•å»ºè®®
    echo ""
    echo "ğŸ’¡ ä¸‹ä¸€æ­¥å»ºè®®:"
    echo "   æµ‹è¯•æ¨ç†: ./test_ptuning_inference.sh basic --prompt_embeddings \"$OUTPUT_DIR/prompt_embeddings.pt\" --config \"$OUTPUT_DIR/ptuning_config.json\""
    echo "   å¯¹æ¯”æµ‹è¯•: ./test_ptuning_inference.sh compare --prompt_embeddings \"$OUTPUT_DIR/prompt_embeddings.pt\""
else
    echo "âŒ è®­ç»ƒå¤±è´¥ï¼é€€å‡ºç : $TRAINING_EXIT_CODE"
    echo "ğŸ“ æ£€æŸ¥æ—¥å¿—: $OUTPUT_DIR"
    echo "ğŸ“ é…ç½®æ–‡ä»¶: $CONFIG_FILE"
    exit $TRAINING_EXIT_CODE
fi

echo "================================"
