import torch
import torch.nn as nn
import torch.nn.functional as F
from transformers import PreTrainedModel
from typing import Optional, Dict, Any
import os


class PTuningModel(nn.Module):
    """
    P-tuningæ¨¡å‹åŒ…è£…å™¨ï¼Œä¸ºåŸºç¡€æ¨¡å‹æ·»åŠ å¯å­¦ä¹ çš„æç¤ºåµŒå…¥
    
    P-tuningæ˜¯ä¸€ç§å‚æ•°é«˜æ•ˆçš„å¾®è°ƒæ–¹æ³•ï¼Œé€šè¿‡åœ¨è¾“å…¥åºåˆ—å‰æ·»åŠ å¯è®­ç»ƒçš„è¿ç»­æç¤ºå‘é‡ï¼Œ
    è€Œä¸æ˜¯å¾®è°ƒæ•´ä¸ªæ¨¡å‹ï¼Œæ¥é€‚åº”ç‰¹å®šä»»åŠ¡ã€‚è¿™ç§æ–¹æ³•ç‰¹åˆ«é€‚ç”¨äºåå¥½å­¦ä¹ ä»»åŠ¡ã€‚
    """
    
    def __init__(
        self,
        base_model: PreTrainedModel,
        num_virtual_tokens: int = 50,
        prompt_embedding_dim: Optional[int] = None,
        init_method: str = "random",
        margin: float = 0.1,
        natural_prompts: Optional[list] = None,
        cluster_centers: Optional[torch.Tensor] = None
    ):
        """
        åˆå§‹åŒ–P-tuningæ¨¡å‹
        
        Args:
            base_model: åŸºç¡€é¢„è®­ç»ƒæ¨¡å‹
            num_virtual_tokens: è™šæ‹Ÿtokenæ•°é‡ï¼ˆè½¯æç¤ºé•¿åº¦ï¼‰
            prompt_embedding_dim: æç¤ºåµŒå…¥ç»´åº¦ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨æ¨¡å‹çš„éšè—ç»´åº¦
            init_method: åˆå§‹åŒ–æ–¹æ³•ï¼Œ"random"ã€"vocab"ã€"natural_language"æˆ–"cluster_center"
            margin: åå¥½æŸå¤±çš„è¾¹è·å‚æ•°
            natural_prompts: è‡ªç„¶è¯­è¨€æç¤ºåˆ—è¡¨ï¼ˆç”¨äºnatural_languageåˆå§‹åŒ–ï¼‰
            cluster_centers: èšç±»ä¸­å¿ƒå¼ é‡ï¼ˆç”¨äºcluster_centeråˆå§‹åŒ–ï¼‰
        """
        super().__init__()
        self.base_model = base_model
        self.num_virtual_tokens = num_virtual_tokens
        self.margin = margin
        
        # ä»åŸºç¡€æ¨¡å‹è·å–åµŒå…¥ç»´åº¦
        if prompt_embedding_dim is None:
            prompt_embedding_dim = base_model.config.hidden_size
        
        self.prompt_embedding_dim = prompt_embedding_dim
        
        # åˆå§‹åŒ–æç¤ºåµŒå…¥å±‚ï¼šè¿™æ˜¯P-tuningçš„æ ¸å¿ƒç»„ä»¶
        self.prompt_embeddings = nn.Embedding(num_virtual_tokens, prompt_embedding_dim)
        self._init_prompt_embeddings(init_method, natural_prompts, cluster_centers)
        
        # å†»ç»“åŸºç¡€æ¨¡å‹å‚æ•°ï¼Œåªè®­ç»ƒæç¤ºåµŒå…¥
        for param in self.base_model.parameters():
            param.requires_grad = False
        
        # ç¡®ä¿prompt embeddingså¯ä»¥è®­ç»ƒ
        for param in self.prompt_embeddings.parameters():
            param.requires_grad = True
        
        # éªŒè¯å‚æ•°å†»ç»“çŠ¶æ€
        total_params = sum(p.numel() for p in self.parameters())
        trainable_params = sum(p.numel() for p in self.parameters() if p.requires_grad)
        print(f"ğŸ”’ Frozen parameters: {total_params - trainable_params:,}")
        print(f"ğŸ”¥ Trainable parameters (prompt embeddings): {trainable_params:,}")
        print(f"ğŸ“Š Trainable ratio: {trainable_params/total_params*100:.4f}%")
    
    def _init_prompt_embeddings(
        self, 
        init_method: str, 
        natural_prompts: Optional[list] = None,
        cluster_centers: Optional[torch.Tensor] = None
    ):
        """
        åˆå§‹åŒ–æç¤ºåµŒå…¥æƒé‡
        
        Args:
            init_method: åˆå§‹åŒ–æ–¹æ³•
                - "random": éšæœºæ­£æ€åˆ†å¸ƒåˆå§‹åŒ–
                - "vocab": ä½¿ç”¨éšæœºè¯æ±‡åµŒå…¥åˆå§‹åŒ–
                - "natural_language": ä½¿ç”¨è‡ªç„¶è¯­è¨€æç¤ºçš„å¹³å‡åµŒå…¥åˆå§‹åŒ–
                - "cluster_center": ä½¿ç”¨èšç±»ä¸­å¿ƒåˆå§‹åŒ–
            natural_prompts: è‡ªç„¶è¯­è¨€æç¤ºåˆ—è¡¨ï¼Œç”¨äºnatural_languageæ–¹æ³•
            cluster_centers: èšç±»ä¸­å¿ƒå¼ é‡ï¼Œç”¨äºcluster_centeræ–¹æ³•
        """
        if init_method == "random":
            # ä½¿ç”¨æ­£æ€åˆ†å¸ƒéšæœºåˆå§‹åŒ–
            nn.init.normal_(self.prompt_embeddings.weight, std=0.02)
            
        elif init_method == "vocab":
            # ä½¿ç”¨éšæœºè¯æ±‡è¡¨åµŒå…¥åˆå§‹åŒ–ï¼Œè¿™æ ·å¯ä»¥è®©è½¯æç¤ºæœ‰æ›´å¥½çš„èµ·å§‹ç‚¹
            vocab_size = self.base_model.config.vocab_size
            random_indices = torch.randint(0, vocab_size, (self.num_virtual_tokens,))
            with torch.no_grad():
                self.prompt_embeddings.weight.copy_(
                    self.base_model.get_input_embeddings().weight[random_indices]
                )
                
        elif init_method == "natural_language":
            # ä½¿ç”¨è‡ªç„¶è¯­è¨€æç¤ºçš„å¹³å‡åµŒå…¥åˆå§‹åŒ–
            # è¿™ç§æ–¹æ³•åˆ©ç”¨äººç±»æ’°å†™çš„é«˜è´¨é‡æç¤ºä½œä¸ºåˆå€¼ï¼Œæ—¢ç®€å•åˆå¸¦è¯­ä¹‰
            self._init_with_natural_prompts(natural_prompts)
            
        elif init_method == "cluster_center":
            # ä½¿ç”¨èšç±»ä¸­å¿ƒåˆå§‹åŒ–
            # å¦‚æœå¯¹è¾“å…¥è¿›è¡Œäº†èšç±»ï¼Œä¸ºæ¯ä¸ªç°‡åˆå§‹åŒ–ä¸€ä¸ªæ›´è¯­ä¹‰è´´è¿‘è¯¥ç°‡çš„æç¤º
            self._init_with_cluster_centers(cluster_centers)
            
        else:
            raise ValueError(f"Unknown init_method: {init_method}")
    
    def _init_with_natural_prompts(self, natural_prompts: Optional[list]):
        """
        ä½¿ç”¨è‡ªç„¶è¯­è¨€æç¤ºçš„å¹³å‡åµŒå…¥åˆå§‹åŒ–
        
        Args:
            natural_prompts: è‡ªç„¶è¯­è¨€æç¤ºåˆ—è¡¨ï¼Œä¾‹å¦‚ï¼š
                ["Please help me with this task:", "You are a helpful assistant."]
        """
        if natural_prompts is None:
            # å¦‚æœæ²¡æœ‰æä¾›è‡ªç„¶è¯­è¨€æç¤ºï¼Œä½¿ç”¨é»˜è®¤çš„é«˜è´¨é‡æç¤º
            natural_prompts = [
                "You are a helpful, harmless, and honest assistant.",
                "Please provide a thoughtful and accurate response.",
                "Consider the following carefully and respond appropriately:"
            ]
            print("âš ï¸  No natural prompts provided, using default prompts.")
        
        # è·å–åˆ†è¯å™¨ï¼ˆå‡è®¾base_modelæœ‰tokenizerå±æ€§ï¼Œæˆ–ä»å…¨å±€è·å–ï¼‰
        try:
            # å°è¯•ä»æ¨¡å‹è·å–åˆ†è¯å™¨
            if hasattr(self.base_model, 'tokenizer'):
                tokenizer = self.base_model.tokenizer
            else:
                # å¦‚æœæ¨¡å‹æ²¡æœ‰åˆ†è¯å™¨å±æ€§ï¼Œéœ€è¦æ‰‹åŠ¨ä¼ å…¥æˆ–ä»å…¶ä»–åœ°æ–¹è·å–
                print("âš ï¸  Cannot find tokenizer, falling back to random initialization")
                nn.init.normal_(self.prompt_embeddings.weight, std=0.02)
                return
        except:
            print("âš ï¸  Error accessing tokenizer, falling back to random initialization")
            nn.init.normal_(self.prompt_embeddings.weight, std=0.02)
            return
        
        # å¯¹è‡ªç„¶è¯­è¨€æç¤ºè¿›è¡Œç¼–ç 
        all_prompt_embeddings = []
        
        for prompt_text in natural_prompts:
            # åˆ†è¯å¹¶è·å–åµŒå…¥
            tokens = tokenizer.encode(prompt_text, add_special_tokens=False, return_tensors="pt")
            tokens = tokens.to(self.base_model.device)
            
            # è·å–tokençš„åµŒå…¥å‘é‡
            token_embeddings = self.base_model.get_input_embeddings()(tokens)  # [1, seq_len, hidden_size]
            
            # è®¡ç®—å¹³å‡åµŒå…¥ï¼ˆå¯¹åºåˆ—é•¿åº¦ç»´åº¦æ±‚å¹³å‡ï¼‰
            avg_embedding = token_embeddings.mean(dim=1)  # [1, hidden_size]
            all_prompt_embeddings.append(avg_embedding)
        
        # å°†æ‰€æœ‰æç¤ºçš„åµŒå…¥å †å å¹¶è®¡ç®—æ€»å¹³å‡
        stacked_embeddings = torch.cat(all_prompt_embeddings, dim=0)  # [num_prompts, hidden_size]
        mean_embedding = stacked_embeddings.mean(dim=0)  # [hidden_size]
        
        # ç”¨å¹³å‡åµŒå…¥åˆå§‹åŒ–æ‰€æœ‰è™šæ‹Ÿtokenï¼Œå¹¶æ·»åŠ å°çš„éšæœºæ‰°åŠ¨ä»¥å¢åŠ å¤šæ ·æ€§
        with torch.no_grad():
            for i in range(self.num_virtual_tokens):
                # åŸºç¡€åµŒå…¥ + å°çš„éšæœºæ‰°åŠ¨
                noise = torch.randn_like(mean_embedding) * 0.01
                self.prompt_embeddings.weight[i] = mean_embedding + noise
        
        print(f"âœ… Initialized {self.num_virtual_tokens} prompt embeddings using {len(natural_prompts)} natural language prompts")
    
    def _init_with_cluster_centers(self, cluster_centers: Optional[torch.Tensor]):
        """
        ä½¿ç”¨èšç±»ä¸­å¿ƒåˆå§‹åŒ–æç¤ºåµŒå…¥
        
        Args:
            cluster_centers: èšç±»ä¸­å¿ƒå¼ é‡ [num_clusters, embedding_dim]
                           æ¯ä¸ªèšç±»ä¸­å¿ƒä»£è¡¨ä¸€ç±»è¾“å…¥çš„è¯­ä¹‰ç‰¹å¾
        """
        if cluster_centers is None:
            print("âš ï¸  No cluster centers provided, falling back to random initialization")
            nn.init.normal_(self.prompt_embeddings.weight, std=0.02)
            return
        
        # ç¡®ä¿cluster_centersåœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
        cluster_centers = cluster_centers.to(self.prompt_embeddings.weight.device)
        
        num_clusters = cluster_centers.size(0)
        embedding_dim = cluster_centers.size(1)
        
        # æ£€æŸ¥ç»´åº¦åŒ¹é…
        if embedding_dim != self.prompt_embedding_dim:
            print(f"âš ï¸  Cluster center dimension ({embedding_dim}) doesn't match prompt embedding dimension ({self.prompt_embedding_dim})")
            print("   Falling back to random initialization")
            nn.init.normal_(self.prompt_embeddings.weight, std=0.02)
            return
        
        with torch.no_grad():
            if num_clusters >= self.num_virtual_tokens:
                # å¦‚æœèšç±»ä¸­å¿ƒæ•°é‡ >= è™šæ‹Ÿtokenæ•°é‡ï¼Œç›´æ¥ä½¿ç”¨å‰num_virtual_tokensä¸ªä¸­å¿ƒ
                self.prompt_embeddings.weight.copy_(cluster_centers[:self.num_virtual_tokens])
            else:
                # å¦‚æœèšç±»ä¸­å¿ƒæ•°é‡ < è™šæ‹Ÿtokenæ•°é‡ï¼Œéœ€è¦é‡å¤æˆ–æ’å€¼
                for i in range(self.num_virtual_tokens):
                    cluster_idx = i % num_clusters
                    # ä½¿ç”¨å¯¹åº”çš„èšç±»ä¸­å¿ƒï¼Œå¹¶æ·»åŠ å°çš„éšæœºæ‰°åŠ¨ä»¥å¢åŠ å¤šæ ·æ€§
                    noise = torch.randn_like(cluster_centers[cluster_idx]) * 0.01
                    self.prompt_embeddings.weight[i] = cluster_centers[cluster_idx] + noise
        
        print(f"âœ… Initialized {self.num_virtual_tokens} prompt embeddings using {num_clusters} cluster centers")

    def get_prompt_embeddings(self, batch_size: int) -> torch.Tensor:
        """
        è·å–æ‰¹æ¬¡çš„æç¤ºåµŒå…¥
        
        Args:
            batch_size: æ‰¹æ¬¡å¤§å°
            
        Returns:
            æç¤ºåµŒå…¥å¼ é‡ [batch_size, num_virtual_tokens, embedding_dim]
        """
        # åˆ›å»ºæç¤ºtokençš„ç´¢å¼•
        prompt_indices = torch.arange(self.num_virtual_tokens).unsqueeze(0).expand(
            batch_size, -1
        ).to(self.prompt_embeddings.weight.device)
        
        # è¿”å›å¯¹åº”çš„åµŒå…¥å‘é‡
        return self.prompt_embeddings(prompt_indices)
    
    def forward(
        self,
        input_ids: torch.Tensor,
        attention_mask: Optional[torch.Tensor] = None,
        labels: Optional[torch.Tensor] = None,
        **kwargs
    ):
        """
        å‰å‘ä¼ æ’­ï¼šå°†æç¤ºåµŒå…¥ä¸è¾“å…¥åµŒå…¥æ‹¼æ¥åä¼ å…¥åŸºç¡€æ¨¡å‹
        
        Args:
            input_ids: è¾“å…¥token ID [batch_size, seq_len]
            attention_mask: æ³¨æ„åŠ›æ©ç  [batch_size, seq_len]
            labels: æ ‡ç­¾ [batch_size, seq_len]
            
        Returns:
            æ¨¡å‹è¾“å‡º
        """
        batch_size = input_ids.size(0)
        
        # è·å–è¾“å…¥çš„åµŒå…¥è¡¨ç¤º
        input_embeddings = self.base_model.get_input_embeddings()(input_ids)
        
        # è·å–æç¤ºåµŒå…¥
        prompt_embeddings = self.get_prompt_embeddings(batch_size)
        
        # å°†æç¤ºåµŒå…¥ä¸è¾“å…¥åµŒå…¥æ‹¼æ¥ï¼š[prompt_embeddings | input_embeddings]
        inputs_embeds = torch.cat([prompt_embeddings, input_embeddings], dim=1)
        
        # è°ƒæ•´æ³¨æ„åŠ›æ©ç ä»¥åŒ…å«æç¤ºéƒ¨åˆ†
        if attention_mask is not None:
            # æ³¨æ„åŠ›æ©ç çš„å€¼å«ä¹‰ï¼š
            # - 1: å…è®¸æ³¨æ„åŠ›è®¡ç®—ï¼Œtokenå‚ä¸æ³¨æ„åŠ›æœºåˆ¶
            # - 0: å±è”½æ³¨æ„åŠ›è®¡ç®—ï¼Œtokenä¸å‚ä¸æ³¨æ„åŠ›æœºåˆ¶ï¼ˆå¦‚padding tokenï¼‰
            #
            # ä¸ºæç¤ºéƒ¨åˆ†åˆ›å»ºå…¨1çš„æ³¨æ„åŠ›æ©ç çš„åŸå› ï¼š
            # 1. æç¤ºåµŒå…¥æ˜¯å¯å­¦ä¹ çš„å‚æ•°ï¼Œéœ€è¦å‚ä¸æ³¨æ„åŠ›è®¡ç®—æ‰èƒ½å‘æŒ¥ä½œç”¨
            # 2. æç¤ºtokenä¸æ˜¯paddingï¼Œè€Œæ˜¯æœ‰æ„ä¹‰çš„è½¯æç¤ºå‘é‡
            # 3. æ¨¡å‹éœ€è¦èƒ½å¤Ÿ"çœ‹åˆ°"è¿™äº›æç¤ºä¿¡æ¯æ¥ç†è§£ä»»åŠ¡æŒ‡ä»¤
            # 4. å¦‚æœè®¾ä¸º0ï¼Œæç¤ºåµŒå…¥å°†è¢«å¿½ç•¥ï¼Œå¤±å»P-tuningçš„æ•ˆæœ
            prompt_attention_mask = torch.ones(
                batch_size, self.num_virtual_tokens,
                dtype=attention_mask.dtype,
                device=attention_mask.device
            )
            # æ‹¼æ¥æç¤ºå’Œè¾“å…¥çš„æ³¨æ„åŠ›æ©ç 
            # æœ€ç»ˆå½¢çŠ¶: [batch_size, num_virtual_tokens + original_seq_len]
            # å‰num_virtual_tokensä½ä¸º1ï¼ˆæç¤ºéƒ¨åˆ†ï¼‰ï¼Œåç»­ä¸ºåŸå§‹è¾“å…¥çš„æ©ç 
            attention_mask = torch.cat([prompt_attention_mask, attention_mask], dim=1)
        
        # è°ƒæ•´æ ‡ç­¾ä»¥åŒ…å«æç¤ºéƒ¨åˆ†ï¼ˆæç¤ºéƒ¨åˆ†æ ‡ç­¾è®¾ä¸º-100ï¼Œä¸å‚ä¸æŸå¤±è®¡ç®—ï¼‰
        if labels is not None:
            prompt_labels = torch.full(
                (batch_size, self.num_virtual_tokens),
                -100,  # -100è¡¨ç¤ºåœ¨æŸå¤±è®¡ç®—ä¸­å¿½ç•¥è¿™äº›ä½ç½®
                dtype=labels.dtype,
                device=labels.device
            )
            labels = torch.cat([prompt_labels, labels], dim=1)
        
        # é€šè¿‡åŸºç¡€æ¨¡å‹è¿›è¡Œå‰å‘ä¼ æ’­
        return self.base_model(
            inputs_embeds=inputs_embeds,
            attention_mask=attention_mask,
            labels=labels,
            **kwargs
        )
    
    def generate(self, input_ids: torch.Tensor, attention_mask: Optional[torch.Tensor] = None, **kwargs):
        """
        ç”Ÿæˆæ–‡æœ¬ï¼šåœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­åŒ…å«æç¤ºåµŒå…¥ - å®Œå…¨ä¿®å¤ç‰ˆæœ¬
        """
        batch_size = input_ids.size(0)
        
        # è·å–è¾“å…¥åµŒå…¥
        input_embeddings = self.base_model.get_input_embeddings()(input_ids)
        
        # è·å–æç¤ºåµŒå…¥
        prompt_embeddings = self.get_prompt_embeddings(batch_size)
        
        # æ‹¼æ¥æç¤ºåµŒå…¥å’Œè¾“å…¥åµŒå…¥
        inputs_embeds = torch.cat([prompt_embeddings, input_embeddings], dim=1)
        
        # ğŸš¨ å…³é”®ä¿®å¤ï¼šæ­£ç¡®å¤„ç†attention_mask
        if attention_mask is not None:
            # ä¸ºpromptéƒ¨åˆ†åˆ›å»ºattention mask (å…¨éƒ¨ä¸º1ï¼Œå› ä¸ºpromptéœ€è¦å‚ä¸attention)
            prompt_attention_mask = torch.ones(
                batch_size, self.num_virtual_tokens,
                dtype=attention_mask.dtype,
                device=attention_mask.device
            )
            # æ‹¼æ¥promptå’Œè¾“å…¥çš„attention mask
            combined_attention_mask = torch.cat([prompt_attention_mask, attention_mask], dim=1)
        else:
            # å¦‚æœæ²¡æœ‰æä¾›attention_maskï¼Œä¸ºæ•´ä¸ªåºåˆ—åˆ›å»ºå…¨1çš„mask
            total_length = self.num_virtual_tokens + input_ids.size(1)
            combined_attention_mask = torch.ones(
                batch_size, total_length,
                dtype=torch.long,
                device=input_ids.device
            )
        
        # ğŸš¨ ä¿®å¤ï¼šæ›´æ–°kwargsï¼Œç§»é™¤å†²çªå‚æ•°
        generation_kwargs = kwargs.copy()
        
        # ç§»é™¤input_idsï¼Œå› ä¸ºæˆ‘ä»¬ä½¿ç”¨inputs_embeds
        generation_kwargs.pop('input_ids', None)
        
        # è®¾ç½®æ­£ç¡®çš„å‚æ•°
        generation_kwargs['inputs_embeds'] = inputs_embeds
        generation_kwargs['attention_mask'] = combined_attention_mask
        
        # ğŸš¨ é‡è¦ä¿®å¤ï¼šå¦‚æœåŒæ—¶è®¾ç½®äº†max_lengthå’Œmax_new_tokensï¼Œä¼˜å…ˆä½¿ç”¨max_new_tokens
        if 'max_length' in generation_kwargs and 'max_new_tokens' in generation_kwargs:
            generation_kwargs.pop('max_length', None)
        
        # ğŸš¨ æ–°å¢ï¼šè®¾ç½®é»˜è®¤çš„åœæ­¢æ¡ä»¶
        if 'eos_token_id' not in generation_kwargs:
            generation_kwargs['eos_token_id'] = self.base_model.config.eos_token_id
        
        if 'pad_token_id' not in generation_kwargs:
            generation_kwargs['pad_token_id'] = self.base_model.config.pad_token_id
        
        # ğŸš¨ æ–°å¢ï¼šç¡®ä¿ç”Ÿæˆä¼šåœæ­¢
        if 'max_new_tokens' not in generation_kwargs and 'max_length' not in generation_kwargs:
            generation_kwargs['max_new_tokens'] = 100  # é»˜è®¤æœ€å¤šç”Ÿæˆ100ä¸ªtoken
        
        # ä½¿ç”¨åŸºç¡€æ¨¡å‹ç”Ÿæˆ
        return self.base_model.generate(**generation_kwargs)
    
    def save_prompt_embeddings(self, path: str):
        """
        ä¿å­˜è®­ç»ƒå¥½çš„æç¤ºåµŒå…¥
        
        Args:
            path: ä¿å­˜è·¯å¾„
        """
        torch.save(self.prompt_embeddings.state_dict(), path)
    
    def load_prompt_embeddings(self, path: str):
        """
        åŠ è½½é¢„è®­ç»ƒçš„æç¤ºåµŒå…¥
        
        Args:
            path: åŠ è½½è·¯å¾„
        """
        state_dict = torch.load(path, map_location='cpu')
        self.prompt_embeddings.load_state_dict(state_dict)
        print(f"âœ… Loaded prompt embeddings from {path}")
    
    @classmethod
    def from_pretrained_prompts(
        cls, 
        base_model: PreTrainedModel, 
        prompt_embeddings_path: str,
        config_path: Optional[str] = None
    ):
        """
        ä»ä¿å­˜çš„prompt embeddingsåˆ›å»ºP-tuningæ¨¡å‹
        
        Args:
            base_model: åŸºç¡€é¢„è®­ç»ƒæ¨¡å‹
            prompt_embeddings_path: prompt embeddingsæ–‡ä»¶è·¯å¾„
            config_path: é…ç½®æ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰
            
        Returns:
            åŠ è½½äº†è®­ç»ƒå¥½çš„prompt embeddingsçš„P-tuningæ¨¡å‹
        """
        # å¦‚æœæä¾›äº†é…ç½®æ–‡ä»¶ï¼Œä½¿ç”¨é…ç½®ä¸­çš„å‚æ•°
        if config_path and os.path.exists(config_path):
            import json
            with open(config_path, 'r') as f:
                config = json.load(f)
            
            model = cls(
                base_model=base_model,
                num_virtual_tokens=config['num_virtual_tokens'],
                prompt_embedding_dim=config.get('prompt_embedding_dim'),
                init_method=config.get('prompt_init_method', 'random')
            )
        else:
            # ä½¿ç”¨é»˜è®¤å‚æ•°
            model = cls(base_model=base_model)
        
        # åŠ è½½è®­ç»ƒå¥½çš„prompt embeddings
        model.load_prompt_embeddings(prompt_embeddings_path)
        return model

    # ğŸš¨ ç§»é™¤å†—ä½™çš„_compute_preference_losså’Œ_compute_log_probsæ–¹æ³•
    # è¿™äº›æ–¹æ³•å·²ç»åœ¨PTuningTrainerä¸­å®ç°ï¼Œä¿ç•™è¿™é‡Œä¼šé€ æˆæ··æ·†
