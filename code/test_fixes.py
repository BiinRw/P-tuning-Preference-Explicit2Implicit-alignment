#!/usr/bin/env python3
"""
æµ‹è¯•è„šæœ¬ï¼šéªŒè¯BFloat16å…¼å®¹æ€§å’Œembeddingç¼“å­˜ä¼˜åŒ–
"""

import torch
import sys
import os

# æ·»åŠ è·¯å¾„
sys.path.append('/home/wangbinrui/research_projects/llama_rlhf/code')

def test_bfloat16_conversion():
    """æµ‹è¯•BFloat16å¼ é‡åˆ°numpyçš„è½¬æ¢"""
    print("ğŸ§ª æµ‹è¯•BFloat16å¼ é‡è½¬æ¢...")
    
    try:
        # æµ‹è¯•BFloat16å¼ é‡
        if torch.cuda.is_available():
            tensor = torch.tensor([1.0, 2.0, 3.0], dtype=torch.bfloat16, device='cuda')
        else:
            tensor = torch.tensor([1.0, 2.0, 3.0], dtype=torch.bfloat16)
        
        print(f"åŸå§‹BFloat16å¼ é‡: {tensor}")
        
        # æµ‹è¯•æˆ‘ä»¬çš„ä¿®å¤æ–¹æ³•
        result = tensor.cpu().detach().float().numpy().tolist()
        print(f"è½¬æ¢ç»“æœ: {result}")
        
        # éªŒè¯ç»“æœæ­£ç¡®æ€§
        expected = [1.0, 2.0, 3.0]
        if result == expected:
            print("âœ… BFloat16è½¬æ¢æµ‹è¯•é€šè¿‡")
            return True
        else:
            print(f"âŒ BFloat16è½¬æ¢æµ‹è¯•å¤±è´¥: æœŸæœ› {expected}, å¾—åˆ° {result}")
            return False
            
    except Exception as e:
        print(f"âŒ BFloat16è½¬æ¢æµ‹è¯•å‡ºé”™: {e}")
        return False

def test_embedding_cache():
    """æµ‹è¯•embeddingç¼“å­˜æœºåˆ¶"""
    print("\nğŸ§ª æµ‹è¯•embeddingç¼“å­˜æœºåˆ¶...")
    
    try:
        from pro_utils.trainers import PreferenceDPO_trainer
        print("âœ… æˆåŠŸå¯¼å…¥PreferenceDPO_trainer")
        
        # åˆ›å»ºä¸€ä¸ªç®€å•çš„æ¨¡æ‹Ÿæ¨¡å‹ç”¨äºæµ‹è¯•
        class MockModel(torch.nn.Module):
            def __init__(self):
                super().__init__()
                self.embedding = torch.nn.Embedding(1000, 512)
                
            def get_input_embeddings(self):
                return self.embedding
        
        model = MockModel()
        print("âœ… åˆ›å»ºæµ‹è¯•æ¨¡å‹æˆåŠŸ")
        
        return True
        
    except Exception as e:
        print(f"âŒ embeddingç¼“å­˜æµ‹è¯•å‡ºé”™: {e}")
        return False

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("=" * 60)
    print("ğŸš€ å¼€å§‹è¿è¡ŒBFloat16å’Œembeddingç¼“å­˜æµ‹è¯•")
    print("=" * 60)
    
    print(f"PyTorchç‰ˆæœ¬: {torch.__version__}")
    print(f"CUDAå¯ç”¨: {torch.cuda.is_available()}")
    if torch.cuda.is_available():
        print(f"BFloat16æ”¯æŒ: {torch.cuda.is_bf16_supported()}")
        print(f"CUDAè®¾å¤‡æ•°: {torch.cuda.device_count()}")
    
    print("\n" + "=" * 40)
    
    # è¿è¡Œæµ‹è¯•
    test1_passed = test_bfloat16_conversion()
    test2_passed = test_embedding_cache()
    
    print("\n" + "=" * 60)
    print("ğŸ“‹ æµ‹è¯•ç»“æœæ‘˜è¦:")
    print(f"  BFloat16è½¬æ¢æµ‹è¯•: {'âœ… é€šè¿‡' if test1_passed else 'âŒ å¤±è´¥'}")
    print(f"  Embeddingç¼“å­˜æµ‹è¯•: {'âœ… é€šè¿‡' if test2_passed else 'âŒ å¤±è´¥'}")
    
    if test1_passed and test2_passed:
        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼å¯ä»¥å¼€å§‹è®­ç»ƒäº†ã€‚")
        return 0
    else:
        print("\nâš ï¸  éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥ä»£ç ä¿®å¤ã€‚")
        return 1

if __name__ == "__main__":
    exit(main())
