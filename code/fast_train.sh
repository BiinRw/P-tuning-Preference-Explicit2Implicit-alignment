#!/bin/bash

# ğŸ¯ åå¥½å¼•å¯¼çš„DPOè®­ç»ƒè„šæœ¬
# æ”¯æŒæ–‡æœ¬æŒ‡ä»¤å’ŒåµŒå…¥å‘é‡ä¸¤ç§æ¨¡å¼
# ä½œè€…: ç‹æ–Œç¿
# æ›´æ–°æ—¥æœŸ: 2025-05-29

echo "=================================================="
echo "ğŸš€ å¼€å§‹åå¥½å¼•å¯¼DPOè®­ç»ƒ (Preference-Guided DPO Training)"
echo "=================================================="

# ğŸ”§ è®¾ç½®ç¯å¢ƒå˜é‡
echo "ğŸ”§ é…ç½®è®­ç»ƒç¯å¢ƒ..."
export HF_DATASETS_OFFLINE=1
export TRANSFORMERS_OFFLINE=1
export HF_HUB_DISABLE_TELEMETRY=1
export HF_HUB_OFFLINE=1
export CUDA_DEVICE_ORDER=PCI_BUS_ID
export CUDA_VISIBLE_DEVICES=0,1,2,3
export TOKENIZERS_PARALLELISM=false

# ğŸ¯ ä¼˜åŒ–PyTorchæ€§èƒ½å’Œé¿å…trace cacheå¤±æ•ˆ
export TORCH_COMPILE_DEBUG=0
export TORCH_LOGS=""
export PYTORCH_CUDA_ALLOC_CONF="max_split_size_mb:512"
# ç¦ç”¨ä¸å¿…è¦çš„CUDAä¼˜åŒ–ä»¥å‡å°‘trace cacheå¤±æ•ˆ
export CUDA_LAUNCH_BLOCKING=0
export TORCH_CUDNN_V8_API_ENABLED=1

echo "âœ… ç¯å¢ƒå˜é‡è®¾ç½®å®Œæˆ"

# ğŸ“ è®­ç»ƒé…ç½®å‚æ•°
echo ""
echo "ğŸ“ è®¾ç½®è®­ç»ƒé…ç½®å‚æ•°..."

# ğŸ–¥ï¸ GPUèŠ‚ç‚¹é…ç½®
GPU_NODES="localhost:1,2"  # å¯æ ¹æ®å®é™…GPUé…ç½®ä¿®æ”¹ï¼Œå¦‚: "localhost:0,1,2,3" æˆ– "node1:0,1,node2:0,1"

# ğŸ›ï¸ è®­ç»ƒæ¨¡å¼é€‰æ‹© (äºŒé€‰ä¸€)
# MODE 1: ä½¿ç”¨æ–‡æœ¬æŒ‡ä»¤æ¨¡å¼
TRAINING_MODE="embedding"  # å¯é€‰: "text" æˆ– "embedding"

# MODE 2: ä½¿ç”¨åµŒå…¥å‘é‡æ¨¡å¼ (å¦‚æœæœ‰é¢„è®­ç»ƒçš„prompt embeddingæ–‡ä»¶)
# TRAINING_MODE="embedding"
PROMPT_EMBEDDING_PATH="/home/wangbinrui/research_projects/llama_rlhf/code/Ptuning/ptuning_outputs/qwen2.5-1.5b_vtokens10_initnatural_language_kl0.1_margin0.05_lr1e-5_ep10_bs2_20250528_133602/checkpoint-32600/prompt_embeddings.pt"

# ğŸ—ï¸ æ¨¡å‹è·¯å¾„é…ç½®
POLICY_MODEL_PATH="Qwen/Qwen2.5-1.5B-Instruct"
REFERENCE_MODEL_PATH="Qwen/Qwen2.5-1.5B-Instruct"

# ğŸ“Š æ•°æ®é›†è·¯å¾„é…ç½®
TRAIN_DATASET_PATH="/home/wangbinrui/research_projects/llama_rlhf/datasets/ultrafeedback_binarized/train_prefs_ultrafeedback_binarized.jsonl"
TEST_DATASET_PATH="/home/wangbinrui/research_projects/llama_rlhf/datasets/ultrafeedback_binarized/test_prefs_ultrafeedback_binarized.jsonl"

# ğŸ’¬ åå¥½æ–‡æœ¬æŒ‡ä»¤ (ä»…åœ¨æ–‡æœ¬æ¨¡å¼ä¸‹ä½¿ç”¨)
PREFERENCE_TEXT="Please provide a helpful, honest, harmless, and concise response."

# âš™ï¸ è®­ç»ƒè¶…å‚æ•°
BETA=0.05              # DPOæŸå¤±çš„æ¸©åº¦å‚æ•°
ALPHA=0.1              # åå¥½ä¸€è‡´æ€§æŸå¤±æƒé‡
LEARNING_RATE=5e-4     # å­¦ä¹ ç‡
NUM_EPOCHS=1           # è®­ç»ƒè½®æ•°
GRADIENT_ACCUM_STEPS=512  # æ¢¯åº¦ç´¯ç§¯æ­¥æ•°
MAX_LENGTH=300         # æœ€å¤§åºåˆ—é•¿åº¦
MAX_PROMPT_LENGTH=128  # æœ€å¤§æç¤ºé•¿åº¦

# ğŸ”— LoRAé…ç½®
LORA_R=16              # LoRAç§©
LORA_ALPHA=32          # LoRAç¼©æ”¾å‚æ•°
LORA_DROPOUT=0.1       # LoRA dropoutç‡

# ğŸ“ è¾“å‡ºå’Œæ—¥å¿—é…ç½®
OUTPUT_DIR="./model_output/Preference_Guided_Ptuning"
WANDB_PROJECT="Preference_Guided_Ptuning"

echo "âœ… å‚æ•°é…ç½®å®Œæˆ"

# ğŸ” æ•°æ®é›†éªŒè¯
echo ""
echo "ğŸ” éªŒè¯æ•°æ®é›†æ–‡ä»¶..."
if [ ! -f "$TRAIN_DATASET_PATH" ]; then
    echo "âŒ é”™è¯¯: è®­ç»ƒæ•°æ®é›†æ–‡ä»¶ä¸å­˜åœ¨: $TRAIN_DATASET_PATH"
    exit 1
fi

if [ ! -f "$TEST_DATASET_PATH" ]; then
    echo "âŒ é”™è¯¯: æµ‹è¯•æ•°æ®é›†æ–‡ä»¶ä¸å­˜åœ¨: $TEST_DATASET_PATH"
    exit 1
fi

# ç»Ÿè®¡æ•°æ®é›†æ ·æœ¬æ•°é‡
TRAIN_SAMPLES=$(wc -l < "$TRAIN_DATASET_PATH")
TEST_SAMPLES=$(wc -l < "$TEST_DATASET_PATH")
echo "ğŸ“Š è®­ç»ƒæ ·æœ¬æ•°é‡: $TRAIN_SAMPLES"
echo "ğŸ“Š æµ‹è¯•æ ·æœ¬æ•°é‡: $TEST_SAMPLES"
echo "âœ… æ•°æ®é›†éªŒè¯é€šè¿‡"

# ğŸ—‚ï¸ åˆ›å»ºè¾“å‡ºç›®å½•
echo ""
echo "ğŸ—‚ï¸ åˆ›å»ºè¾“å‡ºç›®å½•..."
mkdir -p "$OUTPUT_DIR"
mkdir -p "./logs/$WANDB_PROJECT"
echo "âœ… è¾“å‡ºç›®å½•åˆ›å»ºå®Œæˆ"

# ğŸ§  æ„å»ºè®­ç»ƒå‘½ä»¤
echo ""
echo "ğŸ§  æ„å»ºè®­ç»ƒå‘½ä»¤..."

# ğŸš€ DeepSpeedé…ç½®
DEEPSPEED_CMD="CUDA_ALLOC_CONF=expandable_segments deepspeed --include=$GPU_NODES"
TRAIN_SCRIPT="train_with_preference_prompt.py"

# ğŸ“ æ·»åŠ åŸºç¡€å‚æ•°
ARGS="--policy-model-path $POLICY_MODEL_PATH"
ARGS="$ARGS --reference-model-path $REFERENCE_MODEL_PATH"
ARGS="$ARGS --dataset-path $TRAIN_DATASET_PATH"
ARGS="$ARGS --test-dataset-path $TEST_DATASET_PATH"
ARGS="$ARGS --beta $BETA"
ARGS="$ARGS --alpha $ALPHA"
ARGS="$ARGS --learning-rate $LEARNING_RATE"
ARGS="$ARGS --num-train-epochs $NUM_EPOCHS"
ARGS="$ARGS --gradient-accumulation-steps $GRADIENT_ACCUM_STEPS"
ARGS="$ARGS --max-length $MAX_LENGTH"
ARGS="$ARGS --max-prompt-length $MAX_PROMPT_LENGTH"
ARGS="$ARGS --lora-r $LORA_R"
ARGS="$ARGS --lora-alpha $LORA_ALPHA"
ARGS="$ARGS --lora-dropout $LORA_DROPOUT"
ARGS="$ARGS --output-dir $OUTPUT_DIR"
ARGS="$ARGS --wandb-project $WANDB_PROJECT"

# ğŸ¯ æ ¹æ®è®­ç»ƒæ¨¡å¼æ·»åŠ ç‰¹å®šå‚æ•°
if [ "$TRAINING_MODE" = "embedding" ]; then
    echo "ğŸ¯ è®­ç»ƒæ¨¡å¼: åµŒå…¥å‘é‡æ¨¡å¼"
    if [ -z "$PROMPT_EMBEDDING_PATH" ]; then
        echo "âŒ é”™è¯¯: åµŒå…¥å‘é‡æ¨¡å¼éœ€è¦è®¾ç½® PROMPT_EMBEDDING_PATH"
        exit 1
    fi
    if [ ! -f "$PROMPT_EMBEDDING_PATH" ]; then
        echo "âŒ é”™è¯¯: åµŒå…¥å‘é‡æ–‡ä»¶ä¸å­˜åœ¨: $PROMPT_EMBEDDING_PATH"
        exit 1
    fi
    ARGS="$ARGS --use-prompt-embedding --prompt-embedding-path $PROMPT_EMBEDDING_PATH"
    echo "ğŸ“ åµŒå…¥å‘é‡æ–‡ä»¶: $PROMPT_EMBEDDING_PATH"
    echo "ğŸ“ åµŒå…¥å‘é‡æ–‡ä»¶å¤§å°: $(du -h "$PROMPT_EMBEDDING_PATH" | cut -f1)"
elif [ "$TRAINING_MODE" = "text" ]; then
    echo "ğŸ¯ è®­ç»ƒæ¨¡å¼: æ–‡æœ¬æŒ‡ä»¤æ¨¡å¼"
    ARGS="$ARGS --preference-text \"$PREFERENCE_TEXT\""
    echo "ğŸ’¬ åå¥½æŒ‡ä»¤: $PREFERENCE_TEXT"
else
    echo "âŒ é”™è¯¯: æ— æ•ˆçš„è®­ç»ƒæ¨¡å¼: $TRAINING_MODE (åº”ä¸º 'text' æˆ– 'embedding')"
    exit 1
fi

# ğŸ”¥ æ„å»ºå®Œæ•´çš„DeepSpeedè®­ç»ƒå‘½ä»¤
FULL_CMD="$DEEPSPEED_CMD $TRAIN_SCRIPT $ARGS"

echo "âœ… è®­ç»ƒå‘½ä»¤æ„å»ºå®Œæˆ"

# ğŸ“‹ æ‰“å°å®Œæ•´é…ç½®ä¿¡æ¯
echo ""
echo "=================================================="
echo "ğŸ“‹ å®Œæ•´è®­ç»ƒé…ç½®"
echo "=================================================="
echo "ğŸ¯ è®­ç»ƒæ¨¡å¼: $TRAINING_MODE"
echo "ğŸ—ï¸ ç­–ç•¥æ¨¡å‹: $POLICY_MODEL_PATH"
echo "ğŸ—ï¸ å‚è€ƒæ¨¡å‹: $REFERENCE_MODEL_PATH"
echo "ğŸ“Š è®­ç»ƒæ•°æ®: $TRAIN_DATASET_PATH ($TRAIN_SAMPLES æ ·æœ¬)"
echo "ğŸ“Š æµ‹è¯•æ•°æ®: $TEST_DATASET_PATH ($TEST_SAMPLES æ ·æœ¬)"
if [ "$TRAINING_MODE" = "text" ]; then
    echo "ğŸ’¬ åå¥½æŒ‡ä»¤: $PREFERENCE_TEXT"
else
    echo "ğŸ“ åµŒå…¥å‘é‡: $PROMPT_EMBEDDING_PATH"
fi
echo "âš™ï¸ Beta: $BETA, Alpha: $ALPHA"
echo "âš™ï¸ å­¦ä¹ ç‡: $LEARNING_RATE, è®­ç»ƒè½®æ•°: $NUM_EPOCHS"
echo "âš™ï¸ LoRAé…ç½®: r=$LORA_R, alpha=$LORA_ALPHA, dropout=$LORA_DROPOUT"
echo "ğŸ”¥ DeepSpeedèŠ‚ç‚¹: $GPU_NODES"
echo "ğŸ“ è¾“å‡ºç›®å½•: $OUTPUT_DIR"
echo "=================================================="

# ğŸš¦ æœ€ç»ˆç¡®è®¤
echo ""
echo "ğŸš¦ å³å°†å¼€å§‹è®­ç»ƒï¼Œè¯·ç¡®è®¤é…ç½®æ— è¯¯..."
echo "ğŸš€ DeepSpeedè®­ç»ƒå‘½ä»¤:"
echo "   $FULL_CMD"
echo ""
read -p "æ˜¯å¦ç»§ç»­è®­ç»ƒï¼Ÿ(y/N): " -n 1 -r
echo
if [[ ! $REPLY =~ ^[Yy]$ ]]; then
    echo "âŒ è®­ç»ƒå·²å–æ¶ˆ"
    exit 1
fi

# ğŸš€ å¼€å§‹è®­ç»ƒ
echo ""
echo "ğŸš€ å¼€å§‹è®­ç»ƒ..."
echo "=================================================="

# è®°å½•å¼€å§‹æ—¶é—´
START_TIME=$(date +%s)
echo "â° è®­ç»ƒå¼€å§‹æ—¶é—´: $(date)"

# ğŸ”¥ æ‰§è¡ŒDeepSpeedè®­ç»ƒ
echo "ğŸ”¥ å¯åŠ¨DeepSpeedåˆ†å¸ƒå¼è®­ç»ƒè¿›ç¨‹..."
echo "ğŸ–¥ï¸  ä½¿ç”¨GPUèŠ‚ç‚¹: $GPU_NODES"
echo "âš¡ æ‰©å±•å†…å­˜æ®µé…ç½®å·²å¯ç”¨"
echo ""

# æ‰§è¡Œè®­ç»ƒå‘½ä»¤
eval $FULL_CMD

# æ£€æŸ¥è®­ç»ƒç»“æœ
TRAIN_EXIT_CODE=$?
END_TIME=$(date +%s)
DURATION=$((END_TIME - START_TIME))
HOURS=$((DURATION / 3600))
MINUTES=$(((DURATION % 3600) / 60))
SECONDS=$((DURATION % 60))

echo ""
echo "=================================================="
if [ $TRAIN_EXIT_CODE -eq 0 ]; then
    echo "âœ… è®­ç»ƒæˆåŠŸå®Œæˆï¼"
    echo "ğŸ† æ¨¡å‹å·²ä¿å­˜åˆ°: $OUTPUT_DIR"
    echo "ğŸ“Š æ—¥å¿—æ–‡ä»¶ä½ç½®: ./logs/$WANDB_PROJECT"
    echo "â° æ€»è®­ç»ƒæ—¶é—´: ${HOURS}å°æ—¶ ${MINUTES}åˆ†é’Ÿ ${SECONDS}ç§’"
    
    # æ£€æŸ¥è¾“å‡ºæ–‡ä»¶
    echo ""
    echo "ğŸ“ æ£€æŸ¥è¾“å‡ºæ–‡ä»¶..."
    if [ -d "$OUTPUT_DIR" ]; then
        echo "ğŸ“‚ è¾“å‡ºç›®å½•å†…å®¹:"
        ls -la "$OUTPUT_DIR"
    fi
    
    echo ""
    echo "ğŸ‰ è®­ç»ƒæµç¨‹å…¨éƒ¨å®Œæˆï¼"
else
    echo "âŒ è®­ç»ƒå¤±è´¥ï¼Œé€€å‡ºç : $TRAIN_EXIT_CODE"
    echo "â° è®­ç»ƒæŒç»­æ—¶é—´: ${HOURS}å°æ—¶ ${MINUTES}åˆ†é’Ÿ ${SECONDS}ç§’"
    echo "ğŸ” è¯·æ£€æŸ¥ä¸Šæ–¹çš„é”™è¯¯ä¿¡æ¯"
fi
echo "=================================================="
