# ğŸš€ åå¥½å¼•å¯¼DPOè®­ç»ƒä½¿ç”¨æŒ‡å—

## ğŸ“š è®­ç»ƒæ¨¡å¼ä»‹ç»

### 1. ğŸ”¤ æ–‡æœ¬æŒ‡ä»¤æ¨¡å¼ (Text Instruction Mode)
- **ç”¨é€”**: ä½¿ç”¨ç¡¬ç¼–ç çš„æ–‡æœ¬ä½œä¸ºåå¥½æŒ‡ä»¤
- **é€‚ç”¨åœºæ™¯**: å¿«é€Ÿå®éªŒã€åŸºç¡€è®­ç»ƒã€æ²¡æœ‰é¢„è®­ç»ƒåµŒå…¥å‘é‡çš„æƒ…å†µ
- **é…ç½®**: ä¿®æ”¹ `PREFERENCE_TEXT` å˜é‡

### 2. ğŸ§  åµŒå…¥å‘é‡æ¨¡å¼ (Prompt Embedding Mode)
- **ç”¨é€”**: ä½¿ç”¨é¢„è®­ç»ƒçš„æç¤ºåµŒå…¥å‘é‡ä»£æ›¿æ–‡æœ¬æŒ‡ä»¤
- **é€‚ç”¨åœºæ™¯**: æœ‰é¢„è®­ç»ƒP-tuningç»“æœã€è¿½æ±‚æ›´å¥½æ€§èƒ½çš„æƒ…å†µ
- **é…ç½®**: è®¾ç½® `TRAINING_MODE="embedding"` å’Œ `PROMPT_EMBEDDING_PATH`

## âš™ï¸ å…³é”®å‚æ•°è¯´æ˜

### ğŸ–¥ï¸ GPUèŠ‚ç‚¹é…ç½®
```bash
GPU_NODES="localhost:1,2"   # å¯é…ç½®çš„GPUèŠ‚ç‚¹ï¼Œç¤ºä¾‹é…ç½®ï¼š
                             # "localhost:0,1"     - ä½¿ç”¨æœ¬æœºGPU 0,1
                             # "localhost:2,3"     - ä½¿ç”¨æœ¬æœºGPU 2,3  
                             # "localhost:0,1,2,3" - ä½¿ç”¨æœ¬æœºæ‰€æœ‰4å¼ GPU
                             # "node1:0,1,node2:0,1" - è·¨èŠ‚ç‚¹å¤šGPUè®­ç»ƒ
```

### ğŸ›ï¸ æ ¸å¿ƒè®­ç»ƒå‚æ•°
```bash
BETA=0.05              # DPOæŸå¤±æ¸©åº¦å‚æ•° (0.01-0.5, è¶Šå°è¶Šä¿å®ˆ)
ALPHA=0.1              # åå¥½ä¸€è‡´æ€§æŸå¤±æƒé‡ (0.05-0.2, æ§åˆ¶åå¥½å¯¹é½å¼ºåº¦)
LEARNING_RATE=5e-4     # å­¦ä¹ ç‡ (1e-5 to 1e-3)
NUM_EPOCHS=1           # è®­ç»ƒè½®æ•° (1-3è½®é€šå¸¸è¶³å¤Ÿ)
GRADIENT_ACCUM_STEPS=512  # æ¢¯åº¦ç´¯ç§¯æ­¥æ•° (æ ¹æ®æ˜¾å­˜è°ƒæ•´)
```

### ğŸ”— LoRAé…ç½®
```bash
LORA_R=16              # LoRAç§© (8,16,32,64 - è¶Šå¤§æ¨¡å‹å®¹é‡è¶Šå¼ºä½†è®­ç»ƒè¶Šæ…¢)
LORA_ALPHA=32          # LoRAç¼©æ”¾ (é€šå¸¸è®¾ä¸º2*LORA_R)
LORA_DROPOUT=0.1       # Dropoutç‡ (0.05-0.1)
```

### ğŸ“ åºåˆ—é•¿åº¦é…ç½®
```bash
MAX_LENGTH=300         # æœ€å¤§åºåˆ—é•¿åº¦ (æ ¹æ®æ•°æ®é›†å’Œæ˜¾å­˜è°ƒæ•´)
MAX_PROMPT_LENGTH=128  # æœ€å¤§æç¤ºé•¿åº¦ (é˜²æ­¢æç¤ºè¿‡é•¿)
```

## ğŸš€ å¿«é€Ÿå¼€å§‹

### æ­¥éª¤1: æ£€æŸ¥ç¯å¢ƒ
```bash
cd /home/wangbinrui/research_projects/llama_rlhf/code
ls -la train_with_preference_prompt.py fast_train.sh
```

### æ­¥éª¤2: é…ç½®å‚æ•°
ç¼–è¾‘ `fast_train.sh` æ–‡ä»¶ä¸­çš„é…ç½®éƒ¨åˆ†:

#### æ–‡æœ¬æ¨¡å¼é…ç½®:
```bash
TRAINING_MODE="text"
PREFERENCE_TEXT="è¯·æä¾›ä¸€ä¸ªæœ‰ç”¨ã€è¯šå®ã€æ— å®³ä¸”ç®€æ´çš„å›ç­”ã€‚"
```

#### åµŒå…¥æ¨¡å¼é…ç½®:
```bash
TRAINING_MODE="embedding"
PROMPT_EMBEDDING_PATH="/path/to/your/prompt_embeddings.pt"
```

### æ­¥éª¤3: è¿è¡Œè®­ç»ƒ
```bash
chmod +x fast_train.sh
./fast_train.sh
```

**æ³¨æ„**: è„šæœ¬ä¼šè‡ªåŠ¨ä½¿ç”¨æ‚¨çš„åŸå§‹DeepSpeedå‘½ä»¤æ ¼å¼:
```bash
CUDA_ALLOC_CONF=expandable_segments deepspeed --include=localhost:1,2 train_with_preference_prompt.py
```

è¿™ç¡®ä¿äº†æ‰©å±•å†…å­˜æ®µé…ç½®å’ŒæŒ‡å®šGPUèŠ‚ç‚¹çš„åˆ†å¸ƒå¼è®­ç»ƒã€‚

## ğŸ“ è¾“å‡ºæ–‡ä»¶ç»“æ„
```
model_output/Preference_Guided_DPO/
â”œâ”€â”€ pytorch_model.bin           # è®­ç»ƒåçš„æ¨¡å‹æƒé‡
â”œâ”€â”€ config.json                 # æ¨¡å‹é…ç½®
â”œâ”€â”€ adapter_config.json         # LoRAé€‚é…å™¨é…ç½®
â”œâ”€â”€ adapter_model.bin           # LoRAæƒé‡
â”œâ”€â”€ training_args.bin           # è®­ç»ƒå‚æ•°
â”œâ”€â”€ tokenizer.json              # åˆ†è¯å™¨
â”œâ”€â”€ tokenizer_config.json       # åˆ†è¯å™¨é…ç½®
â””â”€â”€ trainer_state.json          # è®­ç»ƒçŠ¶æ€

logs/Preference_Guided_DPO/
â”œâ”€â”€ events.out.tfevents.*       # TensorBoardæ—¥å¿—
â””â”€â”€ runs/                       # è®­ç»ƒè¿è¡Œè®°å½•
```

## ğŸ¯ è®­ç»ƒç›‘æ§

### å…³é”®æŒ‡æ ‡è§£é‡Š:
- **loss/train**: æ€»è®­ç»ƒæŸå¤±
- **loss_components/dpo_loss**: DPOæŸå¤±åˆ†é‡
- **loss_components/aligned_loss**: åå¥½å¯¹é½æŸå¤±
- **rewards_train/accuracy**: å¥–åŠ±å‡†ç¡®ç‡
- **preference_metrics/pref_advantage_ratio**: åå¥½ä¼˜åŠ¿æ¯”ç‡

### TensorBoardç›‘æ§:
```bash
tensorboard --logdir=./logs/Preference_Guided_DPO --port=6006
```

## ğŸ”§ å¸¸è§é—®é¢˜ä¸è§£å†³æ–¹æ¡ˆ

### é—®é¢˜1: CUDAå†…å­˜ä¸è¶³
**è§£å†³æ–¹æ¡ˆ:**
- å‡å°‘ `GRADIENT_ACCUM_STEPS`
- é™ä½ `MAX_LENGTH`
- å‡å° `LORA_R`

### é—®é¢˜2: è®­ç»ƒé€Ÿåº¦å¤ªæ…¢
**è§£å†³æ–¹æ¡ˆ:**
- å¢åŠ  `GRADIENT_ACCUM_STEPS`
- ä½¿ç”¨æ›´å°çš„æ¨¡å‹
- å¯ç”¨ `gradient_checkpointing`

### é—®é¢˜3: æ¨¡å‹æ€§èƒ½ä¸ä½³
**è§£å†³æ–¹æ¡ˆ:**
- è°ƒæ•´ `BETA` å‚æ•° (0.01-0.1)
- å¢åŠ  `ALPHA` æƒé‡
- ä½¿ç”¨æ›´å¥½çš„åå¥½æ–‡æœ¬

### é—®é¢˜4: åµŒå…¥å‘é‡åŠ è½½å¤±è´¥
**è§£å†³æ–¹æ¡ˆ:**
- æ£€æŸ¥æ–‡ä»¶è·¯å¾„å’Œæ ¼å¼
- ç¡®ä¿æ–‡ä»¶åŒ…å«æ­£ç¡®çš„tensoræ ¼å¼
- éªŒè¯åµŒå…¥å‘é‡ç»´åº¦åŒ¹é…

## ğŸ“Š æ€§èƒ½è°ƒä¼˜å»ºè®®

### 1. å‚æ•°è°ƒä¼˜ç­–ç•¥
```bash
# ä¿å®ˆè®¾ç½® (ç¨³å®šä½†æå‡æœ‰é™)
BETA=0.01, ALPHA=0.05

# ä¸­ç­‰è®¾ç½® (å¹³è¡¡æ€§èƒ½ä¸ç¨³å®šæ€§)
BETA=0.05, ALPHA=0.1

# æ¿€è¿›è®¾ç½® (è¿½æ±‚æœ€å¤§æ€§èƒ½ä½†å¯èƒ½ä¸ç¨³å®š)
BETA=0.1, ALPHA=0.2
```

### 2. ç¡¬ä»¶é…ç½®ä¼˜åŒ–
```bash
# 4x RTX 3090/4090 æ¨èé…ç½®
GRADIENT_ACCUM_STEPS=512
MAX_LENGTH=300
LORA_R=16

# 8x A100 æ¨èé…ç½®  
GRADIENT_ACCUM_STEPS=256
MAX_LENGTH=512
LORA_R=32
```

## ğŸ”„ æ¨¡å‹ä½¿ç”¨

### åŠ è½½è®­ç»ƒåçš„æ¨¡å‹:
```python
from peft import PeftModel
from transformers import AutoModelForCausalLM, AutoTokenizer

# åŠ è½½åŸºç¡€æ¨¡å‹
base_model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen2.5-1.5B-Instruct")
tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen2.5-1.5B-Instruct")

# åŠ è½½LoRAé€‚é…å™¨
model = PeftModel.from_pretrained(base_model, "./model_output/Preference_Guided_DPO")

# æ¨ç†ä½¿ç”¨
inputs = tokenizer("ä½ çš„é—®é¢˜", return_tensors="pt")
outputs = model.generate(**inputs, max_length=300)
response = tokenizer.decode(outputs[0], skip_special_tokens=True)
```

## ğŸ“ˆ å®éªŒå»ºè®®

1. **é¦–æ¬¡è®­ç»ƒ**: ä½¿ç”¨æ–‡æœ¬æ¨¡å¼ï¼Œè¾ƒå°çš„betaå€¼ (0.01-0.05)
2. **æ€§èƒ½æå‡**: å°è¯•ä¸åŒçš„åå¥½æ–‡æœ¬å’Œalphaå€¼
3. **æœ€ä¼˜ç»“æœ**: ä½¿ç”¨é¢„è®­ç»ƒçš„åµŒå…¥å‘é‡æ¨¡å¼
4. **ç”Ÿäº§éƒ¨ç½²**: è¿›è¡Œå……åˆ†çš„è¯„ä¼°å’Œæµ‹è¯•
