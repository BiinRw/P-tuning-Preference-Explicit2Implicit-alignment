# 🚀 偏好引导DPO训练使用指南

## 📚 训练模式介绍

### 1. 🔤 文本指令模式 (Text Instruction Mode)
- **用途**: 使用硬编码的文本作为偏好指令
- **适用场景**: 快速实验、基础训练、没有预训练嵌入向量的情况
- **配置**: 修改 `PREFERENCE_TEXT` 变量

### 2. 🧠 嵌入向量模式 (Prompt Embedding Mode)
- **用途**: 使用预训练的提示嵌入向量代替文本指令
- **适用场景**: 有预训练P-tuning结果、追求更好性能的情况
- **配置**: 设置 `TRAINING_MODE="embedding"` 和 `PROMPT_EMBEDDING_PATH`

## ⚙️ 关键参数说明

### 🖥️ GPU节点配置
```bash
GPU_NODES="localhost:1,2"   # 可配置的GPU节点，示例配置：
                             # "localhost:0,1"     - 使用本机GPU 0,1
                             # "localhost:2,3"     - 使用本机GPU 2,3  
                             # "localhost:0,1,2,3" - 使用本机所有4张GPU
                             # "node1:0,1,node2:0,1" - 跨节点多GPU训练
```

### 🎛️ 核心训练参数
```bash
BETA=0.05              # DPO损失温度参数 (0.01-0.5, 越小越保守)
ALPHA=0.1              # 偏好一致性损失权重 (0.05-0.2, 控制偏好对齐强度)
LEARNING_RATE=5e-4     # 学习率 (1e-5 to 1e-3)
NUM_EPOCHS=1           # 训练轮数 (1-3轮通常足够)
GRADIENT_ACCUM_STEPS=512  # 梯度累积步数 (根据显存调整)
```

### 🔗 LoRA配置
```bash
LORA_R=16              # LoRA秩 (8,16,32,64 - 越大模型容量越强但训练越慢)
LORA_ALPHA=32          # LoRA缩放 (通常设为2*LORA_R)
LORA_DROPOUT=0.1       # Dropout率 (0.05-0.1)
```

### 📏 序列长度配置
```bash
MAX_LENGTH=300         # 最大序列长度 (根据数据集和显存调整)
MAX_PROMPT_LENGTH=128  # 最大提示长度 (防止提示过长)
```

## 🚀 快速开始

### 步骤1: 检查环境
```bash
cd /home/wangbinrui/research_projects/llama_rlhf/code
ls -la train_with_preference_prompt.py fast_train.sh
```

### 步骤2: 配置参数
编辑 `fast_train.sh` 文件中的配置部分:

#### 文本模式配置:
```bash
TRAINING_MODE="text"
PREFERENCE_TEXT="请提供一个有用、诚实、无害且简洁的回答。"
```

#### 嵌入模式配置:
```bash
TRAINING_MODE="embedding"
PROMPT_EMBEDDING_PATH="/path/to/your/prompt_embeddings.pt"
```

### 步骤3: 运行训练
```bash
chmod +x fast_train.sh
./fast_train.sh
```

**注意**: 脚本会自动使用您的原始DeepSpeed命令格式:
```bash
CUDA_ALLOC_CONF=expandable_segments deepspeed --include=localhost:1,2 train_with_preference_prompt.py
```

这确保了扩展内存段配置和指定GPU节点的分布式训练。

## 📁 输出文件结构
```
model_output/Preference_Guided_DPO/
├── pytorch_model.bin           # 训练后的模型权重
├── config.json                 # 模型配置
├── adapter_config.json         # LoRA适配器配置
├── adapter_model.bin           # LoRA权重
├── training_args.bin           # 训练参数
├── tokenizer.json              # 分词器
├── tokenizer_config.json       # 分词器配置
└── trainer_state.json          # 训练状态

logs/Preference_Guided_DPO/
├── events.out.tfevents.*       # TensorBoard日志
└── runs/                       # 训练运行记录
```

## 🎯 训练监控

### 关键指标解释:
- **loss/train**: 总训练损失
- **loss_components/dpo_loss**: DPO损失分量
- **loss_components/aligned_loss**: 偏好对齐损失
- **rewards_train/accuracy**: 奖励准确率
- **preference_metrics/pref_advantage_ratio**: 偏好优势比率

### TensorBoard监控:
```bash
tensorboard --logdir=./logs/Preference_Guided_DPO --port=6006
```

## 🔧 常见问题与解决方案

### 问题1: CUDA内存不足
**解决方案:**
- 减少 `GRADIENT_ACCUM_STEPS`
- 降低 `MAX_LENGTH`
- 减小 `LORA_R`

### 问题2: 训练速度太慢
**解决方案:**
- 增加 `GRADIENT_ACCUM_STEPS`
- 使用更小的模型
- 启用 `gradient_checkpointing`

### 问题3: 模型性能不佳
**解决方案:**
- 调整 `BETA` 参数 (0.01-0.1)
- 增加 `ALPHA` 权重
- 使用更好的偏好文本

### 问题4: 嵌入向量加载失败
**解决方案:**
- 检查文件路径和格式
- 确保文件包含正确的tensor格式
- 验证嵌入向量维度匹配

## 📊 性能调优建议

### 1. 参数调优策略
```bash
# 保守设置 (稳定但提升有限)
BETA=0.01, ALPHA=0.05

# 中等设置 (平衡性能与稳定性)
BETA=0.05, ALPHA=0.1

# 激进设置 (追求最大性能但可能不稳定)
BETA=0.1, ALPHA=0.2
```

### 2. 硬件配置优化
```bash
# 4x RTX 3090/4090 推荐配置
GRADIENT_ACCUM_STEPS=512
MAX_LENGTH=300
LORA_R=16

# 8x A100 推荐配置  
GRADIENT_ACCUM_STEPS=256
MAX_LENGTH=512
LORA_R=32
```

## 🔄 模型使用

### 加载训练后的模型:
```python
from peft import PeftModel
from transformers import AutoModelForCausalLM, AutoTokenizer

# 加载基础模型
base_model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen2.5-1.5B-Instruct")
tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen2.5-1.5B-Instruct")

# 加载LoRA适配器
model = PeftModel.from_pretrained(base_model, "./model_output/Preference_Guided_DPO")

# 推理使用
inputs = tokenizer("你的问题", return_tensors="pt")
outputs = model.generate(**inputs, max_length=300)
response = tokenizer.decode(outputs[0], skip_special_tokens=True)
```

## 📈 实验建议

1. **首次训练**: 使用文本模式，较小的beta值 (0.01-0.05)
2. **性能提升**: 尝试不同的偏好文本和alpha值
3. **最优结果**: 使用预训练的嵌入向量模式
4. **生产部署**: 进行充分的评估和测试
